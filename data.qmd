---
execute:
  freeze: auto
---

# Local data {#data}

```{r, message = FALSE, warning = FALSE, echo = FALSE}
knitr::opts_knit$set(root.dir = fs::dir_create(tempfile()))
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", eval = TRUE)
```

```{r, message = FALSE, warning = FALSE, echo = FALSE, eval = TRUE}
library(targets)
```

This chapter describes how the `targets` package stores data, manages memory, allows you to customize the data processing model.

:::{.callout-tip}
## Performance

See the [performance chapter](#performance) for options, settings, and other choices to make storage and memory more efficient for large data workflows.
:::

## Local data store

`tar_make()` creates a special data folder called `_targets/` at the root of your project.

```{r, eval = FALSE}
fs::dir_tree("_targets")
_targets
├── meta
│   ├── crew
│   ├── meta
│   ├── process
│   └── progress
├── objects
│   ├── target1
│   ├── target2
│   ├── dynamic_branch_c7bcb4bd
│   ├── dynamic_branch_285fb6a9
│   └── dynamic_branch_874ca381
├── scratch # tar_make() deletes this folder after it finishes.
└── user # gittargets users can put custom files here for data version control.
```

The two most important parts are:


1. `_targets/meta/meta`: a text file with critical metadata.
2. `_targets/objects/`: a folder with the output data of each target.

Consider this pipeline:

```{r, eval = FALSE, echo = TRUE}
library(targets)
list(
  tar_target(
    name = target1,
    command = 11 + 46,
    format = "rds",
    repository = "local"
  )
)
```

`tar_make()` does the following:

1. Run the command of `target1` and observe a return value of `57`.
1. Save the value `57` to `_targets/objects/target1` using [saveRDS()](https://stat.ethz.ch/R-manual/R-devel/library/base/html/saveRDS.html).
1. Append a line to `_targets/meta/meta` containing the hash, time stamp, file size, warnings, errors, and execution time of `target1`.
1. Append a line to `_targets/meta/progress` to indicate that `target1` finished.

Remarks:

* To read the value of `target1` back into R, `tar_read(target1)` is much better than `readRDS("_targets/objects/target1")`.
* The `format` argument of `tar_target()` controls how `tar_make()` saves the return value. The default is `"rds"`, and there are more efficient formats such as `"qs"` and `"feather"`. Some of these formats require external packages. See <https://docs.ropensci.org/targets/reference/tar_target.html#storage-formats> for details.
* For efficiency, `tar_make()` does not write to `_targets/meta/meta` or `_targets/meta/progress` every single time a target completes. Instead, it waits and gathers a backlog of text lines in memory, then writes whole batches of lines at a time. This behavior risks losing metadata in the event of a crash, but it reduces costly interactions with the file system. The `seconds_meta` argument controls how often `tar_make()` writes metadata. `seconds_reporter` does the same for messages printed to the R console.

## External files

If your pipeline loads a preexisting data file or creates files outside the data store, it is good practice to watch them for changes. That way, `tar_make()` will automatically rerun the appropriate targets if these files change. To watch one of more files, create a target that

1. Has `format = "file"` in `tar_target()`, and
2. Returns a character vector of local files and/or directories.

The example sketch of a pipeline below follows this pattern.

```{r, eval = FALSE, echo = TRUE}
# _targets.R
library(targets)
create_output <- function(file) {
  data <- read.csv(file)
  output <- head(data)
  write.csv(output, "output.csv")
  "output.csv"
}
list(
  tar_target(name = input, command = "data.csv", format = "file"),
  tar_target(name = output, command = create_output(input), format = "file")
)
```

We assume a file called `data.csv` exists prior to running the pipeline. When `tar_make()` runs the first time, target `input` runs and returns the value `"data.csv"`. Because `format` is `"file"`, no extra file is saved to `_targets/meta/objects/`. Instead, `"data.csv"` gets hashed, and the hash is stored in the metadata. Then, target `output` runs, creates the file `"output.csv"`, and that file gets processed the same way.

Target `output` depends on target `input` because the command of target `output` mentions the symbol `input`. (Verify with `tar_visnetwork()`.) That way, `output` does not run until `input` is finished, and `output` reruns if the hash of `input` changes. It is good practice to write target symbols instead of literal input paths to ensure the proper dependency relationships. In this case, if `output` were written with the literal input path as `tar_target(name = output, command = create_output("data.csv"), format = "file")`, then the dependency relationship would break, and `output` would not rerun if `input` changed.

The mechanism of `format = "file"` applies equally to input files and output files. In fact, a target can track both input and output files at the same time. This is part of how [`tar_render()`](https://docs.ropensci.org/tarchetypes/reference/tar_render.html) works. As discussed in the [R Markdown chapter](#literate-programming), [`tar_render()`](https://docs.ropensci.org/tarchetypes/reference/tar_render.html) takes an R Markdown source file as input, write a rendered report file as output, and returns a character vector with the paths to both files.

## Memory

A typical target has dependencies upstream. In order to run properly, it needs the return values of those dependencies to exist in the random access memory (RAM). By default, `tar_make()` reads those dependency targets from the data store, and it keeps in memory those targets and any targets that run. For big data workflows where not all data can fit into RAM, it is wiser to set `memory = "transient"` and `garbage_collection = TRUE` in `tar_target()` (and `tar_option_set()`). That way, the target return value is removed from memory at the earliest opportunity. The next time the target value is needed, it is reread from storage again, and then removed from memory as soon as possible. Reading a big dataset from storage can take time, which may slow down some pipelines, but it may be worth the extra time to make sure memory usage stays within reasonable limits. It is also worth considering `format = "file"` in `tar_target()` so the file is hashed but not loaded into memory and downstream targets can read only small subsets of the data in the file. See the [performance chapter](#performance) for more details.

## Cleaning up local internal data files

There are [multiple functions](https://docs.ropensci.org/targets/reference/index.html#section-clean) to remove or clean up target storage. Most of these functions delete internal files or records from the data store and delete objects from cloud buckets. They do not delete local external files (i.e. `tar_target(..., format = "file", repository = "local")`) because some of those files could be local input data that exists prior to `tar_make()`.

* [`tar_destroy()`](https://docs.ropensci.org/targets/reference/tar_destroy.html) is by far the most commonly used cleaning function. It removes the `_targets/` folder (or optionally a subfolder in `_targets/`) and all the cloud targets mentioned in the metadata. Use it if you intend to start the pipeline from scratch without any trace of a previous run.
* [`tar_prune()`](https://docs.ropensci.org/targets/reference/tar_prune.html) deletes the data and metadata of all the targets no longer present in your current target script file (default: `_targets.R`). This is useful if you recently worked through multiple changes to your project and are now trying to discard irrelevant data while keeping the results that still matter.
* [`tar_delete()`](https://docs.ropensci.org/targets/reference/tar_delete.html) is more selective than [`tar_destroy()`](https://docs.ropensci.org/targets/reference/tar_destroy.html) and [`tar_prune()`](https://docs.ropensci.org/targets/reference/tar_prune.html). It removes the individual data files of a given set of targets from `_targets/objects/` and cloud buckets while leaving the metadata in `_targets/meta/meta` alone. If you have a small number of data-heavy targets you need to discard to conserve storage, this function can help.
* [`tar_invalidate()`](https://docs.ropensci.org/targets/reference/tar_invalidate.html) is the complement of [`tar_delete()`](https://docs.ropensci.org/targets/reference/tar_delete.html): for the selected targets, it deletes the metadata in `_targets/meta/meta` and does not delete the return values. After invalidation, you will still be able to locate the data files with [`tar_path()`](https://docs.ropensci.org/targets/reference/tar_path.html) and manually salvage them in an emergency. However, [`tar_load()`](https://docs.ropensci.org/targets/reference/tar_load.html) and [`tar_read()`](https://docs.ropensci.org/targets/reference/tar_read.html) will not be able to read the data into R, and subsequent calls to [`tar_make()`](https://docs.ropensci.org/targets/reference/tar_make.html) will attempt to rebuild those targets.
* [`tar_meta_delete()`](https://docs.ropensci.org/targets/reference/tar_meta_delete.html) removes all the files in `_targets/meta/` that `targets` created, not just `_targets/meta/meta`, but also `_targets/meta/progress`, `_targets/meta/process`, and `_targets/meta/crew`. It also deletes the copies of these files in cloud buckets. See the [cloud storage](@cloud-storage) chapter for details.
