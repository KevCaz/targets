# Cloud integration {#cloud}

```{r, message = FALSE, warning = FALSE, echo = FALSE}
knitr::opts_knit$set(root.dir = fs::dir_create(tempfile()))
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", eval = FALSE)
```

```{r, message = FALSE, warning = FALSE, echo = FALSE}
library(targets)
```

`targets` has built-in cloud capabilities to help scale pipelines up and out. Cloud storage solutions are already available, and cloud computing computing solutions are in the works.

Before getting started, please familiarize yourself with the [pricing model](https://aws.amazon.com/pricing/) and [cost management and monitoring tools](https://docs.aws.amazon.com/account-billing/index.html) of [Amazon Web Services](https://aws.amazon.com/). Everything has a cost, from [virtual instances](https://aws.amazon.com/ec2/) to [web API calls](https://docs.aws.amazon.com/#user_guides). [Free tier accounts](https://aws.amazon.com/free/) give you a modest monthly budget for some services for the first year, but it is easy to exceed the limits. Developers of reusable software should consider applying for [promotional credit](https://aws.amazon.com/blogs/opensource/aws-promotional-credits-open-source-projects/) using [this application form](https://pages.awscloud.com/AWS-Credits-for-Open-Source-Projects).

## Compute

Right now, `targets` does not have built-in cloud-based distributed computing support. However, future development plans include seamless integration with [AWS Batch](https://aws.amazon.com/batch/). As a temporary workaround, it is possible to deploy a burstable [SLURM](https://slurm.schedmd.com/documentation.html) cluster using [AWS ParallelCluster](https://aws.amazon.com/hpc/parallelcluster/) and leverage `targets`' [existing support for traditional schedulers](#hpc).

## Storage

`targets` supports cloud storage on a target-by-target basis using [Amazon Simple Storage Service, or S3](https://aws.amazon.com/s3/). After a target completes, the return value is uploaded to a user-defined S3 bucket [S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html). Follow these steps to get started.

### Get started with the Amazon S3 web console

If you do not already have an Amazon Web Services account, sign up for the free tier at <https://aws.amazon.com/free>. Then, follow [these step-by-step instructions](https://docs.aws.amazon.com/AmazonS3/latest/gsg/GetStartedWithS3.html) to practice using Amazon S3 through the web console at <https://console.aws.amazon.com/s3/>.

### Configure your local machine

`targets` uses the [`aws.s3`](https://github.com/cloudyr/aws.s3) package behind the scenes. It is not a strict dependency of `targets`, so you will need to install it yourself.

```{r, eval = FALSE}
install.packages("aws.s3")
```

Next, `aws.s3` needs an access ID, secret access key, and default region. Follow [these steps](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-creds) to generate the keys, and choose a region from [this table of endpoints](https://docs.aws.amazon.com/general/latest/gr/s3.html). Then, open the `.Renviron` file in your home directory with `usethis::edit_r_environ()` and store this information in [special environment variables](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html). Here is an example `.Renviron` file.

```{r, eval = FALSE}
# Example .Renviron file
AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE
AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
AWS_DEFAULT_REGION=us-east-1
```

Restart your R session so the changes take effect. Your keys are sensitive personal information. You can print them in your private console to verify correctness, but otherwise please avoid saving them to any persistent documents other than `.Renviron`.

```{r, eval = FALSE}
Sys.getenv("AWS_ACCESS_KEY_ID")
#> [1] "AKIAIOSFODNN7EXAMPLE"
Sys.getenv("AWS_SECRET_ACCESS_KEY")
#> [1] "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
Sys.getenv("AWS_DEFAULT_REGION")
#> [1] "us-east-1"
```

### Create S3 buckets

Now, you are ready to create one or more [S3 buckets](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html) for your `targets` pipeline. Each pipeline should have its own set of buckets. Create one through the web console or with `aws.s3::put_bucket()`.

```{r, eval = FALSE}
library(aws.s3)
put_bucket("my-test-bucket-25edb4956460647d")
#> [1] TRUE
```

Sign in to <https://s3.console.aws.amazon.com/s3> to verify that the bucket exists.

### Configure the pipeline

To connect your pipeline with S3,

1. Supply your bucket name to `resources` in `tar_option_set()`. To use different buckets for different targets, set `resources` directly in `tar_target()`.
1. Supply AWS-powered storage formats to `tar_option_set()` and/or `tar_target()`. See the [`tar_target()` help file](https://wlandau.github.io/targets/reference/tar_target.html#arguments) for the full list of formats.

Your `_targets.R` file will look something like this.

```{r, eval = FALSE}
# Example _targets.R
library(targets)
tar_option_set(resources = list(bucket = "my-test-bucket-25edb4956460647d"))
write_mean <- function(data) {
  tmp <- tempfile()
  writeLines(as.character(mean(data)), tmp)
  tmp
}
list(
  tar_target(data, rnorm(5), format = "aws_qs"),
  tar_target(mean_file, write_mean(data), format = "aws_file")
)
```

### Run the pipeline

When you run the pipeline above with [`tar_make()`](https://wlandau.github.io/targets/reference/tar_make.html), your local R session computes `rnorm(5)`, saves it to a temporary [`qs`](https://github.com/traversc/qs) file on disk, and then uploads it to a file called `_targets/objects/data` on your S3 bucket. Likewise for `mean_file`, but because the format is `"aws_file"`, you are responsible for supplying the path to the file that gets uploaded to `_targets/objects/mean_file`.

```{r, eval = FALSE}
tar_make()
#> ● run target data
#> ● run target mean_file
```

And of course, your targets stay up to date if you make no changes.

```{r, eval = FALSE}
tar_make()
#> ✓ skip target data
#> ✓ skip target mean_file
#> ✓ Already up to date.
```

### Manage the data

Log into <https://s3.console.aws.amazon.com/s3>. You should see objects `_targets/objects/data` and `_targets/objects/mean_file` in your bucket. To download this data locally, use `tar_read()` and `tar_load()` like before. These functions download the data from the bucket and load it into R.

```{r, eval = FALSE}
tar_read(data)
#> [1] -0.74654607 -0.59593497 -1.57229983  0.40915323  0.02579023
```

The `"aws_file"` format is different from the other AWS-powered formats. `tar_read()` and `tar_load()` download the object to a temporary file and return the path so you can process it yourself.^[Non-"file" AWS formats also download temporary files, but they are immediately discarded after they are read into memory.]

```{r, eval = FALSE}
tar_load(mean_file)
mean_file
#> [1] "_targets/scratch/mean_fileff086e70876d"
```

```{r, eval = FALSE}
readLines(mean_file)
#> [1] "-0.495967480886693"
```

When you are done with these temporary files and the pipeline is no longer running, you can safely remove everything in `_targets/scratch/`.

```{r, eval = FALSE}
unlink("_targets/scratch/", recursive = TRUE)
```

Lastly, if you want to erase the whole project or start over from scratch, consider removing the S3 bucket to avoid incurring storage fees. The easiest way to do this is through the [S3 console](https://s3.console.aws.amazon.com/s3). You can alternatively call `aws.s3::delete_bucket()`, but you have to make sure the bucket is empty first.

```{r, eval = FALSE}
delete_bucket("my-test-bucket-25edb4956460647d")
```
