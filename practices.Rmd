```{r, message = FALSE, warning = FALSE, echo = FALSE}
knitr::opts_knit$set(root.dir = fs::dir_create(tempfile()))
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

```{r, message = FALSE, warning = FALSE, echo = FALSE}
library(targets)
```

# Best practices {#practices}

This chapter describes additional best practices for developing and maintaining `targets`-powered projects.

## How to define good targets

Targets are imperative high-level steps of the workflow that run the work you define in your functions. Like functions, targets generally focus on datasets, analyses, and summaries. The `targets` package automatically skips targets that are already up to date, so you should strive to define targets that maximize time savings. Good targets usually

1. Are large enough to subtract a decent amount of runtime when skipped.
1. Are small enough that some targets can be skipped even if others need to run.
1. Invoke no side effects such as modifications to the global environment. (But targets with `tar_target(format = "file")` can save files.)
1. Return a single value that is
    i. Easy to understand and introspect.
    i. Meaningful to the project.
    i. Easy to save as a file, e.g. with `readRDS()`. Please avoid [non-exportable objects](https://cran.r-project.org/web/packages/future/vignettes/future-4-non-exportable-objects.html) as target return values or global variables.

Regarding the last point above, it is possible to customize the storage format of the target. For details, enter `?tar_target` in the console and scroll down to the description of the `format` argument.

## Dependencies

Adept pipeline construction requires an understanding of dependency detection. To identify the targets and global objects that each target depends on, the `targets` package uses static code analysis with [`codetools`](https://CRAN.R-project.org/package=codetools), and you can emulate this process with `tar_deps()`. Let us look at the dependencies of the `raw_data` target.

```{r}
tar_deps(function() {
  read_csv(raw_data_file, col_types = cols())
})
```

The `raw_data` target depends on target `raw_data_file` because the command for `raw_data` mentions the symbol `raw_data_file`. Similarly, if we were to create a user-defined `read_csv()` function, the `raw_data` target would also depend on `read_csv()` and any other user-defined global functions and objects nested inside `read_csv()`. Changes to any of these objects would cause the `raw_data` target to rerun on the next `tar_make()`.

Not all of the objects from `tar_deps()` actually register as dependencies. When it comes to detecting dependencies, `targets` only recognizes

1. Other targets (such as `raw_data_file`).
1. Functions and objects in the main environment. This environment is almost always the global environment of the R process that runs `_targets.R`, so these dependencies are usually going to be the custom functions and objects you write yourself.

This process excludes many objects from dependency detection. For example, both `{` and `cols()` are excluded because they are defined in the environments of packages (`base` and `readr`, respectively). Functions and objects from packages are ignored unless you supply a package environment to the `envir` argument of `tar_option_set()` when you call it in `_targets.R`, e.g. `tar_option_set(envir = getNamespace("packageName"))`. You should only set `envir` if you write your own package to contain your whole data analysis project.

## Loading and configuring R packages

For most pipelines, it is straightforward to load the R packages that your targets need in order to run.

1. Call `library()` at the top of `_targets.R` to load each package the conventional way, or
2. Name the required packages using the `packages` argument of `tar_option_set()`. 

(2) is often faster, especially for utilities like `tar_visnetwork()`, because it avoids loading packages unless absolutely necessary.

Some package management workflows are more complicated. If your use special configuration with [conflicted](https://github.com/r-lib/conflicted), [`box`](https://klmr.me/box/), [`import`](https://import.rticulate.org/), or similar utility, please do your configuration inside a project-level `.Rprofile` file instead of the `_targets.R` file. In addition, if you use distributed workers inside external containers (Docker, Singularity, AWS AMI, etc.) make sure each container has a copy of this same `.Rprofile` file where the R worker process spawns. This approach is ensures that all [remote workers](#hpc) are configured the same way as the local main process.

## Packages-based invalidation

When it comes time to decide which targets to rerun or skip, the default behavior is to ignore changes to external R packages. Usually, local package libraries do not need to change very often, and it is best to maintain a reproducible project library using [`renv`](https://rstudio.github.io/renv/articles/renv.html).

However, sometimes you may wish to invalidate certain targets based on changes to the contents of certain packages. Example scenarios:

1. You are the developer of a statistical methodology package that serves as the focus of the pipeline.
2. You implement the workflow itself as an R package that contains your [custom functions](#functions).

To track the contents of packages `package1` and `package2`, you must

1. Fully install these packages with `install.packages()` or equivalent. `devtools::load_all()` is insufficient because it does not make the packages available to [parallel workers](#hpc).
2. Write the following in your `_targets.R` file:


```{r, eval = FALSE}
# _targets.R
library(targets)
tar_option_set(
  packages = c("package1", "package2", ...), # `...` is for other packages.
  imports = c("package1", "package2")
)
# Write the rest of _targets.R below.
# ...
```

`packages = c("package1", "package2", ...)` tells `targets` to call `library(package1)`, `library(package2)`, etc. before running each target. `imports = c("package1", "package2")` tells `targets` to dive into the environments of `package1` and `package2` and reproducibly track all the objects it finds. For example, if you define a function `f()` in `package1`, then you should see a function node for `f()` in the graph produced by `tar_visnetwork(targets_only = FALSE)`, and targets downstream of `f()` will invalidate if you install an update to `package1` with a new version of `f()`. The next time you call `tar_make()`, those invalidated targets will automatically rerun.

## Working with tools outside R

`targets` lives and operates entirely within the R interpreter, so working with outside tools is a matter of finding the right functionality in R itself. `system2()` and [`processx`](https://processx.r-lib.org) can invoke system commands outside R, and you can include them in your targets' R commands to run shell scripts, Python scripts, etc. There are also specialized R packages to retrieve data from remote sources and invoke web APIs, including [`rnoaa`](https://github.com/ropensci/rnoaa), [`ots`](https://github.com/ropensci/ots), and [`aws.s3`](https://github.com/cloudyr/aws.s3).

## Monitoring the pipeline

If you are using `targets`, then you probably have an intense computation like Bayesian data analysis or machine learning. These tasks take a long time to run, and it is a good idea to monitor them. If you are running the work on your local machine, you can monitor parallel workers with a utility like `top` or `htop`. If you are using a traditional HPC scheduler like SLURM or SGE, you can check the status of the workers with `squeue`, `qstat`, or similar. But those tools do not always give you a high-level view of what the pipeline has done and which targets are going to run next. For that information, `targets` has options:

1. `tar_progress()` returns a data frame describing the targets that started, finished running successfully, got canceled, or errored out.
1. `tar_visnetwork()` shows this progress information in an interactive `visNetwork` widget. Set `outdated` to `FALSE` to get slightly more detailed progress information.
1. `tar_watch()` launches an Shiny app that automatically refreshes the graph every few seconds. Try it out in the example below.

```{r, eval = FALSE}
# Define an example _targets.R file with a slow pipeline.
library(targets)
tar_script({
  sleep_run <- function(...) {
    Sys.sleep(10)
  }
  list(
    tar_target(settings, sleep_run()),
    tar_target(data1, sleep_run(settings)),
    tar_target(data2, sleep_run(settings)),
    tar_target(data3, sleep_run(settings)),
    tar_target(model1, sleep_run(data1)),
    tar_target(model2, sleep_run(data2)),
    tar_target(model3, sleep_run(data3)),
    tar_target(figure1, sleep_run(model1)),
    tar_target(figure2, sleep_run(model2)),
    tar_target(figure3, sleep_run(model3)),
    tar_target(conclusions, sleep_run(c(figure1, figure2, figure3)))
  )
})

# Launch the app in a background process.
# You may need to refresh the browser if the app is slow to start.
# The graph automatically refreshes every 10 seconds
tar_watch(seconds = 10, outdated = FALSE, targets_only = TRUE)

# Now run the pipeline and watch the graph change.
px <- tar_make()
```

![](./man/figures/tar_watch.png)
`tar_watch_ui()` and `tar_watch_server()` make this functionality available to other apps through a Shiny module.


## Performance

If your pipeline has several thousand targets, functions like `tar_make()`, `tar_outdated()`, and `tar_visnetwork()` may take longer to run. There is an inevitable per-target runtime cost because package needs to check the code and data of each target individually. If this overhead becomes too much, consider batching your work into a smaller group of heavier targets. Using your custom functions, you can make each target perform multiple iterations of a task that was previously given to targets one at a time. For details and an example, please see the discussion on batching at the bottom of the [dynamic branching chapter](#dynamic).

Alternatively, if you see slowness in your project, you can contribute to the package with a profiling study. These contributions are great because they help improve the package. Here are the recommended steps.

1. Install the [`proffer`](https://github.com/r-prof/proffer) R package and its dependencies.
1. Run `proffer::pprof(tar_make(callr_function = NULL))` on your project.
1. When a web browser pops up with `pprof`, select the flame graph and screenshot it.
1. Post the flame graph, along with any code and data you can share, to the [`targets` package issue tracker](https://github.com/ropensci/targets/issues). The maintainer will have a look and try to make the package faster for your use case if speedups are possible.

## Cleaning up

There are [multiple functions](https://docs.ropensci.org/targets/reference/index.html#section-clean) to help you manually remove data or force targets to rerun.

* [`tar_destroy()`](https://docs.ropensci.org/targets/reference/tar_destroy.html) is by far the most commonly used cleaning function. It removes the `_targets/` data store completely, deleting all the results from [`tar_make()`](https://docs.ropensci.org/targets/reference/tar_make.html) except for external files. Use it if you intend to start the pipeline from scratch without any trace of a previous run.
* [`tar_prune()`](https://docs.ropensci.org/targets/reference/tar_prune.html) deletes the data and metadata of all the targets no longer present in your current `_targets.R` file. This is useful if you recently worked through multiple changes to your project and are now trying to discard irrelevant data while keeping the results that still matter.
* [`tar_delete()`](https://docs.ropensci.org/targets/reference/tar_delete.html) is more selective than [`tar_destroy()`](https://docs.ropensci.org/targets/reference/tar_destroy.html) and [`tar_prune()`](https://docs.ropensci.org/targets/reference/tar_prune.html). It removes the individual data files of a given set of targets from `_targets/objects/` while leaving the metadata in `_targets/meta/meta` alone. If you have a small number of data-heavy targets you need to discard to conserve storage, this function can help.
* [`tar_invalidate()`](https://docs.ropensci.org/targets/reference/tar_invalidate.html) is the opposite of [`tar_delete()`](https://docs.ropensci.org/targets/reference/tar_delete.html): for the selected targets, it deletes the metadata in `_targets/meta/meta` but keeps the return values in `_targets/objects/`. After invalidation, you will still be able to locate the data files with [`tar_path()`](https://docs.ropensci.org/targets/reference/tar_path.html) and manually salvage them in an emergency. However, [`tar_load()`](https://docs.ropensci.org/targets/reference/tar_load.html) and [`tar_read()`](https://docs.ropensci.org/targets/reference/tar_read.html) will not be able to read the data into R, and subesequent calls to [`tar_make()`](https://docs.ropensci.org/targets/reference/tar_make.html) will attempt to rebuild those targets.
