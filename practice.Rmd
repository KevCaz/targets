```{r, message = FALSE, warning = FALSE, echo = FALSE}
knitr::opts_knit$set(root.dir = fs::dir_create(tempfile()))
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

```{r, message = FALSE, warning = FALSE, echo = FALSE}
library(targets)
```

# Best practices for `targets`-powered projects {#practice}

The [`targets`](https://github.com/wlandau/targets) package espouses a clean, organized, modular, function-oriented style of programming. Many data scientists and researchers initially find this style uncomfortable, not only because it can be unfamiliar, but also because may requires us to postpone our thinking about actual results and conclusions while we focus on the process of creating a new workflow. However, this up-front cost pays off. Data science projects become easier to extend, maintain, and share with collaborators, and they have longer shelf lives than otherwise. In other words, we increase reproducibility while decreasing technical debt. Once these patterns become habits, the pace of development quickens while still lending more trust and credibility to the results.

## A move away from imperative scripts 

Traditional data analysis projects are usually coded as collections of imperative scripts, often with numeric prefixes.

```
01-data.R
02-preprocess.R
03-analysis.R
04-summaries.R
```

This approach does not scale well in large, cumbersome projects. Code gets tangled, messy, and difficult to test. Results fail to keep pace with rapid development, and reproducibility suffers.

## Functions

Functions are the most idiomatic way to express the modularity we need. They are custom shorthand that makes code easier to read and easier to test. A good pure function has

1. An informative name that describes what the function does.
1. Input arguments that are easy to generate.
1. A return value that is convenient to introspect and meaningful to the project.

(1) is the shorthand, and (2) and (3) make testing easier. In data science workflows, we typically write individual functions to

1. Retrieve or generate a dataset.
1. Preprocess a dataset.
1. Fit a model to a preprocessed dataset.
1. Generate machine-readable summaries of the model fit (tables of summary statistics).
1. Generate *human*-readable summaries of the model fit (plots and reports).

Each of the functions in (1)-(5) typically calls custom inner functions to increase modularity even further. For examples, see the `functions.R` files in the example projects [listed here](https://wlandau.github.io/targets/index.html#examples).

## Packages

When it comes time to decide which targets to rerun or skip, the default behavior is to ignore changes to external R packages. Usually, local package libraries do not need to change very often, and it is best to maintain a reproducible project library using [`renv`](https://rstudio.github.io/renv/articles/renv.html).

However, there are some situations where it makes sense to watch a package for changes. For example, you could be in the middle of developing a methodology package that serves as the focus of the pipeline, or you could implement the workflow itself as a package. In either case, you can tell `targets` to track changes using the `imports` argument to `tar_option_set()`. If you write `tar_option_set(imports = "package1")` in `_targets.R`, then `targets` will analyze the R objects in `package1` and automatically rerun the dependent targets when these objects change. These tracked objects include unexported functions internal to the package. You can track multiple packages this way, e.g. `tar_option_set(imports = c("package1", "package2"))`. In this case, the contents of `package1` override those of `package2` when there are name conflicts. Likewise, `tar_option_get("envir")` (usually the global environment) overrides both.

## Targets

Targets are imperative high-level steps of the workflow that run the work you define in your functions. Like functions, targets generally focus on datasets, analyses, and summaries. The `targets` package automatically skips targets that are already up to date, so you should strive to define targets that maximize time savings. Good targets usually

1. Are large enough to subtract a decent amount of runtime when skipped.
1. Are small enough that some targets can be skipped even if others need to run.
1. Invoke no side effects such as modifications to the global environment. (But targets with `tar_target(format = "file")` can save files.)
1. Return a single value that is
    i. Easy to understand and introspect.
    i. Meaningful to the project.
    i. Easy to save as a file, e.g. with `readRDS()`.

Regarding the last point above, it is possible to customize the storage format of the target. For details, enter `?tar_target` in the console and scroll down to the description of the `format` argument.

## Dependencies

Adept pipeline construction requires an understanding of dependency detection. To identify the targets and global objects that each target depends on, the `targets` package uses static code analysis with [`codetools`](https://CRAN.R-project.org/package=codetools), and you can emulate this process with `tar_deps()`. Let us look at the dependencies of the `raw_data` target.

```{r}
tar_deps(function() {
  read_csv(raw_data_file, col_types = cols())
})
```

The `raw_data` target depends on target `raw_data_file` because the command for `raw_data` mentions the symbol `raw_data_file`. Similarly, if we were to create a user-defined `read_csv()` function, the `raw_data` target would also depend on `read_csv()` and any other user-defined global functions and objects nested inside `read_csv()`. Changes to any of these objects would cause the `raw_data` target to rerun on the next `tar_make()`.

Not all of the objects from `tar_deps()` actually register as dependencies. When it comes to detecting dependencies, `targets` only recognizes

1. Other targets (such as `raw_data_file`).
1. Functions and objects in the main environment. This environment is almost always the global environment of the R process that runs `_targets.R`, so these dependencies are usually going to be the custom functions and objects you write yourself.

This process excludes many objects from dependency detection. For example, both `{` and `cols()` are excluded because they are defined in the environments of packages (`base` and `readr`, respectively). Functions and objects from packages are ignored unless you supply a package environment to the `envir` argument of `tar_option_set()` when you call it in `_targets.R`, e.g. `tar_option_set(envir = getNamespace("packageName"))`. You should only set `envir` if you write your own package to contain your whole data analysis project.

## Debugging

If one of your targets fails, first look up the error message in `tar_meta()`. If that does not help, try one of the following techniques.

### Workspaces

Workspaces are special lightweight reference files that allow `tar_workspace()` to recreate the runtime environment of a target. This lets you troubleshoot issues outside the pipeline in an interactive session. There are two ways to save a workspace file:

1. Set `error = "workspace"` in `tar_option_set()` or `tar_target()`. Then, `tar_make()` and friends will save a workspace file for every target that errors out.
1. In the `workspaces` argument of `tar_option_set()`, specify the targets for which you want to save workspaces. Then, run `tar_make()` or similar. A workspace file will be saved for each existing target, regardless of whether the target runs or gets skipped in the pipeline.

Here is an example of (1).

```{r, eval = FALSE}
# _targets.R file:
options(tidyverse.quiet = TRUE)
library(targets)
library(tidyverse)
options(crayon.enabled = FALSE)
tar_option_set(error = "workspace")
f <- function(x) {
  stopifnot(x < 4)
}
tar_pipeline(
  tar_target(x, seq_len(4)),
  tar_target(y, f(x), pattern = map(x)) # The branching chapter describes patterns.
)
```

```{r, eval = FALSE}
# R console:
tar_make()
#> ● run target x
#> ● run branch y_29239c8a
#> ● run branch y_7cc32924
#> ● run branch y_bd602d50
#> ● run branch y_05f206d7
#> x error branch y_05f206d7
#> ● save workspace y_05f206d7
#> Error : x < 4 is not TRUE .
#> Error: callr subprocess failed: x < 4 is not TRUE .
```

One of the `y_*******` targets errored out.

```{r, eval = FALSE}
failed <- tar_meta(fields = error) %>%
  na.omit() %>%
  pull(name)

print(failed)
#> [1] "y_05f206d7"
```

`tar_workspace()` reads the special metadata in the workspace file and then loads the target's dependencies from various locations in `_targets/objects` and/or the [cloud](#cloud). It also sets the random number generator seed to the seed of the target, loads the required packages, and runs `_targets.R` to load other global object dependencies such as functions. 

```{r, eval = FALSE}
tar_workspace(y_05f206d7)
```

We now have the dependencies of `y_05f206d7` in memory, which allows you to try out any failed function calls interactively.

```{r, eval = FALSE}
print(x)
#> [1] 4
f(x)
#> Error in f(x) : x < 4 is not TRUE
```


In addition, current random number generator seed (`.Random.seed`) is also the value `y_05f206d7` started with.

Remove all workspace files with:

```{r, eval = FALSE}
tar_destroy(destroy = "workspaces")
```

### Interactive debugging

Interactive debugging is a way to troubleshoot targets without saving or loading workspace files. Here are the steps.

1. In `_targets.R`, write a call to `tar_option_set()` with `debug` equal to the target name. Consider also setting `cue` equal to `tar_cue(mode = "never")` so `tar_make()` reaches the target you want to debug more quickly.
1. Launch a fresh clean new interactive R session with the `_targets.R` script in your working directory.
1. Run `targets::tar_make()` (or `targets::tar_make_clustermq()`, or `targets::tar_make_future()`) with `callr_function = NULL`.
1. When `targets` reaches the target you selected to debug, your R session will start an interactive debugger, and you should see `Browse[1]>` in your console. Run `targets::tar_name()` to verify that you are debugging the correct target.
1. Interactively run any R code that helps you troubleshoot the problem.^[Because of the way `targets` manages environments, `ls()` will not report all your global objects, but you should still be able to access them.]

To try it out yourself, write the following `_targets.R` file.

```{r, eval = FALSE}
# _targets.R
library(targets)
tar_option_set(debug = "b")
f <- function(x) x + 1
tar_pipeline(
  tar_target(a, 1),
  tar_target(b, f(a))
)
```

Then, call `tar_make(callr_function = NULL)` to drop into a debugger at the command of `b`. 

```{r, eval = FALSE}
# R console
tar_make(callr_function = NULL)
#> ● run target a
#> ● run target b
#> Called from: eval(expr, envir)
Browse[1]>
```

When the debugger launches, run `targets::tar_name()` to confirm you are running the correct target.

```{r, eval = FALSE}
Browse[1]> targets::tar_name()
#> [1] "b"
```

In the debugger, the dependency targets of `b` are available in the current environment, and the global objects and functions are available in the parent environment.

```{r, eval = FALSE}
Browse[1]> ls()
#> [1] "a"
Browse[1]> a
#> [1] 1
Browse[1]> ls(parent.env(environment()))
#> [1] "f"
Browse[1]> f(1)
#> [1] 2
```

Press `n` or `c` to advance to the next line or breakpoint.

```{r, eval = FALSE}
Browse[1]> n
>
```

For more on debugging R code, visit [this page](https://rstats.wtf/debugging-r-code.html).

## Performance

If your pipeline has several thousand targets, functions like `tar_make()`, `tar_outdated()`, and `tar_visnetwork()` may take longer to run. There is an inevitable per-target runtime cost because package needs to check the code and data of each target individually. If this overhead becomes too much, consider batching your work into a smaller group of heavier targets. Using your custom functions, you can make each target perform multiple iterations of a task that was previously given to targets one at a time. For details and an example, please see the discussion on batching at the bottom of the [dynamic branching chapter](#dynamic).

Alternatively, if you see slowness in your project, you can contribute to the package with a profiling study. These contributions are great because they help improve the package. Here are the recommended steps.

1. Install the [`proffer`](https://github.com/r-prof/proffer) R package and its dependencies.
1. Run `proffer::pprof(tar_make(callr_function = NULL))` on your project.
1. When a web browser pops up with `pprof`, select the flame graph and screenshot it.
1. Post the flame graph, along with any code and data you can share, to the [`targets` package issue tracker](https://github.com/wlandau/targets/issues). The maintainer will have a look and try to make the package faster for your use case if speedups are possible.
