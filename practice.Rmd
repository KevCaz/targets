```{r, message = FALSE, warning = FALSE, echo = FALSE}
knitr::opts_knit$set(root.dir = fs::dir_create(tempfile()))
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

```{r, message = FALSE, warning = FALSE, echo = FALSE}
library(targets)
```

# Best practices for `targets`-powered projects {#practice}

The [`targets`](https://github.com/wlandau/targets) package espouses a clean, organized, modular, function-oriented style of programming. Many data scientists and researchers initially find this style uncomfortable, not only because it can be unfamiliar, but also because may requires us to postpone our thinking about actual results and conclusions while we focus on the process of creating a new workflow. However, this up-front cost pays off. Data science projects become easier to extend, maintain, and share with collabotors, and they have longer shelf lives than otherwise. In other words, we increase reproducibility while decreasing technical debt. Once these patterns become habits, the pace of development quickens while still lending more trust and credibility to the results.

## A move away from imperative scripts 

Traditional data analysis projects are usually coded as collections of imperative scripts, often with numeric prefixes.

```
01-data.R
02-preprocess.R
03-analysis.R
04-summaries.R
```

This approach does not scale well in large, cumbersome projects. Code gets tangled, messy, and difficult to test. Results fail to keep pace with rapid development, and reproducibility suffers.

## Functions

Functions are the most idiomatic way to express the modularity we need. They are custom shorthand that makes code easier to read and easier to test. A good pure function has

1. An informative name that describes what the function does.
1. Input arguments that are easy to generate.
1. A return value that is convenient to introspect and meaningful to the project.

(1) is the shorthand, and (2) and (3) make testing easier. In data science workflows, we typically write individual functions to

1. Retrieve or generate a dataset.
1. Preprocess a dataset.
1. Fit a model to a preprocessed dataset.
1. Generate machine-readable summaries of the model fit (tables of summary statistics).
1. Generate *human*-readable summaries of the model fit (plots and reports).

Each of the functions in (1)-(5) typically calls custom inner functions to increase modularity even further. For examples, see the `functions.R` files in the example projects [listed here](https://wlandau.github.io/targets/index.html#examples).

## Targets

Targets are imperative high-level steps of the workflow that run the work you define in your functions. Like functions, targets generally focus on datasets, analyses, and summaries. The `targets` package automatically skips targets that are already up to date, so you should strive to define targets that maximize time savings. Good targets usually

1. Are large enough to subtract a decent amount of runtime when skipped.
1. Are small enough that some targets can be skipped even if others need to run.
1. Invoke no side effects such as modifications to the global environment. (But targets with `tar_target(format = "file")` can save files.)
1. Return a single value that is
    i. Easy to understand and introspect.
    i. Meaningful to the project.
    i. Easy to save as a file, e.g. with `readRDS()`.

Regarding the last point above, it is possible to customize the storage format of the target. For details, enter `?tar_target` in the console and scroll down to the description of the `format` argument.

## Performance

If your pipeline has more than a few thousand targets, functions like `tar_make()`, `tar_outdated()`, and `tar_vis_drake_graph()` will take longer to run. There is an inevitable per-target runtime cost because package needs to check the code and data of each target individually. If this overhead becomes too much, consider batching your work into a smaller group of heavier targets. Using your custom functions, you can make each target perform multiple iterations of a task that was previously given to targets one at a time.

Alternatively, if you see slowness in your project, you can contribute to the package with a profiling study. These contributions are great because they help improve the package. Here are the recommended steps.

1. Install the [`proffer`](https://github.com/r-prof/proffer) R package and its dependencies.
1. Run `proffer::pprof(tar_make(callr_function = NULL))` on your project.
1. When a web browser pops up with `pprof`, select the flame graph and screenshot it.
1. Post the flame graph, along with any code and data you can share, to the [`targets` package issue tracker](https://github.com/wlandau/targets/issues). The maintainer will have a look and try to make the package faster for your use case if speedups are possible.
