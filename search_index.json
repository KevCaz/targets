[["index.html", "The targets R Package User Manual Chapter 1 Introduction 1.1 Motivation 1.2 Pipeline tools 1.3 The targets package 1.4 About this manual 1.5 What about drake?", " The targets R Package User Manual Will Landau Copyright Eli Lilly and Company Chapter 1 Introduction The targets package is a Make-like pipeline tool for Statistics and data science in R. With targets, you can maintain a reproducible workflow without repeating yourself. targets learns how your pipeline fits together, skips costly runtime for tasks that are already up to date, runs only the necessary computation, supports implicit parallel computing, abstracts files as R objects, and shows tangible evidence that the results match the underlying code and data. 1.1 Motivation Data analysis can be slow. A round of scientific computation can take several minutes, hours, or even days to complete. After it finishes, if you update your code or data, your hard-earned results may no longer be valid. Unchecked, this invalidation creates chronic Sisyphean loop: Launch the code. Wait while it runs. Discover an issue. Restart from scratch. 1.2 Pipeline tools Pipeline tools like GNU Make break the cycle. They watch the dependency graph of the whole workflow and skip steps, or “targets”, whose code, data, and upstream dependencies have not changed since the last run of the pipeline. When all targets are up to date, this is evidence that the results match the underlying code and data, which helps us trust the results and confirm the computation is reproducible. 1.3 The targets package Unlike most pipeline tools, which are language agnostic or Python-focused, the targets package allows data scientists and researchers to work entirely within R. targets implicitly nudges users toward a clean, function-oriented programming style that fits the intent of the R language and helps practitioners maintain their data analysis projects. 1.4 About this manual This manual is a step-by-step user guide to targets. It walks through basic usage, explains best practices for writing code and managing projects, dives deep into advanced features like high-performance computing, and helps drake users transition to targets. See the documentation website for most other major resources, including installation instructions, links to example projects, and a reference page with all user-side functions. 1.5 What about drake? The drake is an older R-focused pipeline tool, and targets is drake’s long-term successor. There is a special chapter to explain why targets was created, what this means for drake’s future, advice for drake users transitioning to the targets, and the main technical advantages of targets over drake. "],["walkthrough.html", "Chapter 2 Walkthrough 2.1 About this example 2.2 File structure 2.3 Target script file 2.4 Inspect the pipeline 2.5 Run the pipeline 2.6 Changes 2.7 Read metadata", " Chapter 2 Walkthrough This chapter walks through a short example of a targets-powered data analysis project. The source code is available at https://github.com/wlandau/targets-four-minutes, and you can visit https://rstudio.cloud/project/3946303 to try out the code in a web browser (no download or installation required). The documentation website links to other examples. The contents of the chapter are also explained in a four-minute video tutorial: 2.1 About this example The goal of this short analysis is to assess the relationship among ozone and temperature in base R’s airquality dataset. We track a data file, prepare a dataset, fit a model, and plot the model against the data. 2.2 File structure The file structure of the project looks like this. ├── _targets.R ├── data.csv └── R/ └──── functions.R data.csv contains the data we want to analyze. Ozone,Solar.R,Wind,Temp,Month,Day 36,118,8.0,72,5,2 12,149,12.6,74,5,3 ... R/functions.R contains our custom user-defined functions. (See the functions chapter for a discussion of function-oriented workflows.) # R/functions.R get_data &lt;- function(file) { read_csv(file, col_types = cols()) %&gt;% filter(!is.na(Ozone)) } fit_model &lt;- function(data) { lm(Ozone ~ Temp, data) %&gt;% coefficients() } plot_model &lt;- function(model, data) { ggplot(data) + geom_point(aes(x = Temp, y = Ozone)) + geom_abline(intercept = model[1], slope = model[2]) } 2.3 Target script file Whereas files data.csv and functions.R are typical user-defined components of a project-oriented workflow, the target script file _targets.R file is special. Every targets workflow needs a target script file to configure and define the pipeline.1 The use_targets() function in targets version &gt;= 0.12.0 creates an initial target script for you to fill in. Ours looks like this: # _targets.R file library(targets) source(&quot;R/functions.R&quot;) tar_option_set(packages = c(&quot;readr&quot;, &quot;dplyr&quot;, &quot;ggplot2&quot;)) list( tar_target(file, &quot;data.csv&quot;, format = &quot;file&quot;), tar_target(data, get_data(file)), tar_target(model, fit_model(data)), tar_target(plot, plot_model(model, data)) ) All target script files have these requirements. Load the packages required for configuration, e.g. targets itself.2 Load your custom functions and small input objects into the R session: in our case, with source(\"R/functions.R\"). Use tar_option_set() to declare the packages required for analysis, as well as other settings such as the default storage format. Write the pipeline at the bottom of _targets.R. A pipeline is a list of target objects, which you can create with tar_target(). Each target is a step of the analysis. It looks and feels like a variable in R, but it runs reproducibly and stores a value in _targets/objects/ when we run the pipeline with tar_make(). Set additional arguments in tar_target() as needed. For example, format = \"file\" says that the target is an external file, and changes to the contents of the file will invalidate the target (i.e. cause it to rerun).3 2.4 Inspect the pipeline Before you run the pipeline for real, it is best to check for obvious errors. tar_manifest() lists verbose information about each target. tar_manifest(fields = all_of(&quot;command&quot;)) #&gt; # A tibble: 4 × 2 #&gt; name command #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 file &quot;\\&quot;data.csv\\&quot;&quot; #&gt; 2 data &quot;get_data(file)&quot; #&gt; 3 model &quot;fit_model(data)&quot; #&gt; 4 plot &quot;plot_model(model, data)&quot; tar_visnetwork() displays the dependency graph. It should show a natural progression of work from left to right as the pipeline progresses, and it targets and functions that depend on one another should have directed edges to identify the dependency relationships.4 Dependency relationships are automatically detected using static code analysis, and the order of tar_target() calls in the target list does not matter at all. tar_visnetwork() 2.5 Run the pipeline tar_make() runs the pipeline. It creates a reproducible new external R process which then reads the target script and runs the correct targets in the correct order.5 tar_make() #&gt; • start target file #&gt; • built target file #&gt; • start target data #&gt; • built target data #&gt; • start target model #&gt; • built target model #&gt; • start target plot #&gt; • built target plot #&gt; • end pipeline: 0.707 seconds The output of the pipeline is saved to the _targets/ data store, and you can read the output with tar_read() (see also tar_load()). tar_read(plot) The next time you run tar_make(), targets skips everything that is already up to date, which saves a lot of time in large projects with long runtimes. tar_make() #&gt; ✔ skip target file #&gt; ✔ skip target data #&gt; ✔ skip target model #&gt; ✔ skip target plot #&gt; ✔ skip pipeline: 0.08 seconds You can use tar_visnetwork() and tar_outdated() to check ahead of time which targets are up to date. tar_visnetwork() tar_outdated() #&gt; character(0) 2.6 Changes The targets package notices when you make changes to code and data, and those changes affect which targets rerun and which targets are skipped.6 2.6.1 Change code If you change one of your functions, the targets that depend on it will no longer be up to date, and tar_make() will rebuild them. For example, let’s increase the font size of the plot. # Edit functions.R... plot_model &lt;- function(model, data) { ggplot(data) + geom_point(aes(x = Temp, y = Ozone)) + geom_abline(intercept = model[1], slope = model[2]) + theme_gray(24) # Increased the font size. } targets detects the change. plot is “outdated” (i.e. invalidated) and the others are still up to date. tar_visnetwork() tar_outdated() #&gt; [1] &quot;plot&quot; Thus, tar_make() reruns plot and nothing else.7 tar_make() #&gt; ✔ skip target file #&gt; ✔ skip target data #&gt; ✔ skip target model #&gt; • start target plot #&gt; • built target plot #&gt; • end pipeline: 1.144 seconds Sure enough, we have a new plot. tar_read(plot) 2.6.2 Change data If we change the data file data.csv, targets notices the change. This is because file is a file target (i.e. with format = \"file\" in tar_target()), and the return value from last tar_make() identified \"data.csv\" as the file to be tracked for changes. Let’s try it out. Below, let’s use only the first 100 rows of the airquality dataset. write_csv(head(airquality, n = 100), &quot;data.csv&quot;) Sure enough, raw_data_file and everything downstream is out of date, so all our targets are outdated. tar_visnetwork() tar_outdated() #&gt; [1] &quot;file&quot; &quot;plot&quot; &quot;data&quot; &quot;model&quot; tar_make() #&gt; • start target file #&gt; • built target file #&gt; • start target data #&gt; • built target data #&gt; • start target model #&gt; • built target model #&gt; • start target plot #&gt; • built target plot #&gt; • end pipeline: 1.951 seconds 2.7 Read metadata To read the build progress of your targets while tar_make() is running, you can open a new R session and run tar_progress(). It reads the flat file in _targets/meta/progress and tells you which targets are running, built, errored, or cancelled. tar_progress() #&gt; # A tibble: 4 × 2 #&gt; name progress #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 file built #&gt; 2 data built #&gt; 3 model built #&gt; 4 plot built Likewise, the tar_meta() function reads _targets/meta/meta and tells you high-level information about the target’s settings, data, and results. The warnings, error, and traceback columns give you diagnostic information about targets with problems. tar_meta() #&gt; # A tibble: 7 × 18 #&gt; name type data command depend seed path time size bytes #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;lis&gt; &lt;dttm&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 data stem 52b5… 4525b9… 60955… 1.59e9 &lt;chr&gt; 2022-06-30 19:13:14 6754… 1004 #&gt; 2 file stem 2b16… 0d4b75… ef46d… -1.30e9 &lt;chr&gt; 2022-06-30 19:13:05 9fb6… 1884 #&gt; 3 fit_… func… 9826… &lt;NA&gt; &lt;NA&gt; NA &lt;chr&gt; NA &lt;NA&gt; NA #&gt; 4 get_… func… caaa… &lt;NA&gt; &lt;NA&gt; NA &lt;chr&gt; NA &lt;NA&gt; NA #&gt; 5 model stem bc77… f3249b… f4bf1… 1.82e9 &lt;chr&gt; 2022-06-30 19:13:14 c4a6… 111 #&gt; 6 plot stem 7eac… 1239c9… 3d5d9… 1.93e9 &lt;chr&gt; 2022-06-30 19:13:14 2410… 40439 #&gt; 7 plot… func… 53e9… &lt;NA&gt; &lt;NA&gt; NA &lt;chr&gt; NA &lt;NA&gt; NA #&gt; # … with 8 more variables: format &lt;chr&gt;, repository &lt;chr&gt;, iteration &lt;chr&gt;, #&gt; # parent &lt;lgl&gt;, children &lt;list&gt;, seconds &lt;dbl&gt;, warnings &lt;lgl&gt;, error &lt;lgl&gt; The _targets/meta/meta file is critically important. Although targets can still work properly if files are missing from _targets/objects, the pipeline will error out if _targets/meta/meta is corrupted. If tar_meta() works, the project should be fine. By default, the target script is a file called _targets.R in the project’s root directory. However, in targets version 0.5.0.9000 and above, you can set the target script file path to something other than _targets.R. You can either set the path persistently for your project using tar_config_set(), or you can set it temporarily for an individual function call using the script argument of tar_make() and related functions.↩︎ target scripts created with tar_script() automatically insert a library(targets) line at the top by default.↩︎ format = \"file\" allows you to track multiple files and directories.↩︎ If you have hundreds of targets, then tar_visnetwork() may be slow. If that happens, consider temporarily commenting out some targets in _targets.R just for visualization purposes.↩︎ In targets version 0.3.1.9000 and above, you can set the path of the local data store to something other than _targets/. A project-level _targets.yaml file keeps track of the path. Functions tar_config_set() and tar_config_get() can help.↩︎ Internally, special rules called “cues” decide whether a target reruns. The tar_cue() function lets you suppress some of these cues, and the tarchetypes package supports nuanced cue factories and target factories to further customize target invalidation behavior. The tar_cue() function documentation explains cues in detail, as well as specifics on how targets detects changes to upstream dependencies.↩︎ We would see similar behavior if we changed the R expressions in any tar_target() calls in the target script file.↩︎ "],["debugging.html", "Chapter 3 Debugging 3.1 Error messages 3.2 Environment browser 3.3 The debug option 3.4 Workspaces 3.5 Tradeoffs", " Chapter 3 Debugging Under the default settings, conventional debugging tools such as traceback(), debug(), browser() and other popular debugging techniques may not provide useful information on why a given target is failing. Not even .Last.error or .Last.error.trace from callr are automatically informative. However, targets provides its own extensive support for debugging and troubleshooting errors. This chapter demonstrates the techniques. 3.1 Error messages The metadata in _targets/meta/meta contains error messages and warning messages from when each target last ran. tar_meta() can retrieve these clues. tar_meta(fields = error, complete_only = TRUE) tar_meta(fields = warnings, complete_only = TRUE) 3.2 Environment browser By default, tar_make() runs in a reproducible background process, so debug() and browser() do not interrupt the pipeline. To use the environment browser your the main session, restart R and supply callr_function = NULL to tar_make(). callr_function = NULL risks invalidating your hard-earned results, so only use it after you have just restarted R and only use it for debugging. # Restart R first... debug(custom_function_called_from_a_target) tar_make(names = target_to_debug, callr_function = NULL) #&gt; debugging in: custom_function_called_from_a_target() #&gt; Browse[1]&gt; 3.3 The debug option targets has a more convenient way to launch the environment browser from inside a target: In the target script file (default: _targets.R) write a call to tar_option_set() with debug equal to the target name. Launch a fresh clean new interactive R session with the target script file (default: _targets.R) script in your working directory. Run targets::tar_make() (or targets::tar_make_clustermq(), or targets::tar_make_future()) with callr_function = NULL. If you are using targets version 0.5.0.9000 or above, consider also setting shortcut to TRUE and supplying the target name to names.8 This allows tar_make() to reach the desired target more quickly. When targets reaches the target you selected to debug, your R session will start an interactive debugger, and you should see Browse[1]&gt; in your console. Run targets::tar_name() to verify that you are debugging the correct target. Interactively run any R code that helps you troubleshoot the problem. For example, if the target invokes a function f(), enter debug(f) and then c to immediately enter the function’s calling environment where all its arguments are defined. To try it out yourself, write the following target script file file. # _targets.R file library(targets) tar_option_set(debug = &quot;b&quot;) f &lt;- function(x, another_arg = 123) x + another_arg list( tar_target(a, 1), tar_target(b, f(a)) ) Then, call tar_make(callr_function = NULL) to drop into a debugger at the command of b. # R console tar_make(callr_function = NULL, names = any_of(&quot;b&quot;), shortcut = TRUE) #&gt; ● run target b #&gt; Called from: eval(expr, envir) Browse[1]&gt; When the debugger launches, run targets::tar_name() to confirm you are running the correct target. Browse[1]&gt; targets::tar_name() #&gt; [1] &quot;b&quot; In the debugger, the dependency targets of b are available in the current environment, and the global objects and functions are available in the parent environment. Browse[1]&gt; ls() #&gt; [1] &quot;a&quot; Browse[1]&gt; a #&gt; [1] 1 Browse[1]&gt; ls(parent.env(environment())) #&gt; [1] &quot;f&quot; Browse[1]&gt; f(1) #&gt; [1] 124 Enter debug(f) to debug the function f(), and press c to enter the function’s calling environment where another_arg is defined. Browse[1]&gt; debug(f) Browse[1]&gt; c #&gt; debugging in: f(a) #&gt; debug at _targets.R#3: x + another_arg Browse[2]&gt; ls() #&gt; [1] &quot;another_arg&quot; &quot;x&quot; Browse[2]&gt; another_arg #&gt; [1] 123 3.4 Workspaces Workspaces are a persistent alternative to the environment browser. A workspace is a special lightweight reference file that lists the elements of a target’s runtime environment. Using tar_workspace(), you can recover a target’s workspace and locally debug it even if the pipeline is not running. If you tell targets to record workspaces in advance, you can preempt errors and debug later at your convenience. To enable workspaces, use the workspace_on_error and workspaces arguments of tar_option_set(). These arguments set the conditions under which workspace files are saved. For example, tar_option_set(workspace_on_error = TRUE, workspaces = c(\"x\", \"y\")) tells tar_make() and friends to save a workspace for a target named x, a target named y, and every target that throws and error. Example in a pipeline: # _targets.R file: options(tidyverse.quiet = TRUE) library(targets) options(crayon.enabled = FALSE) tar_option_set(workspace_on_error = TRUE, packages = &quot;tidyverse&quot;) f &lt;- function(arg, value, ...) { stopifnot(arg &lt; 4) } list( tar_target(x, seq_len(4)), tar_target( y, f(arg = x, value = &quot;succeeded&quot;, a = 1, b = 2, key = &quot;my_api_key&quot;), pattern = map(x) # The branching chapter describes patterns. ) ) # R console: tar_make() #&gt; ● run target x #&gt; ● run branch y_29239c8a #&gt; ● run branch y_7cc32924 #&gt; ● run branch y_bd602d50 #&gt; ● run branch y_05f206d7 #&gt; x error branch y_05f206d7 #&gt; ● save workspace y_05f206d7 #&gt; Error : x &lt; 4 is not TRUE . #&gt; Error: callr subprocess failed: x &lt; 4 is not TRUE . One of the y_******* targets errored out. failed &lt;- tar_meta(fields = error) %&gt;% na.omit() %&gt;% pull(name) print(failed) #&gt; [1] &quot;y_05f206d7&quot; tar_workspace() reads the special metadata in the workspace file and then loads the target’s dependencies from various locations in _targets/objects and/or the cloud. It also sets the random number generator seed to the seed of the target, loads the required packages, and runs the target script file (default: _targets.R) to load other global object dependencies such as functions. tar_workspace(y_05f206d7) We now have the dependencies of y_05f206d7 in memory, which allows you to try out any failed function calls in your local R session. 9 10 print(x) #&gt; [1] 4 f(arg = 0, value = &quot;my_value&quot;, a = 1, b = 2, key = &quot;my_api_key&quot;) #&gt; [1] &quot;my_value&quot; f(arg = x, value = &quot;my_value&quot;, a = 1, b = 2, key = &quot;my_api_key&quot;) #&gt; Error in f(x) : x &lt; 4 is not TRUE Keep in mind that that although the dependencies of y_05f206d7 are in memory, the arguments of f() are not. arg #&gt; Error: object &#39;arg&#39; not found value #&gt; Error: object &#39;value&#39; not found The workspace also has a useful traceback, and you can retrieve it with tar_traceback(). The last couple lines of the traceback are unavoidably cryptic, but they do sometimes contain useful information. tar_traceback(y_05f206d7, characters = 77) #&gt; [1] &quot;f(arg = x, value = \\&quot;succeeded\\&quot;, a = 1, b = 2, key = \\&quot;my_api_key\\&quot;)&quot; #&gt; [2] &quot;stopifnot(arg &lt; 4)&quot; #&gt; [3] &quot;stop(simpleError(msg, call = if (p &lt;- sys.parent(1)) sys.call(p)))&quot; #&gt; [4] &quot;(function (condition) \\n{\\n state$error &lt;- build_message(condition)\\n stat&quot; 3.5 Tradeoffs For small to medium-sized workloads, the environment browser and the debug option are usually the best choices. These techniques immediately direct control to prewritten function calls and get you as close to the error as possible. However, this may not always be feasible in large distributed workloads, e.g. tar_make_clustermq(), where most of your targets are not even running on the same computer as your main R process. For those complicated situations where it is not possible to access the R interpreter, workspaces are ideal because they store a persistent reproducible runtime state that you can recover locally. In the case of dynamic branching, names does not accept individual branch names, but you can still supply the name of the overarching dynamic target.↩︎ In addition, current random number generator seed (.Random.seed) is also the value y_05f206d7 started with.↩︎ When you are finished debugging, you can remove all workspace files with tar_destroy(destroy = \"workspaces\").↩︎ "],["performance.html", "Chapter 4 Performance 4.1 Monitoring the pipeline 4.2 Performance", " Chapter 4 Performance This chapter explains how to monitor the progress of your pipeline and troubleshoot performance issues. 4.1 Monitoring the pipeline If you are using targets, then you probably have an intense computation like Bayesian data analysis or machine learning. These tasks take a long time to run, and it is a good idea to monitor them. Here are some options built directly into targets: tar_poll() continuously refreshes a text summary of runtime progress in the R console. Run it in a new R session at the project root directory. (Only supported in targets version 0.3.1.9000 and higher.) tar_visnetwork(), tar_progress_summary(), tar_progress_branches(), and tar_progress() show runtime information at a single moment in time. tar_watch() launches an Shiny app that automatically refreshes the graph every few seconds. Try it out in the example below. # Define an example target script file with a slow pipeline. library(targets) tar_script({ sleep_run &lt;- function(...) { Sys.sleep(10) } list( tar_target(settings, sleep_run()), tar_target(data1, sleep_run(settings)), tar_target(data2, sleep_run(settings)), tar_target(data3, sleep_run(settings)), tar_target(model1, sleep_run(data1)), tar_target(model2, sleep_run(data2)), tar_target(model3, sleep_run(data3)), tar_target(figure1, sleep_run(model1)), tar_target(figure2, sleep_run(model2)), tar_target(figure3, sleep_run(model3)), tar_target(conclusions, sleep_run(c(figure1, figure2, figure3))) ) }) # Launch the app in a background process. # You may need to refresh the browser if the app is slow to start. # The graph automatically refreshes every 10 seconds tar_watch(seconds = 10, outdated = FALSE, targets_only = TRUE) # Now run the pipeline and watch the graph change. px &lt;- tar_make() tar_watch_ui() and tar_watch_server() make this functionality available to other apps through a Shiny module. Unfortunately, none of these options can tell you if any parallel workers or external processes are still running. You can monitor local processes with a utility like top or htop, and traditional HPC scheduler like SLURM or SGE support their own polling utilities such as squeue and qstat. tar_process() and tar_pid() get the process ID of the main R process that last attempted to run the pipeline. 4.2 Performance If your pipeline has several thousand targets, functions like tar_make(), tar_outdated(), and tar_visnetwork() may take longer to run. There is an inevitable per-target runtime cost because package needs to check the code and data of each target individually. If this overhead becomes too much, consider batching your work into a smaller group of heavier targets. Using your custom functions, you can make each target perform multiple iterations of a task that was previously given to targets one at a time. For details and an example, please see the discussion on batching at the bottom of the dynamic branching chapter. With dynamic branching, it is super easy to create an enormous number of targets. But when the number of targets starts to exceed a couple hundred, tar_make() slows down, and graphs from tar_visnetwork() start to become unmanageable. In targets version 0.5.0.9000, the names and shortcut arguments to tar_make() provide an alternative workaround. tar_make(names = all_of(\"only\", \"these\", \"targets\"), shortcut = TRUE) can completely omit thousands of upstream targets for the sake of concentrating on one section at a time. However, this technique is only a temporary measure, and it is best to eventually revert back to the default names = NULL and shortcut = FALSE to ensure reproducibility. In the case of dynamic branching, another temporary workaround is to temporarily select subsets of branches. For example, instead of pattern = map(large_target) in tar_target(), you could prototype on a target that uses pattern = head(map(large_target), n = 1) or pattern = slice(map(large_target), c(4, 5, 6)). In the case of slice(), the tar_branch_index() function (only in targets version 0.5.0.9000 and above) can help you find the required integer indexes corresponding to individual branch names you may want. Alternatively, if you see slowness in your project, you can contribute to the package with a profiling study. These contributions are great because they help improve the package. Here are the recommended steps. Install the proffer R package and its dependencies. Run proffer::pprof(tar_make(callr_function = NULL)) on your project. When a web browser pops up with pprof, select the flame graph and screenshot it. Post the flame graph, along with any code and data you can share, to the targets package issue tracker. The maintainer will have a look and try to make the package faster for your use case if speedups are possible. "],["functions.html", "Chapter 5 Functions 5.1 Problems with script-based workflows 5.2 Functions 5.3 Writing functions 5.4 Functions in pipelines 5.5 Tracking changes", " Chapter 5 Functions targets expects users to adopt a function-oriented style of programming. User-defined R functions are essential to express the complexities of data generation, analysis, and reporting. This chapter explains what makes functions useful and how to leverage them in your pipelines. It is based on the example from the walkthrough chapter. 5.1 Problems with script-based workflows Traditional data analysis projects consist of imperative scripts, often with with numeric prefixes. 01-data.R 02-model.R 03-plot.R To run the project, the user runs each of the scripts in order. source(&quot;01-data.R&quot;) source(&quot;02-model.R&quot;) source(&quot;03-plot.R&quot;) Each script executes a different part of the workflow. # 01-data.R library(tidyverse) data &lt;- &quot;data.csv&quot; %&gt;% read_csv(col_types = cols()) %&gt;% filter(!is.na(Ozone)) write_csv(data, &quot;data.rds&quot;) # 02-model.R library(biglm) library(tidyverse) data &lt;- read_rds(&quot;data.rds&quot;, col_types = cols()) model &lt;- lm(Ozone ~ Temp, data) %&gt;% coefficients() saveRDS(model, &quot;model.rds&quot;) # 03-plot.R library(tidyverse) model &lt;- readRDS(&quot;model.rds&quot;) data &lt;- readRDS(&quot;data.rds&quot;) plot &lt;- ggplot(data) + geom_point(aes(x = Temp, y = Ozone)) + geom_abline(intercept = model[1], slope = model[2]) + theme_gray(24) ggsave(&quot;plot.png&quot;, plot) Although this approach may feel convenient at first, it scales poorly for medium-sized workflows. These imperative scripts are monolithic, and they grow too large and complicated to understand or maintain. 5.2 Functions Functions are the building blocks of most computer code. They make code easier to think about, and they break down complicated ideas into small manageable pieces. Out of context, you can develop and test a function in isolation without mentally juggling the rest of the project. In the context of the whole workflow, functions are convenient shorthand to make your work easier to read. In addition, functions are a nice mental model to express data science. A data analysis workflow is a sequence of transformations: datasets map to analyses, and analyses map to summaries. In fact, a function for data science typically falls into one of three categories: Process a dataset. Analyze a dataset. Summarize an analysis. The example from the walkthrough chapter is a simple instance of this structure. 5.3 Writing functions Let us begin with our imperative code for data processing. Every time you look at it, you need to read it carefully and relearn what it does. And test it, you need to copy the entire block into the R console. data &lt;- &quot;data.csv&quot; %&gt;% read_csv(col_types = cols()) %&gt;% filter(!is.na(Ozone)) It is better to encapsulate this code in a function. get_data &lt;- function(file) { read_csv(file, col_types = cols()) %&gt;% as_tibble() %&gt;% filter(!is.na(Ozone)) } Now, instead of invoking a whole block of text, all you need to do is type a small reusable command. The function name speaks for itself, so you can recall what it does without having to mentally process all the details again. get_data(&quot;data.csv&quot;) As with the data, we can write a function to fit a model, fit_model &lt;- function(data) { lm(Ozone ~ Temp, data) %&gt;% coefficients() } and another function to plot the model against the data. plot_model &lt;- function(model, data) { ggplot(data) + geom_point(aes(x = Temp, y = Ozone)) + geom_abline(intercept = model[1], slope = model[2]) + theme_gray(24) } 5.4 Functions in pipelines Without those functions, our pipeline in the walkthrough chapter would look long, complicated, and difficult to digest. # _targets.R library(targets) source(&quot;R/functions.R&quot;) tar_option_set(packages = c(&quot;tibble&quot;, &quot;readr&quot;, &quot;dplyr&quot;, &quot;ggplot2&quot;)) list( tar_target(file, &quot;data.csv&quot;, format = &quot;file&quot;), tar_target( data, read_csv(file, col_types = cols()) %&gt;% filter(!is.na(Ozone)) ), tar_target( model, lm(Ozone ~ Temp, data) %&gt;% coefficients() ), tar_target( plot, ggplot(data) + geom_point(aes(x = Temp, y = Ozone)) + geom_abline(intercept = model[1], slope = model[2]) + theme_gray(24) ) ) But if we write our functions in R/functions.R and source() them into the target script file (default: _targets.R) the pipeline becomes much easier to read. We can even condense out raw_data and data targets together without creating a large command. # _targets.R library(targets) source(&quot;R/functions.R&quot;) tar_option_set(packages = c(&quot;tibble&quot;, &quot;readr&quot;, &quot;dplyr&quot;, &quot;ggplot2&quot;)) list( tar_target(file, &quot;data.csv&quot;, format = &quot;file&quot;), tar_target(data, get_data(file)), tar_target(model, fit_model(data)), tar_target(plot, plot_model(model, data)) ) 5.5 Tracking changes To help figure out which targets to rerun and which ones to skip, the targets package tracks changes to the functions you define. To track changes to a function, targets computes a hash. This hash fingerprints the deparsed function (body and arguments) together with the hashes of all global functions and objects called that function. So if the function’s body, arguments, or dependencies change nontrivially, that change will be detected. This hashing system is not perfect. For example, functions created by Rcpp::cppFunction() do not show the state of the underlying C++ code. As a workaround, you can use a wrapper that inserts the C++ code into the R function body so targets can track it for meaningful changes. cpp_function &lt;- function(code) { out &lt;- Rcpp::cppFunction(code) body(out) &lt;- rlang::call2(&quot;{&quot;, code, body(out)) out } your_function &lt;- cpp_function( &quot;int your_function(int x, int y, int z) { int sum = x + y + z; return sum; }&quot; ) Functions produced by Vectorize() and purrr::safely() suffer similar issues because the actual function code is in the closure of the function instead of the body. In addition, functions from packages are not automatically tracked, and extra steps documented in the packages chapter are required to enable this. It is impossible to eliminate every edge case, so before running the pipeline, please run the dependency graph and other utilities to check your understanding of the state of project. "],["targets.html", "Chapter 6 Target construction 6.1 Target names 6.2 What a target should do 6.3 How much a target should do 6.4 Working with tools outside R 6.5 Side effects 6.6 Dependencies 6.7 Return value", " Chapter 6 Target construction Targets are high-level steps of the workflow that run the work you define in your functions. A target runs some R code and saves the returned R object to storage, usually a single file inside _targets/objects/. 6.1 Target names A target is an abstraction. The targets package automatically manages data storage and retrieval under the hood, which means you do not need to reference a target’s data file directly (e.g. _targets/objects/your_target_name). Instead, your R code should refer to a target name as if it were a variable in an R session. In other words, from the point of view of the user, a target is an R object in memory. That means a target name must be a valid visible symbol name for an R variable. The name must not begin with a dot, and it must be a string that lets you assign a value, e.g. your_target_name &lt;- TRUE. For stylistic considerations, please refer to the tidyverse style guide syntax chapter. 6.2 What a target should do Like a good function, a good target generally does one of three things: Create a dataset. Analyze a dataset with a model. Summarize an analysis or dataset. If a function gets too long, you can split it into nested sub-functions that make your larger function easier to read and maintain. 6.3 How much a target should do The targets package automatically skips targets that are already up to date, so it is best to define targets that maximize time savings. Good targets usually Are large enough to subtract a decent amount of runtime when skipped. Are small enough that some targets can be skipped even if others need to run. Invoke no side effects such as modifications to the global environment. (But targets with tar_target(format = \"file\") can save files.) Return a single value that is Easy to understand and introspect. Meaningful to the project. Easy to save as a file, e.g. with readRDS(). Please avoid non-exportable objects as target return values or global variables. Regarding the last point above, it is possible to customize the storage format of the target. For details, enter ?tar_target in the console and scroll down to the description of the format argument. 6.4 Working with tools outside R Each target runs R code, so to invoke a tool outside R, consider system2() or processx to call the appropriate system commands. This technique allows you to run shell scripts, Python scripts, etc. from within R. External scripts should ideally be tracked as input files using tar_target(format = \"file\") as described in section on external input files. There are also specialized R packages to retrieve data from remote sources and invoke web APIs, including rnoaa, ots, and aws.s3, and you may wish to use custom cues to automatically invalidate a target when the upstream remote data changes. 6.5 Side effects Like a good pure function, a good target should return a single value and not produce side effects. (The exception is output file targets which create files and return their paths.) Avoid modifying the global environment with calls to data() or source(). If you need to source scripts to define global objects, please do so at the top of your target script file (default: _targets.R) just like source(\"R/functions.R\") from the walkthrough vignette. 6.6 Dependencies A dependency x of target y is a target, global object, or global function that y requires in order to run its R command. When target y is about to run, x is up to date and loaded into memory in the R session.11 Consequently, when targets x and y are not up to date, target x always finishes running before target y starts running. The targets package automatically discovers dependencies using static code analysis. In the example below, x is automatically detected as a dependency of y because the R command of y is the expression x + 1, which contains the symbol x. Consequently, the tar_visnetwork() dependency graph contains a left-to-right arrow from x to y. # _targets.R file library(targets) source(&quot;R/functions.R&quot;) list( tar_target(x, 2), tar_target(y, x + 1) ) # R console tar_visnetwork() To force a dependency relationship, simply mention the dependency in the target’s command. In the following example, target y depends on target z even though z does not actually contribute to the return value of y. Target z will still finish before target y begins, and target y will still load target z into its memory space before running the R command. # _targets.R file library(targets) source(&quot;R/functions.R&quot;) list( tar_target(x, 2), tar_target( y, { z # Merely mentioned to force y to depend on z. x + 1 } ), tar_target(z, 1) ) # R console tar_visnetwork() The tar_deps() function shows you which dependencies will be detected in a given function or R command.12 tar_deps(x + 2) #&gt; [1] &quot;+&quot; &quot;x&quot; tar_deps(command_of_y(dependency = x)) #&gt; [1] &quot;command_of_y&quot; &quot;x&quot; tar_deps(function() { read_csv(raw_data_file, col_types = cols()) }) #&gt; [1] &quot;{&quot; &quot;cols&quot; &quot;raw_data_file&quot; &quot;read_csv&quot; However, the findings from tar_deps() are only candidate dependencies. Unless they are either targets in the pipeline or global objects13, they will be ignored. For example, functions and objects defined in R packages are ignored. To force a pipeline to notice dependencies from an R package, include the name of the package in the imports and packages fields of tar_option_set(). 6.7 Return value The return value of a target should be an R object that can be saved to disk and hashed. 6.7.1 Saving The object should be compatible with the storage format you choose using the format argument of tar_target() or tar_option_set(). For example, if the format is \"rds\" (default), then the target should return an R object that can be saved with saveRDS() and safely loaded properly into another session. Please avoid returning non-exportable objects such as connection objects, Rcpp pointers, xgboost matrices, and greta models14. 6.7.2 Hashing Once a target is saved to disk, targets computes a digest hash to track changes to the data file(s). These hashes are used to decide whether each target is up to date or needs to rerun. In order for the hash to be useful, the data you return from a target must be an accurate reflection of the underlying content of the data. So please try to return the actual data instead of an object that wraps or points to the data. Otherwise, the package will make incorrect decisions regarding which targets can skip and which need to rerun. 6.7.3 Workaround As a workaround, you can write custom functions to create temporary instances of these non-exportable/non-hashable objects and clean them up after the task is done. The following sketch creates a target that returns a database table while managing a transient connection object. # _targets.R library(targets) get_from_database &lt;- function(table, ...) { con &lt;- DBI::dbConnect(...) on.exit(close(con)) dbReadTable(con, table) } list( tar_target( table_from_database, get_from_database(&quot;my_table&quot;, ...), # ... has use-case-specific arguments. format = &quot;feather&quot; # Requires that the return value is a data frame. ) ) targets automatically loads dependencies into memory when they are required, so it is rarely advisable to call tar_read() or tar_load() from inside a target. Except in rare circumstances, tar_read() and tar_load() are only for exploratory data analysis and literate programming.↩︎ tar_deps() works mostly like the findGlobals() function from the codetools package, except the former makes special adjustments for odd cases like formulas.↩︎ i.e. loaded into tar_option_get(\"envir) from within _targets.R↩︎ Special exceptions are granted to Keras and Torch models, which can be safely returned from targets if you specify format = \"keras\" or format = \"torch\".↩︎ "],["packages.html", "Chapter 7 Packages 7.1 Loading and configuring R packages 7.2 R packages as projects 7.3 Target Factories 7.4 Package-based invalidation", " Chapter 7 Packages This chapter describes the recommended roles of R packages in targets pipelines and how to manage them in different situations. 7.1 Loading and configuring R packages For most pipelines, it is straightforward to load the R packages that your targets need in order to run. You can either: Call library() at the top of the target script file (default: _targets.R) to load each package the conventional way, or Name the required packages using the packages argument of tar_option_set(). 2. is often faster, especially for utilities like tar_visnetwork(), because it avoids loading packages unless absolutely necessary. Some package management workflows are more complicated. If your use special configuration with conflicted, box, import, or similar utility, please do your configuration inside a project-level .Rprofile file instead of the target script file (default: _targets.R). In addition, if you use distributed workers inside external containers (Docker, Singularity, AWS AMI, etc.) make sure each container has a copy of this same .Rprofile file where the R worker process spawns. This approach is ensures that all remote workers are configured the same way as the local main process. 7.2 R packages as projects It is good practice to organize the files of a targets project similar to a research compendium or R package. However, unless have a specific reason to do so, it is usually not necessary to literally implement your targets pipeline as an installable R package with its own DESCRIPTION file. A research compendium backed by a renv library and Git-backed version control is enough reproducibility for most projects. 7.3 Target Factories To make specific targets pipelines reusable, it is usually better to create a package with specialized target factories tailored to your use case. Packages stantargets and jagstargets are examples, and you can find more information on the broader R Targetopia at https://wlandau.github.io/targetopia/. 7.4 Package-based invalidation Still, it is sometimes desirable to treat functions and objects from a package as dependencies when it comes to deciding which targets to rerun and which targets to skip. targets does not track package functions by default because this is not a common need. Usually, local package libraries do not need to change very often, and it is best to maintain a reproducible project library using renv. However, if you are developing a package alongside a targets pipeline that uses it, you may wish to invalidate certain targets as you make changes to your package. For example, if you are working on a novel statistical method, it is good practice to implement the method itself as an R package and perform the computation for the research paper in one or more targets pipelines. To track the contents of packages package1 and package2, you must Fully install these packages with install.packages() or equivalent. devtools::load_all() is insufficient because it does not make the packages available to parallel workers. Write the following in your target script file (default: _targets.R): # _targets.R library(targets) tar_option_set( packages = c(&quot;package1&quot;, &quot;package2&quot;, ...), # `...` is for other packages. imports = c(&quot;package1&quot;, &quot;package2&quot;) ) # Write the rest of _targets.R below. # ... packages = c(\"package1\", \"package2\", ...) tells targets to call library(package1), library(package2), etc. before running each target. imports = c(\"package1\", \"package2\") tells targets to dive into the environments of package1 and package2 and reproducibly track all the objects it finds. For example, if you define a function f() in package1, then you should see a function node for f() in the graph produced by tar_visnetwork(targets_only = FALSE), and targets downstream of f() will invalidate if you install an update to package1 with a new version of f(). The next time you call tar_make(), those invalidated targets will automatically rerun. "],["projects.html", "Chapter 8 Projects 8.1 Extra reproducibility 8.2 Project files 8.3 Multiple projects 8.4 Interdependent projects 8.5 The config package", " Chapter 8 Projects A project is a targets pipeline together with its supporting source code, data, and configuration settings. This chapter explains best practices when it comes to organizing and configuring targets projects. 8.1 Extra reproducibility For extra reproducibility, it is good practice to use the renv R package for package management and Git/GitHub for code version control. The entire _targets/ data store should generally not be committed to Git because of its large size.15 The broader R community has excellent resources and tutorials on getting started with these third-party tools. 8.2 Project files targets is mostly indifferent to how you organize the files in your project. However, it is good practice to follow the overall structure of a research compendium or R package (not necessarily with a DESCRIPTION file). It also is good practice to give each project its own unique folder with one targets pipeline, one renv library for package management, and one Git/GitHub repository for code version control. As described later, it is possible to create multiple overlapping projects within a single folder, but this is not recommended for most situations. The walkthrough chapter shows the file structure for a minimal targets project. For more serious projects, the file system may expand to look something like this: ├── .git/ ├── .Rprofile ├── .Renviron ├── renv/ ├── index.Rmd ├── _targets/ ├── _targets.R ├── _targets.yaml ├── R/ ├──── functions_data.R ├──── functions_analysis.R ├──── functions_visualization.R ├── data/ └──── input_data.csv Some of these files are optional, and they have the following roles. .git/: a folder automatically created by Git for version control purposes. .Rprofile: a text file automatically created by renv to automatically load the project library when you start R at the project root folder. You may wish to add other global configuration here, e.g. declare package precedence using the conflicted package. .Renviron: a text file of key-value pairs defining project-level environment variables, e.g. API keys and package settings. See Sys.getenv() for more information on environment variables and how to work with them in R. index.Rmd: Target Markdown report source file to define the pipeline. _targets/: the data store where tar_make() and similar functions write target storage and metadata when they run the pipeline. _targets.R: the target script file. All targets pipelines must have a target script file that returns a target list at the end. If you use Target Markdown (e.g. index.Rmd above) then the target script will be written automatically. Otherwise, you may write it by hand. Unless you apply the custom configuration described later in this chapter, the target script file will always be called _targets.R and live at the project root folder. _targets.yaml: a YAML file to set default arguments to critical functions like tar_make(). As described below, you can access and modify this file with functions tar_config_get(), tar_config_set(), and tar_config_unset(). targets will attempt to look for _targets.yaml unless you set a different path in the TAR_CONFIG environment variable. R/: directory of scripts containing custom user-defined R code. Most of the code will likely contain custom functions you write to support your targets. You can load these functions with source(\"R/function_script.R\") or eval(parse(text = \"R/function_script.R\"), either in a tar_globals = TRUE code chunk in Target Markdown or directly in _targets.R if you are not using Target Markdown. data/: directory of local input data files. As described in the files chapter, it is good practice to track input files using format = \"file\" in tar_target() and then reference those file targets in downstream targets that directly depend on those files. 8.3 Multiple projects It is generally good practice to give each project its own unique folder with one targets pipeline, one renv library for package management, and one Git/GitHub repository for code version control. However, sometimes it is reasonable to maintain multiple pipelines within a project: for example, if different pipelines have similar research goals and share the same code base of custom user-defined functions. This section explains how to maintain and navigate such a collection of overlapping projects. The functionality below assumes you have targets version 0.7.0.9001 or higher, which you may need to install from GitHub. remotes::install_github(&quot;ropensci/targets&quot;) 8.3.1 Create each project. To begin, write the shared code base of custom user-defined functions in R/, and write one targets pipeline per project. For convenience, we will directly write to the targets script files, but the principles generalize to Target Markdown. The file structure looks something like this: ├── _targets.yaml ├── script_a.R ├── script_b.R ├── R/ ├──── functions_data.R ├──── functions_analysis.R ├──── functions_visualization.R ... All projects share the same functions defined in the scripts in R/, and each project uses a different target script and data store. script_a.R defines the targets for project A. # script_a.R library(targets) source(&quot;R/functions_data.R&quot;) source(&quot;R/functions_analysis.R&quot;) source(&quot;R/functions_visualization.R&quot;) tar_option_set(packages = &quot;tidyverse&quot;) list( tar_target(target_abc, f(..)), tar_target(tarbet_xyz, g(...)) ) Likewise, script_b.R defines the targets for project B. # script_b.R library(targets) source(&quot;R/functions_data.R&quot;) source(&quot;R/functions_analysis.R&quot;) source(&quot;R/functions_visualization.R&quot;) tar_option_set(packages = &quot;tidyverse&quot;) list( tar_target(target_123, f(...)), tar_target(target_456, h(...)) ) 8.3.2 Configure each project. To establish a different store and script per project, write a top-level _targets.yaml configuration to specify these paths explicitly. You can do this from R with tar_config_set(). tar_config_set(script = &quot;script_a.R&quot;, store = &quot;store_a&quot;, project = &quot;project_a&quot;) tar_config_set(script = &quot;script_b.R&quot;, store = &quot;store_b&quot;, project = &quot;project_b&quot;) The R code above writes the following _targets.yaml configuration file. project_a: store: store_a script: script_a.R project_b: store: store_b script: script_b.R 8.3.3 Run each project To run each project, run tar_make() with the correct target script and data store. To select the correct script and store, set the TAR_PROJECT environment variable to the correct project name. that way, tar_config_get() automatically supplies the correct script and store arguments to tar_make(). Sys.setenv(TAR_PROJECT = &quot;project_a&quot;) tar_make() tar_read(target_abc) Sys.setenv(TAR_PROJECT = &quot;project_b&quot;) tar_make() tar_read(target_123) Alternatively, you can manually select the appropriate script and store for each project. This is a less convenient approach, but if you do it, you do not need to set the TAR_PROJECT environment variable or rely on _targets.yaml. tar_make(script = &quot;script_a.R&quot;, store = &quot;store_a&quot;) tar_read(target_abc, sctore = &quot;store_a&quot;) tar_make(script = &quot;script_b.R&quot;, store = &quot;store_b&quot;) tar_read(target_abc, sctore = &quot;store_b&quot;) 8.4 Interdependent projects 8.4.1 Config inheritance _targets.yaml can control more than just the script and store, and different projects can inherit settings from one another. In the following example, project B inherits from project A, so projects A and B both set reporter = \"summary\" and shorcut = TRUE by default in tar_make(). tar_config_set( script = &quot;script_a.R&quot;, store = &quot;store_a&quot;, reporter_make = &quot;summary&quot;, shortcut = TRUE, project = &quot;project_a&quot; ) tar_config_set( script = &quot;script_b.R&quot;, store = &quot;store_b&quot;, inherits = &quot;project_a&quot;, project = &quot;project_b&quot;, ) writeLines(readLines(&quot;_targets.yaml&quot;)) #&gt; project_a: #&gt; reporter_make: summary #&gt; script: script_a.R #&gt; shortcut: yes #&gt; store: store_a #&gt; project_b: #&gt; inherits: project_a #&gt; script: script_b.R #&gt; store: store_b Sys.setenv(TAR_PROJECT = &quot;project_b&quot;) tar_config_get(&quot;script&quot;) #&gt; [1] &quot;script_b.R&quot; tar_config_get(&quot;reporter_make&quot;) #&gt; [1] &quot;summary&quot; tar_config_get(&quot;shortcut&quot;) #&gt; [1] TRUE 8.4.2 Sharing targets For some workflows, the output of one project serves as the input to another project. The easiest way to set this up is through global objects. The first project remains unchanged, and the second project reads from the first before the pipeline begins. # script_b.R library(targets) source(&quot;R/functions_data.R&quot;) source(&quot;R/functions_analysis.R&quot;) source(&quot;R/functions_visualization.R&quot;) tar_option_set(packages = &quot;tidyverse&quot;) object_from_project_a &lt;- tar_read(target_from_project_a, store = &quot;store_a&quot;) list( tar_target(new_target, some_function(object_from_project_a)), ... ) This approach is the most convenient and versatile, but it can be inefficient if target_from_project_a is large. A higher-performant solution for large data is to treat the file in project A’s data store as an input file target in project B. This second approach requires an understanding of the data store and an awareness of which targets are stored locally and which are stored on the cloud. For a target with repository = \"local\", you can begin from the file store_a/objects/target_from_project_a. Otherwise, the target’s file exists on the cloud (AWS or GCP) and you may need to access the target as a URL in a target with format = \"url\". # script_b.R library(targets) source(&quot;R/functions_data.R&quot;) source(&quot;R/functions_analysis.R&quot;) source(&quot;R/functions_visualization.R&quot;) tar_option_set(packages = &quot;tidyverse&quot;) list( tar_target(file_from_project_a, &quot;store_a/objects/target_name&quot;, format = &quot;file&quot;), tar_target(data_from_project_a, readRDS(file_from_project_a)), # Assumes format = &quot;rds&quot; in project A. tar_target(new_target, analyze_data(data_from_project_a)), ... ) 8.5 The config package The _targets.yaml config interface borrows heavily from the ideas in the config R package. However, it does not actually use the config package, nor does it copy or use the config source code in any way. And there are major differences in user-side behavior: There is no requirement to have a configuration (i.e. project) named “default”. The default project is called “main”, and other projects do not inherit from it automatically. Not all fields need to be populated in _targets.yaml because the targets package already has system defaults. However, you may wish to commit _targets/meta/meta, which is critical to checking the status of each target and reading targets into memory.↩︎ "],["data.html", "Chapter 9 Data: storage and memory 9.1 Local data store 9.2 External files 9.3 Memory 9.4 Cloud storage 9.5 Cleaning up local internal data files", " Chapter 9 Data: storage and memory This chapter describes how the targets package stores data, manages memory, allows you to customize the data processing model. 9.1 Local data store When a target finishes running during tar_make(), it returns an R object. Those return values, along with descriptive metadata, are saved to persistent storage so your pipeline stays up to date even after you exit R. By default, this persistent storage is a special _targets/ folder created in your working directory by tar_make(). The files in the local data store are organized as follows. _targets/ # Can be customized with tar_config_set(). ├── meta/ ├────── meta ├────── process ├────── progress ├── objects/ ├────── target1 ├────── target2 ├────── branching_target_c7bcb4bd ├────── branching_target_285fb6a9 ├────── branching_target_874ca381 ├── scratch/ # tar_make() deletes this folder after it finishes. └── user/ # gittargets users can put custom files here for data version control. The two most important components are: _targets/meta/meta, a flat text file with descriptive metadata about each target, and _targets/objects/, a folder with one data file per target. If your pipeline has a target defined by tar_target(name = x, command = 1 + 1, format = \"rds\", repository = \"local\"), during tar_make(): The target runs and returns a value of 2. The return value 2 is saved as an RDS file to _targets/objects/x. You could read the return value back into R with readRDS(\"_targets/objects/x\"), but tar_read(x) is far more convenient. _targets/meta/meta gets a new row of metadata describing target x. You can read that metadata with tar_meta(x). Notably, tar_meta(x)$data contains the hash of file _targets/objects/x. This has helps the next tar_make() decide whether to rerun target x. The format argument of tar_target() (and tar_option_set()) controls how tar_make() saves the return value. The default is \"rds\", which uses saveRDS(), and there are more efficient formats such as \"qs\" and \"feather\". Some of these formats require external packages. See https://docs.ropensci.org/targets/reference/tar_target.html#storage-formats for details. 9.2 External files If your pipeline loads a preexisting data file or creates files outside the data store, it is good practice to watch them for changes. That way, tar_make() will automatically rerun the appropriate targets if these files change. To watch one of more files, create a target that Has format = \"file\" in tar_target(), and Returns a character vector of local files and/or directories. The example sketch of a pipeline below follows this pattern. # _targets.R library(targets) create_output &lt;- function(file) { data &lt;- read.csv(file) output &lt;- head(data) write.csv(output, &quot;output.csv&quot;) &quot;output.csv&quot; } list( tar_target(name = input, command = &quot;data.csv&quot;, format = &quot;file&quot;), tar_target(name = output, command = create_output(input), format = &quot;file&quot;) ) We assume a file called data.csv exists prior to running the pipeline. When tar_make() runs the first time, target input runs and returns the value \"data.csv\". Because format is \"file\", no extra file is saved to _targets/meta/objects/. Instead, \"data.csv\" gets hashed, and the hash is stored in the metadata. Then, target output runs, creates the file \"output.csv\", and that file gets processed the same way. Target output depends on target input because the command of target output mentions the symbol input. (Verify with tar_visnetwork().) That way, output does not run until input is finished, and output reruns if the hash of input changes. It is good practice to write target symbols instead of literal input paths to ensure the proper dependency relationships. In this case, if output were written with the literal input path as tar_target(name = output, command = create_output(\"data.csv\"), format = \"file\"), then the dependency relationship would break, and output would not rerun if input changed. The mechanism of format = \"file\" applies equally to input files and output files. In fact, a target can track both input and output files at the same time. This is part of how tar_render() works. As discussed in the R Markdown chapter, tar_render() takes an R Markdown source file as input, write a rendered report file as output, and returns a character vector with the paths to both files. 9.3 Memory A typical target has dependencies upstream. In order to run properly, it needs the return values of those dependencies to exist in the random access memory (RAM). By default, tar_make() reads those dependency targets from the data store, and it keeps in memory those targets and any targets that run. For big data workflows where not all data can fit into RAM, it is wiser to set memory = \"transient\" and garbage_collection = TRUE in tar_target() (and tar_option_set()). That way, the target return value is removed from memory at the earliest opportunity. The next time the target value is needed, it is reread from storage again, and then removed from memory as soon as possible. Reading a big dataset from storage can take time, which may slow down some pipelines, but it may be worth the extra time to make sure memory usage stays within reasonable limits. 9.4 Cloud storage Cloud data can lighten the burden of local storage, make the pipeline portable, and facilitate data version control. Using arguments repository and resources of tar_target() (and tar_option_set()), you can send the return value to the cloud instead of a local file in _targets/objects/. The repository argument identifies the cloud service of choice: \"aws\" for Amazon Web Service (AWS) Simple Storage Service (S3), and \"gcp\" for Google Cloud Platform (GCP) Google Cloud Storage (GCS). Each platform requires different steps to configure, but there usage in targets is almost exactly the same. 9.4.1 Cost Cloud services cost money. The more resources you use, the more you owe. Resources not only include the data you store, but also the HTTP requests that tar_make() uses to check if a target exists and is up to date. So cost increases with the number of cloud targets and the frequency that you run them. Please proactively monitor usage in the AWS or GCP web console and rethink your strategy if usage is too high. For example, you might consider running the pipeline locally and then sycning the data store to a bucket only at infrequent strategic milestones. 9.4.2 AWS setup Sign up for a free tier account at https://aws.amazon.com/free. Follow these instructions to practice using Simple Storage Service (S3) through the web console at https://console.aws.amazon.com/s3/. Install the paws R package with install.packages(\"paws\"). Follow the credentials section of the paws README to connect paws to your AWS account. You will set special environment variables in your user-level .Renviron file. Example: # Example .Renviron file AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY AWS_REGION=us-east-1 # The paws package and thus targets &gt;= 0.8.1.9000 use this. AWS_DEFAULT_REGION=us-east-1 # For back compatibility with targets &lt;= 0.8.1. Restart your R session and create an S3 buckets to store target data. You can do this either in the AWS S3 web console or the following code. library(paws) s3 &lt;- s3() s3$create_bucket(Bucket = &quot;my-test-bucket-25edb4956460647d&quot;) 9.4.3 GCP setup Activate a Google Cloud Platform account at https://cloud.google.com/. Follow the instructions at https://code.markedmondson.me/googleCloudRunner/articles/setup-gcp.html to set up your GCP account to use locally with R. The video is friendly and helpful. In your .Renviron file, set the GCS_AUTH_FILE environment variable to the same value as GCE_AUTH_FILE from step (2). Create a Google Cloud Storage (GCS) bucket to store target data. You can do this either with the GCP GCS web dashboard or the following code. googleCloudStorageR::gcs_create_bucket( bucket = &quot;my-test-bucket-25edb4956460647d&quot;, projectId = Sys.getenv(&quot;GCE_DEFAULT_PROJECT_ID&quot;) ) 9.4.4 Usage The following is an example pipeline that sends targets to an AWS S3 bucket. Usage in GCP is almost exactly the same. # Example _targets.R file: library(targets) tar_option_set( resources = tar_resources( aws = tar_resources_aws(bucket = &quot;my-test-bucket-25edb4956460647d&quot;) ) ) write_mean &lt;- function(data) { tmp &lt;- tempfile() writeLines(as.character(mean(data)), tmp) tmp } list( tar_target( data, rnorm(5), format = &quot;qs&quot;, # Set format = &quot;aws_qs&quot; in targets &lt;= 0.10.0. repository = &quot;aws&quot; # Set to &quot;gcp&quot; for Google Cloud Platform. ), tar_target( mean_file, write_mean(data), format = &quot;file&quot;, # Set format = &quot;aws_file&quot; in targets &lt;= 0.10.0. repository = &quot;aws&quot; # Set to &quot;gcp&quot; for Google Cloud Platform. ) ) When you run the pipeline above with tar_make(), your local R session computes rnorm(5), saves it to a temporary qs file on disk, and then uploads it to a file called _targets/objects/data on your S3 bucket. Likewise for mean_file, but because the format is \"file\" and the repository is \"aws\", you are responsible for supplying the path to the file that gets uploaded to _targets/objects/mean_file. format = \"file\" works differently for cloud storage than local storage. Here, it is assumed that the command of the target writes a single file, and then targets uploads this file to the cloud and deletes the local copy. At that point, the copy in the cloud is tracked for changes, and the local copy does not exist. tar_make() #&gt; ● run target data #&gt; ● run target mean_file #&gt; ● end pipeline And of course, your targets stay up to date if you make no changes. tar_make() #&gt; ✓ skip target data #&gt; ✓ skip target mean_file #&gt; ✓ skip pipeline If you log into https://s3.console.aws.amazon.com/s3, you should see objects _targets/objects/data and _targets/objects/mean_file in your bucket. To download this data locally, use tar_read() and tar_load() like before. These functions download the data from the bucket and load it into R. tar_read(data) #&gt; [1] -0.74654607 -0.59593497 -1.57229983 0.40915323 0.02579023 The \"file\" format behaves differently on the cloud. tar_read() and tar_load() download the object to a local path (where the target saved it locally before it was uploaded) and return the path so you can process it yourself.16 tar_load(mean_file) mean_file #&gt; [1] &quot;_targets/scratch/mean_fileff086e70876d&quot; readLines(mean_file) #&gt; [1] &quot;-0.495967480886693&quot; When you are done with these temporary files and the pipeline is no longer running, you can safely remove everything in _targets/scratch/. unlink(&quot;_targets/scratch/&quot;, recursive = TRUE) # tar_destroy(destroy = &quot;scratch&quot;) 9.4.5 Data version control Amazon and Google support versioned buckets. If your bucket has versioning turned on, then every version of every target will be stored,17, and the target metadata will contain the version ID (verify with tar_meta(your_target, path)$path). That way, if you roll back _targets/meta/meta to a prior version, then tar_read(your_target) will read a prior target. And if you roll back the metadata and the code together, then your pipeline will journey back in time while stay up to date (old code synced with old data). Rolling back is possible if you use Git/GitHub and commit your R code files and _targets/meta/meta to the repository. An alternative cloudless versioning solution is gittargets, a package that snapshots the local data store and syncs with an existing code repository. 9.5 Cleaning up local internal data files There are multiple functions to remove or clean up target storage. Most of these functions delete internal files or records from the data store and delete objects from cloud buckets. They do not delete local external files (i.e. tar_target(..., format = \"file\", repository = \"local\")) because some of those files could be local input data that exists prior to tar_make(). tar_destroy() is by far the most commonly used cleaning function. It removes the _targets/ folder (or optionally a subfolder in _targets/) and all the cloud targets mentioned in the metadata. Use it if you intend to start the pipeline from scratch without any trace of a previous run. tar_prune() deletes the data and metadata of all the targets no longer present in your current target script file (default: _targets.R). This is useful if you recently worked through multiple changes to your project and are now trying to discard irrelevant data while keeping the results that still matter. tar_delete() is more selective than tar_destroy() and tar_prune(). It removes the individual data files of a given set of targets from _targets/objects/ and cloud buckets while leaving the metadata in _targets/meta/meta alone. If you have a small number of data-heavy targets you need to discard to conserve storage, this function can help. tar_invalidate() is the opposite of tar_delete(): for the selected targets, it deletes the metadata in _targets/meta/meta and does not delete the return values. After invalidation, you will still be able to locate the data files with tar_path() and manually salvage them in an emergency. However, tar_load() and tar_read() will not be able to read the data into R, and subsequent calls to tar_make() will attempt to rebuild those targets. Non-“file” AWS formats also download files, but they are temporary and immediately discarded after the data is read into memory.↩︎ GCP has safety capabilities such as discarding all but the newest n versions.↩︎ "],["literate-programming.html", "Chapter 10 Literate programming 10.1 Literate programming within a target 10.2 Target Markdown", " Chapter 10 Literate programming Literate programming is the practice of mixing code and descriptive writing in order to execute and explain a data analysis simultaneously in the same document. The targets package supports literate programming through tight integration with Quarto, R Markdown, and knitr. It is recommended to learn one of these three tools before proceeding with the following chapter. There are two kinds of literate programming in targets: A literate programming source document (or Quarto project) that renders inside an individual target. Here, you define a special kind of target that runs a lightweight R Markdown report which depends on upstream targets. Target Markdown, an overarching system in which one or more Quarto or R Markdown files write the _targets.R file and encapsulate the pipeline. 10.1 Literate programming within a target 10.1.1 R Markdown targets Here, literate programming serves to display, summarize, and annotate results from upstream in the targets pipeline. The document(s) have little to no computation of their own, and they make heavy use of tar_read() and tar_load() to leverage output from other targets. As an example, let us extend the walkthrough example chapter with the following R Markdown source file report.Rmd. This document depends on targets fit and hist. If we previously ran the pipeline and the data store _targets/ exists, then tar_read() and tar_load() will read those targets and show them in the rendered HTML output report.html. With the tar_render() function in tarchetypes, we can go a step further and include report.Rmd as a target in the pipeline. This new targets re-renders report.Rmd whenever fit or hist changes, which means tar_make() brings the output file report.html up to date. library(tarchetypes) target &lt;- tar_render(report, &quot;report.Rmd&quot;) # Just defines a target object. target$command$expr[[1]] #&gt; tarchetypes::tar_render_run(path = &quot;report.Rmd&quot;, args = list(input = &quot;report.Rmd&quot;, #&gt; knit_root_dir = getwd(), quiet = TRUE), deps = list(fit, #&gt; hist)) tar_render() is like tar_target(), except that you supply the file path to the R Markdown report instead of an R command. Here it is at the bottom of the example _targets.R file below: # _targets.R library(targets) library(tarchetypes) source(&quot;R/functions.R&quot;) list( tar_target( raw_data_file, &quot;data/raw_data.csv&quot;, format = &quot;file&quot; ), tar_target( raw_data, read_csv(raw_data_file, col_types = cols()) ), tar_target( data, raw_data %&gt;% mutate(Ozone = replace_na(Ozone, mean(Ozone, na.rm = TRUE))) ), tar_target(hist, create_plot(data)), tar_target(fit, biglm(Ozone ~ Wind + Temp, data)), tar_render(report, &quot;report.Rmd&quot;) # Here is our call to tar_render(). ) When we visualize the pipeline, we see that our report target depends on targets fit and hist. tar_render() automatically detects these upstream dependencies by statically analyzing report.Rmd for calls to tar_load() and tar_read(). # R console tar_visnetwork() 10.1.2 Quarto targets tarchetypes &gt;= 0.6.0.9000 supports a tar_quarto() function, which is like tar_render(), but for Quarto. For an individual source document, tar_quarto() works exactly the same way as tar_render(). However, tar_quarto() is more powerful: you can supply the path to an entire Quarto project, such as a book, blog, or website. tar_quarto() looks for target dependencies in all the source documents (e.g. listed in _quarto.yml), and it tracks the important files in the project for changes (run tar_quarto_files() to see which ones). 10.1.3 Parameterized documents tarchetypes functions make it straightforward to use parameterized R Markdown and parameterized Quarto in a targets pipeline. The next two subsections walk through the major use cases. 10.1.4 Single parameter set In this scenario, the pipeline renders your parameterized report one time using a single set of parameters. These parameters can be upstream targets, global objects, or fixed values. Simply pass a params argument to tar_render() or an execute_params argument to tar_quarto(). Example: # _targets.R library(targets) library(tarchetypes) list( tar_target(data, data.frame(x = seq_len(26), y = letters)) tar_quarto(report, &quot;report.qmd&quot;, execute_params = list(your_param = data)) ) Internally, the report target runs: # R console quarto::quarto_render(&quot;report.qmd&quot;, params = list(your_param = your_target)) where report.qmd looks like this: See tar_quarto() examples and tar_render() examples for more. 10.1.5 Multiple parameter sets In this scenario, you still have a single report, but you render it multiple times over a grid of parameters. This time, use tar_quarto_rep() or tar_render_rep(). Each of these functions takes as input a grid of parameters with one column per parameter and one row per parameter set, where each parameter set is used to render an instance of the document. In other words, the number of rows in the parameter grid is the number of output documents you will produce. Below is an example _targets.R file using tar_render_rep(). Usage with tar_quarto_rep() is the same18. # _targets.R library(targets) library(tarchetypes) tar_option_set(packages = &quot;tibble&quot;) list( tar_target(x, &quot;value_of_x&quot;), tar_render_rep( report, &quot;report.Rmd&quot;, params = tibble( par = c(&quot;par_val_1&quot;, &quot;par_val_2&quot;, &quot;par_val_3&quot;, &quot;par_val_4&quot;), output_file = c(&quot;f1.html&quot;, &quot;f2.html&quot;, &quot;f3.html&quot;, &quot;f4.html&quot;) ), batches = 2 ) ) where report.Rmd has the following YAML front matter: title: report output_format: html_document params: par: &quot;default value&quot; and the following R code chunk: print(params$par) print(tar_read(x)) tar_render_rep() creates a target for the parameter grid and uses dynamic branching to render the output reports in batches. In this case, we have two batches (dynamic branches) that each produce two reports (four output reports total). # R console tar_make() #&gt; ● run target x #&gt; ● run target report_params #&gt; ● run branch report_9e7470a1 #&gt; ● run branch report_457829de #&gt; ● end pipeline The third output file f3.html is below, and the rest look similar. For more information, see these examples. 10.2 Target Markdown Target Markdown, available in targets &gt; 0.6.0, is a powerful Markdown/knitr-based interface for reproducible analysis pipelines.19 With Target Markdown, you can define a fully scalable pipeline from within one or more Quarto or R Markdown reports or projects (even spreading a single pipeline over multiple source documents). You get the best of both worlds: the human readable narrative of literate programming, and the sophisticated caching and dependency management systems of targets. 10.2.1 Access This chapter’s example Target Markdown document is itself a tutorial and a simplified version of the chapter. There are two convenient ways to access the file: The use_targets() function. The RStudio R Markdown template system. For (2), in the RStudio IDE, select a new Quarto or R Markdown document in the New File dropdown menu in the upper left-hand corner of the window. Then, select the Target Markdown template and click OK to open a copy of the report for editing. 10.2.2 Purpose Target Markdown has two primary objectives: Interactively explore, prototype, and test the components of a targets pipeline using the Quarto notebook interface or the R Markdown notebook interface. Set up a targets pipeline using convenient Markdown-like code chunks. Target Markdown supports a special {targets} language engine with an interactive mode for (1) and a non-interactive mode for (2). By default, the mode is interactive in the notebook interface and non-interactive when you knit/render the whole document.20. You can set the mode using the tar_interactive chunk option. 10.2.3 Example The following example is based on the minimal targets project at https://github.com/wlandau/targets-minimal/. We process the base airquality dataset, fit a model, and display a histogram of ozone concentration. 10.2.4 Required packages This example requires several R packages, and targets must be version 0.6.0 or above. # R console install.packages(c(&quot;biglm&quot;, &quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;readr&quot;, &quot;targets&quot;, &quot;tidyr&quot;)) 10.2.5 Setup First, load targets to activate the specialized knitr engine for Target Markdown. ```{r} library(targets) ``` Non-interactive Target Markdown writes scripts to a special _targets_r/ directory to define individual targets and global objects. In order to keep your target definitions up to date, it is recommended to remove _targets_r/ at the beginning of the R Markdown document(s) in order to clear out superfluous targets and globals from a previous version. tar_unscript() is a convenient way to do this. ```{r} tar_unscript() ``` 10.2.6 Globals As usual, your targets depend on custom functions, global objects, and tar_option_set() options you define before the pipeline begins. Define these globals using the {targets} engine with tar_globals = TRUE chunk option. ```{targets some-globals, tar_globals = TRUE, tar_interactive = TRUE} options(tidyverse.quiet = TRUE) tar_option_set(packages = c(&quot;biglm&quot;, &quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;readr&quot;, &quot;tidyr&quot;)) create_plot &lt;- function(data) { ggplot(data) + geom_histogram(aes(x = Ozone), bins = 12) + theme_gray(24) } ``` In interactive mode, the chunk simply runs the R code in the tar_option_get(\"envir\") environment (usually the global environment) and displays a message: #&gt; Run code and assign objects to the environment. Here is the same chunk in non-interactive mode. Normally, there is no need to duplicate chunks like this, but we do so here in order to demonstrate both modes. ```{targets chunk-name, tar_globals = TRUE, tar_interactive = FALSE} options(tidyverse.quiet = TRUE) tar_option_set(packages = c(&quot;biglm&quot;, &quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;readr&quot;, &quot;tidyr&quot;)) create_plot &lt;- function(data) { ggplot(data) + geom_histogram(aes(x = Ozone), bins = 12) + theme_gray(24) } ``` In non-interactive mode, the chunk establishes a common _targets.R file and writes the R code to a script in _targets_r/globals/, and displays an informative message:21 #&gt; Establish _targets.R and _targets_r/globals/chunk-name.R. It is good practice to assign explicit chunk labels or set the tar_name chunk option on a chunk-by-chunk basis. Each chunk writes code to a script path that depends on the name, and all script paths need to be unique.22 10.2.7 Target definitions To define targets of the pipeline, use the {targets} language engine with the tar_globals chunk option equal FALSE or NULL (default). The return value of the chunk must be a target object or a list of target objects, created by tar_target() or a similar function. Below, we define a target to establish the air quality dataset in the pipeline. ```{targets raw-data, tar_interactive = TRUE} tar_target(raw_data, airquality) ``` If you run this chunk in interactive mode, the target’s R command runs, the engine tests if the output can be saved and loaded from disk correctly, and then the return value gets assigned to the tar_option_get(\"envir\") environment (usually the global environment). #&gt; Run targets and assign them to the environment. In the process, some temporary files are created and destroyed, but your local file space will remain untouched (barring any custom side effects in your custom code). After you run a target in interactive mode, the return value is available in memory, and you can write an ordinary R code chunk to read it. ```{r} head(raw_data) ``` The output is the same as what tar_read(raw_data) would show after a serious pipeline run. head(raw_data) #&gt; Ozone Solar.R Wind Temp Month Day #&gt; 1 41 190 7.4 67 5 1 #&gt; 2 36 118 8.0 72 5 2 #&gt; 3 12 149 12.6 74 5 3 #&gt; 4 18 313 11.5 62 5 4 #&gt; 5 NA NA 14.3 56 5 5 #&gt; 6 28 NA 14.9 66 5 6 For demonstration purposes, here is the raw_data target code chunk in non-interactive mode. ```{targets chunk-name-with-target, tar_interactive = FALSE} tar_target(raw_data, airquality) ``` In non-interactive mode, the {targets} engine does not actually run any targets. Instead, it establishes a common _targets.R and writes the code to a script in _targets_r/targets/. #&gt; Establish _targets.R and _targets_r/targets/chunk-name-with-target.R. Next, we define more targets to process the raw data and plot a histogram. Only the returned value of the chunk code actually becomes part of the pipeline, so if you define multiple targets in a single chunk, be sure to wrap them all in a list. ```{targets downstream-targets} list( tar_target(data, raw_data %&gt;% filter(!is.na(Ozone))), tar_target(hist, create_plot(data)) ) ``` In non-interactive mode, the whole target list gets written to a single script. #&gt; Establish _targets.R and _targets_r/targets/downstream-targets.R. Lastly, we define a target to fit a model to the data. For simple targets like this one, we can use convenient shorthand to convert the code in a chunk into a valid target. Simply set the tar_simple chunk option to TRUE. ```{targets fit, tar_simple = TRUE} analysis_data &lt;- data biglm(Ozone ~ Wind + Temp, analysis_data) ``` When the chunk is preprocessed, chunk label (or the tar_name chunk option if you set it) becomes the target name, and the chunk code becomes the target command. All other arguments of tar_target() remain at their default values (configurable with tar_option_set() in a tar_globals = TRUE chunk). The output in the rendered R Markdown document reflects this preprocessing. tar_target(fit, { biglm(Ozone ~ Wind + Temp, data) }) #&gt; Define target fit from chunk code. #&gt; Establish _targets.R and _targets_r/targets/fit.R. 10.2.8 Pipeline If you ran all the {targets} chunks in non-interactive mode (i.e. pipeline construction mode), then the target script file and helper scripts should all be established, and you are ready to run the pipeline in with tar_make() in an ordinary {r} code chunk. This time, the output is written to persistent storage at the project root. ```{r} tar_make() ``` #&gt; • start target raw_data #&gt; • built target raw_data #&gt; • start target data #&gt; • built target data #&gt; • start target fit #&gt; • built target fit #&gt; • start target hist #&gt; • built target hist #&gt; • end pipeline: 2.103 seconds 10.2.9 Output You can retrieve results from the _targets/ data store using tar_read() or tar_load(). ```{r} library(biglm) tar_read(fit) ``` #&gt; Large data regression model: biglm(Ozone ~ Wind + Temp, data) #&gt; Sample size = 116 ```{r} tar_read(hist) ``` The targets dependency graph helps your readers understand the steps of your pipeline at a high level. ```{r} tar_visnetwork() ``` At this point, you can go back and run {targets} chunks in interactive mode without interfering with the code or data of the non-interactive pipeline. 10.2.10 Conditioning on interactive mode targets version 0.6.0.9001 and above supports the tar_interactive() function, which suppresses code unless Target Markdown interactive mode is turned on. Similarly, tar_noninteractive() suppresses code in interactive mode, and tar_toggle() selects alternative pieces of code based on the current mode. 10.2.11 tar_interactive() tar_interactive() is useful for dynamic branching. If a dynamic target branches over a target from a different chunk, this ordinarily breaks interactive mode. ```{targets condition, tar_interactive = TRUE} tar_target(y, x ^ 2, pattern = map(x)) ``` #&gt; Run targets and assign them to the environment. #&gt; Error: #&gt; ! Target y tried to branch over x, which is illegal. Patterns must only branch over explicitly declared targets in the pipeline. Stems and patterns are fine, but you cannot branch over branches or global objects. Also, if you branch over a target with format = &quot;file&quot;, then that target must also be a pattern. However, with tar_interactive(), you can define a version of x just for testing and prototyping in interactive mode. The chunk below fixes interactive mode without changing the pipeline in non-interactive mode. ```{targets condition-fixed, tar_interactive = TRUE} list( tar_interactive(tar_target(x, seq_len(2))), tar_target(y, x ^ 2, pattern = map(x)) ) ``` #&gt; Run targets and assign them to the environment. 10.2.12 tar_toggle() tar_toggle() is useful for scaling up and down the amount of work based on the current mode. Interactive mode should finish quickly for prototyping and testing, and non-interactive mode should take on the full level work required for a serious pipeline. Below, tar_toggle() seamlessly scales up and down the number of simulations repetitions in the example target from https://wlandau.github.io/rmedicine2021-pipeline/#target-definitions. To learn more about stantargets, visit https://docs.ropensci.org/stantargets/. ```{targets bayesian-model-validation, tar_interactive = TRUE} tar_stan_mcmc_rep_summary( name = mcmc, stan_files = &quot;model.stan&quot;, data = simulate_data(), # Defined in another code chunk. batches = tar_toggle(1, 100), reps = tar_toggle(1, 10), chains = tar_toggle(1, 4), parallel_chains = tar_toggle(1, 4), iter_warmup = tar_toggle(100, 4e4), iter_sampling = tar_toggle(100, 4e4), summaries = list( ~posterior::quantile2(.x, probs = c(0.025, 0.25, 0.5, 0.75, 0.975)), rhat = ~posterior::rhat(.x) ), deployment = &quot;worker&quot; ) ``` 10.2.13 Chunk options tar_globals: Logical of length 1, whether to define globals or targets. If TRUE, the chunk code defines functions, objects, and options common to all the targets. If FALSE or NULL (default), then the chunk returns formal targets for the pipeline. tar_interactive: Logical of length 1 to choose whether to run the chunk in interactive mode or non-interactive mode. tar_name: name to use for writing helper script files (e.g. _targets_r/targets/target_script.R) and specifying target names if the tar_simple chunk option is TRUE. All helper scripts and target names must have unique names, so please do not set this option globally with knitr::opts_chunk$set(). tar_script: Character of length 1, where to write the target script file in non-interactive mode. Most users can skip this option and stick with the default _targets.R script path. Helper script files are always written next to the target script in a folder with an \"_r\" suffix. The tar_script path must either be absolute or be relative to the project root (where you call tar_make() or similar). If not specified, the target script path defaults to tar_config_get(\"script\") (default: _targets.R; helpers default: _targets_r/). When you run tar_make() etc. with a non-default target script, you must select the correct target script file either with the script argument or with tar_config_set(script = ...). The function will source() the script file from the current working directory (i.e. with chdir = FALSE in source()). tar_simple: Logical of length 1. Set to TRUE to define a single target with a simplified interface. In code chunks with tar_simple equal to TRUE, the chunk label (or the tar_name chunk option if you set it) becomes the name, and the chunk code becomes the command. In other words, a code chunk with label targetname and command mycommand() automatically gets converted to tar_target(name = targetname, command = mycommand()). All other arguments of tar_target() remain at their default values (configurable with tar_option_set() in a tar_globals = TRUE chunk). except the parameter grid argument is called execute_params in tar_quarto_rep().↩︎ Target Markdown is powered entirely by targets and knitr. It does not actually require Markdown, although Markdown is the recommended way to interact with it.↩︎ In targets version 0.6.0, the mode is interactive if interactive() is TRUE. In subsequent versions, the mode is interactive if !isTRUE(getOption(\"knitr.in.progress\")) is TRUE.↩︎ The _targets.R file from Target Markdown never changes from chunk to chunk or report to report, so you can spread your work over multiple reports without worrying about aligning _targets.R scripts. Just be sure all your chunk names are unique across all the reports of a project, or you set the tar_name chunk option to specify base names of script file paths.↩︎ In addition, for bookdown projects, chunk labels should only use alphanumeric characters and dashes.↩︎ "],["dynamic.html", "Chapter 11 Dynamic branching 11.1 Branching 11.2 About dynamic branching 11.3 Patterns 11.4 Pattern construction 11.5 Branch provenance 11.6 Testing patterns 11.7 Branching over files 11.8 Branching over rows 11.9 Iteration 11.10 Dynamic branching performance 11.11 Batching", " Chapter 11 Dynamic branching 11.1 Branching Sometimes, a pipeline contains more targets than a user can comfortably type by hand. For projects with hundreds of targets, branching can make the _targets.R file more concise and easier to read and maintain. targets supports two types of branching: dynamic branching and static branching. Some projects are better suited to dynamic branching, while others benefit more from static branching or a combination of both. Some users understand dynamic branching more easily because it avoids metaprogramming, while others prefer static branching because tar_manifest() and tar_visnetwork() provide immediate feedback. Except for the section on dynamic-within-static branching, you can read the two chapters on branching in any order (or skip them) depending on your needs. 11.2 About dynamic branching Dynamic branching is the act of defining new targets (i.e. branches) while the pipeline is running. Prior to launching the pipeline, the user does not necessarily know which branches will spawn or how many branches there will be, and each branch’s inputs are determined at the last minute. Relative to static branching, dynamic branching is better suited to iterating over a larger number of very similar tasks (but can act as an inner layer inside static branching, as the next chapter demonstrates). 11.3 Patterns To use dynamic branching, set the pattern argument of tar_target(). A pattern is a dynamic branching specification expressed in terms of functional programming. The following minimal example explores the mechanics of patterns (and examples of branching in real-world projects are linked from here). # _targets.R library(targets) list( tar_target(w, c(1, 2)), tar_target(x, c(10, 20)), tar_target(y, w + x, pattern = map(w, x)), tar_target(z, sum(y)), tar_target(z2, length(y), pattern = map(y)) ) tar_visnetwork() tar_make() #&gt; • start target w #&gt; • built target w #&gt; • start target x #&gt; • built target x #&gt; • start branch y_61ced14d #&gt; • built branch y_61ced14d #&gt; • start branch y_4096b6a8 #&gt; • built branch y_4096b6a8 #&gt; • built pattern y #&gt; • start branch z2_275d234a #&gt; • built branch z2_275d234a #&gt; • start branch z2_8c730b35 #&gt; • built branch z2_8c730b35 #&gt; • built pattern z2 #&gt; • start target z #&gt; • built target z #&gt; • end pipeline: 2.236 seconds Above, targets w, x, and z are called stems because they do not create branches themselves. Target y is a pattern because it creates its own branches like y_4096b6a8. A branch is a special kind of target whose dependencies are slices of the targets mentioned in pattern = map(...) etc. If we read target y into memory, all the branches will load and automatically aggregate as a vector.23 tar_read(y) #&gt; y_61ced14d y_4096b6a8 #&gt; 11 22 Target z accepts this entire aggregate of y and sums it. tar_read(z) #&gt; [1] 33 Target z2 maps over y, so each each branch of z2 accepts a branch of y. tar_read(z2) #&gt; z2_275d234a z2_8c730b35 #&gt; 1 1 11.4 Pattern construction targets supports the following pattern types. map(): iterate over one or more targets in sequence. cross(): iterate over combinations of slices of targets. slice(): select individual branches slices by numeric index. For example, pattern = slice(x, index = c(3, 4)) applies the target’s command to the third and fourth slices (or branches) of upstream target x. head(): restrict branching to the first few elements. tail(): restrict branching to the last few elements. sample(): restrict branching to a random subset of elements. These patterns are composable. Below, target z creates six branches, one for each combination of w and (x, y) pair. The pattern cross(w, map(x, y)) is equivalent to tidyr::crossing(w, tidyr::nesting(x, y)). # _targets.R library(targets) list( tar_target(w_comp, seq_len(2)), tar_target(x_comp, head(letters, 3)), tar_target(y_comp, head(LETTERS, 3)), tar_target( z_comp, data.frame(w = w_comp, x = x_comp, y = y_comp), pattern = cross(w_comp, map(x_comp, y_comp)) ) ) # R console tar_make() #&gt; • start target w_comp #&gt; • built target w_comp #&gt; • start target x_comp #&gt; • built target x_comp #&gt; • start target y_comp #&gt; • built target y_comp #&gt; • start branch z_comp_a601a6f3 #&gt; • built branch z_comp_a601a6f3 #&gt; • start branch z_comp_38222c1e #&gt; • built branch z_comp_38222c1e #&gt; • start branch z_comp_b9220de4 #&gt; • built branch z_comp_b9220de4 #&gt; • start branch z_comp_c3fe35d0 #&gt; • built branch z_comp_c3fe35d0 #&gt; • start branch z_comp_1a64d49a #&gt; • built branch z_comp_1a64d49a #&gt; • start branch z_comp_7cbac5ae #&gt; • built branch z_comp_7cbac5ae #&gt; • built pattern z_comp #&gt; • end pipeline: 2.999 seconds # R console tar_read(z_comp) #&gt; w x y #&gt; 1 1 a A #&gt; 2 1 b B #&gt; 3 1 c C #&gt; 4 2 a A #&gt; 5 2 b B #&gt; 6 2 c C With slice(), you can select pieces of a pattern or upstream target as follows. # _targets.R library(targets) list( tar_target(a_data, letters), tar_target(b_data, LETTERS), tar_target( c_data, paste(a_data, b_data), pattern = slice(cross(a_data, b_data), index = seq(2, 3)) ) ) # R console tar_make() #&gt; • start target a_data #&gt; • built target a_data #&gt; • start target b_data #&gt; • built target b_data #&gt; • start branch c_data_5a10da89 #&gt; • built branch c_data_5a10da89 #&gt; • start branch c_data_3a92ccbc #&gt; • built branch c_data_3a92ccbc #&gt; • built pattern c_data #&gt; • end pipeline: 1.359 seconds # R console tar_read(c_data) #&gt; c_data_5a10da89 c_data_3a92ccbc #&gt; &quot;a B&quot; &quot;a C&quot; 11.5 Branch provenance The tar_branches() function identifies dependency relationships among individual branches. In the example pipeline below, we can find out the branch of y that each branch of z depends on. # _targets.R library(targets) list( tar_target(x, seq_len(3)), tar_target(y, x + 1, pattern = map(x)), tar_target(z, y + 1, pattern = map(y)) ) tar_make() #&gt; • start target x #&gt; • built target x #&gt; • start branch y_29239c8a #&gt; • built branch y_29239c8a #&gt; • start branch y_7cc32924 #&gt; • built branch y_7cc32924 #&gt; • start branch y_bd602d50 #&gt; • built branch y_bd602d50 #&gt; • built pattern y #&gt; • start branch z_d0697b30 #&gt; • built branch z_d0697b30 #&gt; • start branch z_959da4d3 #&gt; • built branch z_959da4d3 #&gt; • start branch z_92be5b5f #&gt; • built branch z_92be5b5f #&gt; • built pattern z #&gt; • end pipeline: 2.015 seconds branches &lt;- tar_branches(z, map(y)) branches #&gt; # A tibble: 3 × 2 #&gt; z y #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 z_d0697b30 y_29239c8a #&gt; 2 z_959da4d3 y_7cc32924 #&gt; 3 z_92be5b5f y_bd602d50 tar_read_raw(branches$y[2]) #&gt; [1] 3 However, tar_branches() is not always helpful: for example, if we look at how y branches over x. x does not use dynamic branching, so tar_branches() does not return meaningful branch names. branches &lt;- tar_branches(y, map(x)) branches #&gt; # A tibble: 3 × 2 #&gt; y x #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 y_29239c8a x_ccba877f #&gt; 2 y_7cc32924 x_084b9b29 #&gt; 3 y_bd602d50 x_91e9ed7e tar_read_raw(branches$x[2]) #&gt; Error: #&gt; ! target x_084b9b29 not found In situations like this, it is best to proactively write targets that keep track of information about their upstream branches. Data frames and tibbles are useful for this. # _targets.R library(targets) list( tar_target(x, seq_len(3)), tar_target(y, data.frame(x = x, y = x + 1), pattern = map(x)) ) tar_make() #&gt; ✔ skip target x #&gt; • start branch y_29239c8a #&gt; • built branch y_29239c8a #&gt; • start branch y_7cc32924 #&gt; • built branch y_7cc32924 #&gt; • start branch y_bd602d50 #&gt; • built branch y_bd602d50 #&gt; • built pattern y #&gt; • end pipeline: 1.372 seconds tar_read(y) #&gt; x y #&gt; 1 1 2 #&gt; 2 2 3 #&gt; 3 3 4 11.6 Testing patterns To check the correctness of a pattern without running the pipeline, use tar_pattern(). Simply supply the pattern itself and the length of each dependency target. The branch names in the data frames below are made up, but they convey a high-level picture of the branching structure. tar_pattern( cross(w_comp, map(x_comp, y_comp)), w_comp = 2, x_comp = 3, y_comp = 3 ) #&gt; # A tibble: 6 × 3 #&gt; w_comp x_comp y_comp #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 w_comp_1 x_comp_1 y_comp_1 #&gt; 2 w_comp_1 x_comp_2 y_comp_2 #&gt; 3 w_comp_1 x_comp_3 y_comp_3 #&gt; 4 w_comp_2 x_comp_1 y_comp_1 #&gt; 5 w_comp_2 x_comp_2 y_comp_2 #&gt; 6 w_comp_2 x_comp_3 y_comp_3 tar_pattern( head(cross(w_comp, map(x_comp, y_comp)), n = 2), w_comp = 2, x_comp = 3, y_comp = 3 ) #&gt; # A tibble: 2 × 3 #&gt; w_comp x_comp y_comp #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 w_comp_1 x_comp_1 y_comp_1 #&gt; 2 w_comp_1 x_comp_2 y_comp_2 tar_pattern( cross(w_comp, sample(map(x_comp, y_comp), n = 2)), w_comp = 2, x_comp = 3, y_comp = 3 ) #&gt; # A tibble: 4 × 3 #&gt; w_comp x_comp y_comp #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 w_comp_1 x_comp_2 y_comp_2 #&gt; 2 w_comp_1 x_comp_1 y_comp_1 #&gt; 3 w_comp_2 x_comp_2 y_comp_2 #&gt; 4 w_comp_2 x_comp_1 y_comp_1 11.7 Branching over files Dynamic branching over files is tricky. A target with format = \"file\" treats the entire set of files as an irreducible bundle. That means in order to branch over files downstream, each file must already have its own branch. # _targets.R library(targets) list( tar_target(paths, c(&quot;a.csv&quot;, &quot;b.csv&quot;)), tar_target(files, paths, format = &quot;file&quot;, pattern = map(paths)), tar_target(data, read_csv(files), pattern = map(files)) ) The tar_files() function from the tarchetypes package is shorthand for the first two targets above. # _targets.R library(targets) library(tarchetypes) list( tar_files(files, c(&quot;a.csv&quot;, &quot;b.csv&quot;)), tar_target(data, read_csv(files), pattern = map(files)) ) 11.8 Branching over rows By default, if you dynamically branch over a data frame, the branches are the rows. As later sections explain, this behavior changes if you set the iteration argument of tar_target() to anything other than \"vector\" (default). # _targets.R library(targets) list( tar_target(x, head(mtcars[, seq_len(3)])), tar_target(y, x, pattern = map(x), iteration = &quot;vector&quot;) # &quot;vector&quot; is already default ) tar_make() #&gt; • start target x #&gt; • built target x #&gt; • start branch y_ab68055f #&gt; • built branch y_ab68055f #&gt; • start branch y_82fbaf4b #&gt; • built branch y_82fbaf4b #&gt; • start branch y_711d3bba #&gt; • built branch y_711d3bba #&gt; • start branch y_c726abf9 #&gt; • built branch y_c726abf9 #&gt; • start branch y_ad48cb8f #&gt; • built branch y_ad48cb8f #&gt; • start branch y_981c388a #&gt; • built branch y_981c388a #&gt; • built pattern y #&gt; • end pipeline: 2.454 seconds tar_read(y, branches = 2) #&gt; mpg cyl disp #&gt; Mazda RX4 Wag 21 6 160 iteration = \"vector\" also tells tar_read(y) to aggregate all the loaded branches into a data frame. tar_read(y, branches = c(1, 2, 5)) #&gt; mpg cyl disp #&gt; Mazda RX4 21.0 6 160 #&gt; Mazda RX4 Wag 21.0 6 160 #&gt; Hornet Sportabout 18.7 8 360 In addition, helpers in tarchetypes can branch over entire groups of rows. Row grouping happens through iteration = \"group\", as explained later. tar_group_by(): define row groups using dplyr::group_by() semantics. tar_group_select(): define row groups using tidyselect semantics. tar_group_count(): define a given number row groups. tar_group_size(): define row groups of a given size. If you define a target with one of these functions, all downstream dynamic targets will automatically branch over the row groups. # _targets.R file: library(targets) library(tarchetypes) produce_data &lt;- function() { expand.grid(var1 = c(&quot;a&quot;, &quot;b&quot;), var2 = c(&quot;c&quot;, &quot;d&quot;), rep = c(1, 2, 3)) } list( tar_group_by(data, produce_data(), var1, var2), tar_target(group, data, pattern = map(data)) ) # R console: library(targets) tar_make() #&gt; • start target data #&gt; • built target data #&gt; • start branch group_b3d7d010 #&gt; • built branch group_b3d7d010 #&gt; • start branch group_6a76c5c0 #&gt; • built branch group_6a76c5c0 #&gt; • start branch group_164b16bf #&gt; • built branch group_164b16bf #&gt; • start branch group_f5aae602 #&gt; • built branch group_f5aae602 #&gt; • built pattern group #&gt; • end pipeline: 1.222 seconds # First row group: tar_read(group, branches = 1) #&gt; # A tibble: 3 × 4 #&gt; var1 var2 rep tar_group #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 a c 1 1 #&gt; 2 a c 2 1 #&gt; 3 a c 3 1 # Second row group: tar_read(group, branches = 2) #&gt; # A tibble: 3 × 4 #&gt; var1 var2 rep tar_group #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 a d 1 2 #&gt; 2 a d 2 2 #&gt; 3 a d 3 2 For more information on how the row grouping mechanism works, see the section on “group” iteration below. 11.9 Iteration The iteration arguments of tar_target() and tar_option_set() control the slicing of stems and the aggregation of patterns. For example, if you write a stem target with tar_target(x, fun(), iteration = \"list\"), then all downstream targets that branch over x will give list-like slices x[[1]], x[[2]], etc. to the individual branches instead of the default vector-like slices x[1], x[2], etc. Likewise, for a pattern defined with tar_target(y, fun(x), pattern = map(x), iteration = \"list\"), tar_read(y) will return a list instead of a vector, and a downstream target like tar_target(y, fun(z)) will treat z as a list instead of a vector.24 11.9.1 Vector iteration targets uses vector iteration by default, and you can opt into this behavior by setting iteration = \"vector\" in tar_target(). In vector iteration, targets uses the vctrs package to split stems and aggregate branches. That means vctrs::vec_slice() slices up stems like x for mapping, and vctrs::vec_c() aggregates patterns like y for operations like tar_read(). For atomic vectors like in the example above, this behavior is already intuitive. But if we map over a data frame, each branch will get a row of the data frame due to vector iteration. library(targets) print_and_return &lt;- function(x) { print(x) x } list( tar_target( x, data.frame(a = c(1, 2), b = c(&quot;a&quot;, &quot;b&quot;)), # Already the default unless tar_option_get(iteration&quot;) is not &quot;vector&quot;: iteration = &quot;vector&quot; ), tar_target( y, print_and_return(x), pattern = map(x), # Already the default unless tar_option_get(iteration&quot;) is not &quot;vector&quot;: iteration = &quot;vector&quot; ) ) tar_make() #&gt; • start target x #&gt; • built target x #&gt; • start branch y_2e131731 #&gt; a b #&gt; 1 1 a #&gt; • built branch y_2e131731 #&gt; • start branch y_6c3b7977 #&gt; a b #&gt; 1 2 b #&gt; • built branch y_6c3b7977 #&gt; • built pattern y #&gt; • end pipeline: 1.411 seconds And since y also has iteration = \"vector\", the aggregate of y is a single data frame of all the rows. tar_read(y) #&gt; a b #&gt; 1 1 a #&gt; 2 2 b 11.9.2 List iteration List iteration splits and aggregates targets as simple lists. If a non-dynamic target x has list iteration, then all branches of downstream patterns will get x[[1]], x[[2]], and so on. If a dynamic target y has list iteration, then all targets downstream of y will see y as a list, and tar_read(y) will be a list. The following example demonstrates the slicing and aggregation behavior of list iteration. Because stem target x uses list iteration, the downstream target y maps over the columns of the data frame instead of the rows. Likewise, tar_read(z) is a list because z is a dynamic target with list iteration. # _targets.R library(targets) print_and_return &lt;- function(x) { print(x) x } list( tar_target( x, data.frame(a = c(1, 2), b = c(&quot;a&quot;, &quot;b&quot;)), iteration = &quot;list&quot; ), tar_target(y, print_and_return(x), pattern = map(x)), tar_target(z, x, pattern = map(x), iteration = &quot;list&quot;) ) tar_make() #&gt; • start target x #&gt; • built target x #&gt; • start branch y_651b13c7 #&gt; [1] 1 2 #&gt; • built branch y_651b13c7 #&gt; • start branch y_1c8c7726 #&gt; [1] &quot;a&quot; &quot;b&quot; #&gt; • built branch y_1c8c7726 #&gt; • built pattern y #&gt; • start branch z_651b13c7 #&gt; • built branch z_651b13c7 #&gt; • start branch z_1c8c7726 #&gt; • built branch z_1c8c7726 #&gt; • built pattern z #&gt; • end pipeline: 2.203 seconds tar_read(y) #&gt; Error: #&gt; ! Can&#39;t combine `y_651b13c7` &lt;double&gt; and `y_1c8c7726` &lt;character&gt;. tar_read(z) #&gt; $z_651b13c7 #&gt; [1] 1 2 #&gt; #&gt; $z_1c8c7726 #&gt; [1] &quot;a&quot; &quot;b&quot; 11.9.3 Group iteration Group iteration brings dplyr::group_by() functionality to patterns. This way, we can map or cross over custom subsets of rows. Consider the following data frame. object &lt;- data.frame( x = seq_len(6), id = rep(letters[seq_len(3)], each = 2) ) object #&gt; x id #&gt; 1 1 a #&gt; 2 2 a #&gt; 3 3 b #&gt; 4 4 b #&gt; 5 5 c #&gt; 6 6 c To map over the groups of rows defined by the id column, we Use group_by() and tar_group() to define the groups of rows, and Use iteration = \"group\" in tar_target() to tell downstream patterns to use the row groups. Put together, the pipeline looks like this. # _targets.R library(targets) tar_option_set(packages = &quot;dplyr&quot;) list( tar_target( data, data.frame( x = seq_len(6), id = rep(letters[seq_len(3)], each = 2) ) %&gt;% group_by(id) %&gt;% tar_group(), iteration = &quot;group&quot; ), tar_target( subsets, data, pattern = map(data), iteration = &quot;list&quot; ) ) tar_make() #&gt; • start target data #&gt; • built target data #&gt; • start branch subsets_f6af59da #&gt; • built branch subsets_f6af59da #&gt; • start branch subsets_f91945cc #&gt; • built branch subsets_f91945cc #&gt; • start branch subsets_cae649ac #&gt; • built branch subsets_cae649ac #&gt; • built pattern subsets #&gt; • end pipeline: 1.559 seconds lapply(tar_read(subsets), as.data.frame) #&gt; $subsets_f6af59da #&gt; x id tar_group #&gt; 1 1 a 1 #&gt; 2 2 a 1 #&gt; #&gt; $subsets_f91945cc #&gt; x id tar_group #&gt; 1 3 b 2 #&gt; 2 4 b 2 #&gt; #&gt; $subsets_cae649ac #&gt; x id tar_group #&gt; 1 5 c 3 #&gt; 2 6 c 3 Row groups are defined in the special tar_group column created by tar_group(). data.frame( x = seq_len(6), id = rep(letters[seq_len(3)], each = 2) ) %&gt;% dplyr::group_by(id) %&gt;% tar_group() #&gt; # A tibble: 6 × 3 #&gt; x id tar_group #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 1 a 1 #&gt; 2 2 a 1 #&gt; 3 3 b 2 #&gt; 4 4 b 2 #&gt; 5 5 c 3 #&gt; 6 6 c 3 tar_group() creates this column based on the orderings of the grouping variables supplied to dplyr::group_by(), not the order of the rows in the data. flip_order &lt;- function(x) { ordered(x, levels = sort(unique(x), decreasing = TRUE)) } data.frame( x = seq_len(6), id = flip_order(rep(letters[seq_len(3)], each = 2)) ) %&gt;% dplyr::group_by(id) %&gt;% tar_group() #&gt; # A tibble: 6 × 3 #&gt; x id tar_group #&gt; &lt;int&gt; &lt;ord&gt; &lt;int&gt; #&gt; 1 1 a 3 #&gt; 2 2 a 3 #&gt; 3 3 b 2 #&gt; 4 4 b 2 #&gt; 5 5 c 1 #&gt; 6 6 c 1 The ordering in tar_group agrees with the ordering shown by dplyr::group_keys(). data.frame( x = seq_len(6), id = flip_order(rep(letters[seq_len(3)], each = 2)) ) %&gt;% dplyr::group_by(id) %&gt;% dplyr::group_keys() #&gt; # A tibble: 3 × 1 #&gt; id #&gt; &lt;ord&gt; #&gt; 1 c #&gt; 2 b #&gt; 3 a Branches are arranged in increasing order with respect to the integers in tar_group. # _targets.R library(targets) tar_option_set(packages = &quot;dplyr&quot;) flip_order &lt;- function(x) { ordered(x, levels = sort(unique(x), decreasing = TRUE)) } list( tar_target( data, data.frame( x = seq_len(6), id = flip_order(rep(letters[seq_len(3)], each = 2)) ) %&gt;% group_by(id) %&gt;% tar_group(), iteration = &quot;group&quot; ), tar_target( subsets, data, pattern = map(data), iteration = &quot;list&quot; ) ) tar_make() #&gt; • start target data #&gt; • built target data #&gt; • start branch subsets_13c06a5c #&gt; • built branch subsets_13c06a5c #&gt; • start branch subsets_f38b90a2 #&gt; • built branch subsets_f38b90a2 #&gt; • start branch subsets_d2717707 #&gt; • built branch subsets_d2717707 #&gt; • built pattern subsets #&gt; • end pipeline: 1.942 seconds lapply(tar_read(subsets), as.data.frame) #&gt; $subsets_13c06a5c #&gt; x id tar_group #&gt; 1 5 c 1 #&gt; 2 6 c 1 #&gt; #&gt; $subsets_f38b90a2 #&gt; x id tar_group #&gt; 1 3 b 2 #&gt; 2 4 b 2 #&gt; #&gt; $subsets_d2717707 #&gt; x id tar_group #&gt; 1 1 a 3 #&gt; 2 2 a 3 11.10 Dynamic branching performance With dynamic branching, it is super easy to create an enormous number of targets. But when the number of targets starts to exceed a couple hundred, tar_make() slows down, and graphs from tar_visnetwork() start to become unmanageable. If that happens to you, you might be tempted to use the names and shortcut arguments to tar_make(), e.g. tar_make(names = all_of(\"only\", \"these\", \"targets\"), shortcut = TRUE), to completely temporarily omit thousands of upstream targets for the sake of concentrating on one section at a time. However, this technique is only a temporary measure, and it is best to eventually revert back to the default names = NULL and shortcut = FALSE to ensure reproducibility. Another temporary workaround is to temporarily select subsets of branches. For example, instead of pattern = map(large_target) in tar_target(), you could prototype on a target that uses pattern = head(map(large_target), n = 1) or pattern = slice(map(large_target), c(4, 5, 6)). In the case of slice(), the tar_branch_index() function (only in targets version 0.5.0.9000 and above) can help you find the required integer indexes corresponding to individual branch names you may want. However, both the above solutions are temporary workarounds for the sake of prototyping a pipeline and chipping away at a large computation. To help the entire pipeline scale more efficiently, consider batching your work into a smaller number of targets. 11.11 Batching Targetopia packages usually have functions that support batching for various use cases. In stantargets, tar_stan_mcmc_rep_summary() and friends automatically use batching behind the scenes. The user simply needs to select the number of batches and number of reps per batch. Each batch is a dynamic branch with multiple reps, and each rep fits the user’s model once and computes summary statistics. In tarchetypes, tar_rep() is a general-use target factory for dynamic branching. It allows you to repeat arbitrary code over multiple reps split into multiple batches. Each batch gets its own reproducible random number seed generated from the target name (as do all targets) and reps run sequentially within each batch, so the results are reproducible. The targets-stan repository has an example of batching implemented from scratch. The goal of the pipeline is to validate a Bayesian model by simulating thousands of dataset, analyzing each with a Bayesian model, and assessing the overall accuracy of the inference. Rather than define a target for each dataset in model, the pipeline breaks up the work into batches, where each batch has multiple datasets or multiple analyses. Here is a version of the pipeline with 40 batches and 25 simulation reps per batch (1000 reps total in a pipeline of 82 targets). # _targets.R library(targets) list( tar_target(model_file, compile_model(&quot;stan/model.stan&quot;), format = &quot;file&quot;), tar_target(index_batch, seq_len(40)), tar_target(index_sim, seq_len(25)), tar_target( data_continuous, purrr::map_dfr(index_sim, ~simulate_data_continuous()), pattern = map(index_batch) ), tar_target( fit_continuous, map_sims(data_continuous, model_file = model_file), pattern = map(data_continuous) ) ) To use list aggregation, write tar_target(y, w + x, pattern = map(w, x), iteration = \"list\") in the pipeline.↩︎ iteration does not control the aggregation of stems because stems are already aggregated, so there is nothing to be done. Likewise, it does not control the splitting of patterns because the pattern argument of tar_target() already does that.↩︎ "],["static.html", "Chapter 12 Static branching 12.1 Branching 12.2 When to use static branching 12.3 Map 12.4 Dynamic-within-static branching 12.5 Combine 12.6 Metaprogramming 12.7 Hooks", " Chapter 12 Static branching 12.1 Branching Sometimes, a pipeline contains more targets than a user can comfortably type by hand. For projects with hundreds of targets, branching can make the _targets.R file more concise and easier to read and maintain. targets supports two types of branching: dynamic branching and static branching. Some projects are better suited to dynamic branching, while others benefit more from static branching or a combination of both. Some users understand dynamic branching more easily because it avoids metaprogramming, while others prefer static branching because tar_manifest() and tar_visnetwork() provide immediate feedback. Except for the section on dynamic-within-static branching, you can read the two chapters on branching in any order (or skip them) depending on your needs. 12.2 When to use static branching Static branching is the act of defining a group of targets in bulk before the pipeline starts. Whereas dynamic branching uses last-minute dependency data to define the branches, static branching uses metaprogramming to modify the code of the pipeline up front. Whereas dynamic branching excels at creating a large number of very similar targets, static branching is most useful for smaller number of heterogeneous targets. Some users find it more convenient because they can use tar_manifest() and tar_visnetwork() to check the correctness of static branching before launching the pipeline. 12.3 Map tar_map() from the tarchetypes package creates copies of existing target objects, where each new command is a variation on the original. In the example below, we have a data analysis workflow that iterates over datasets and analysis methods. The values data frame has the operational parameters of each data analysis, and tar_map() creates one new target per row. # _targets.R file: library(targets) library(tarchetypes) library(tibble) values &lt;- tibble( method_function = rlang::syms(c(&quot;method1&quot;, &quot;method2&quot;)), data_source = c(&quot;NIH&quot;, &quot;NIAID&quot;) ) targets &lt;- tar_map( values = values, tar_target(analysis, method_function(data_source, reps = 10)), tar_target(summary, summarize_analysis(analysis, data_source)) ) list(targets) tar_manifest() #&gt; # A tibble: 4 × 3 #&gt; name command pattern #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 analysis_method2_NIAID &quot;method2(\\&quot;NIAID\\&quot;, reps = 10)&quot; &lt;NA&gt; #&gt; 2 analysis_method1_NIH &quot;method1(\\&quot;NIH\\&quot;, reps = 10)&quot; &lt;NA&gt; #&gt; 3 summary_method2_NIAID &quot;summarize_analysis(analysis_method2_NIAID, \\&quot;… &lt;NA&gt; #&gt; 4 summary_method1_NIH &quot;summarize_analysis(analysis_method1_NIH, \\&quot;NI… &lt;NA&gt; tar_visnetwork(targets_only = TRUE) For shorter target names, use the names argument of tar_map(). And for more combinations of settings, use tidyr::expand_grid() on values. # _targets.R file: library(targets) library(tarchetypes) library(tidyr) values &lt;- expand_grid( # Use all possible combinations of input settings. method_function = rlang::syms(c(&quot;method1&quot;, &quot;method2&quot;)), data_source = c(&quot;NIH&quot;, &quot;NIAID&quot;) ) targets &lt;- tar_map( values = values, names = &quot;data_source&quot;, # Select columns from `values` for target names. tar_target(analysis, method_function(data_source, reps = 10)), tar_target(summary, summarize_analysis(analysis, data_source)) ) list(targets) tar_manifest() #&gt; # A tibble: 8 × 3 #&gt; name command pattern #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 analysis_NIAID_1 &quot;method2(\\&quot;NIAID\\&quot;, reps = 10)&quot; &lt;NA&gt; #&gt; 2 analysis_NIAID &quot;method1(\\&quot;NIAID\\&quot;, reps = 10)&quot; &lt;NA&gt; #&gt; 3 analysis_NIH_1 &quot;method2(\\&quot;NIH\\&quot;, reps = 10)&quot; &lt;NA&gt; #&gt; 4 analysis_NIH &quot;method1(\\&quot;NIH\\&quot;, reps = 10)&quot; &lt;NA&gt; #&gt; 5 summary_NIAID_1 &quot;summarize_analysis(analysis_NIAID_1, \\&quot;NIAID\\&quot;)&quot; &lt;NA&gt; #&gt; 6 summary_NIAID &quot;summarize_analysis(analysis_NIAID, \\&quot;NIAID\\&quot;)&quot; &lt;NA&gt; #&gt; 7 summary_NIH_1 &quot;summarize_analysis(analysis_NIH_1, \\&quot;NIH\\&quot;)&quot; &lt;NA&gt; #&gt; 8 summary_NIH &quot;summarize_analysis(analysis_NIH, \\&quot;NIH\\&quot;)&quot; &lt;NA&gt; # You may need to zoom out on this interactive graph to see all 8 targets. tar_visnetwork(targets_only = TRUE) 12.4 Dynamic-within-static branching You can even combine together static and dynamic branching. The static tar_map() is an excellent outer layer on top of targets with patterns. The following is a sketch of a pipeline that runs each of two data analysis methods 10 times, once per random seed. Static branching iterates over the method functions, while dynamic branching iterates over the seeds. tar_map() creates new patterns as well as new commands. So below, the summary methods map over the analysis methods both statically and dynamically. # _targets.R file: library(targets) library(tarchetypes) library(tibble) random_seed_target &lt;- tar_target(random_seed, seq_len(10)) targets &lt;- tar_map( values = tibble(method_function = rlang::syms(c(&quot;method1&quot;, &quot;method2&quot;))), tar_target( analysis, method_function(&quot;NIH&quot;, seed = random_seed), pattern = map(random_seed) ), tar_target( summary, summarize_analysis(analysis), pattern = map(analysis) ) ) list(random_seed_target, targets) tar_manifest() #&gt; # A tibble: 5 × 3 #&gt; name command pattern #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 random_seed &quot;seq_len(10)&quot; &lt;NA&gt; #&gt; 2 analysis_method1 &quot;method1(\\&quot;NIH\\&quot;, seed = random_seed)&quot; map(random_seed) #&gt; 3 analysis_method2 &quot;method2(\\&quot;NIH\\&quot;, seed = random_seed)&quot; map(random_seed) #&gt; 4 summary_method1 &quot;summarize_analysis(analysis_method1)&quot; map(analysis_method1) #&gt; 5 summary_method2 &quot;summarize_analysis(analysis_method2)&quot; map(analysis_method2) tar_visnetwork(targets_only = TRUE) 12.5 Combine tar_combine() from the tarchetypes package creates a new target to aggregate the results of upstream targets. In the simple example below, our combined target simply aggregates the rows returned from two other targets. # _targets.R file: library(targets) library(tarchetypes) library(tibble) options(crayon.enabled = FALSE) target1 &lt;- tar_target(head, head(mtcars, 1)) target2 &lt;- tar_target(tail, tail(mtcars, 1)) target3 &lt;- tar_combine(combined_target, target1, target2) list(target1, target2, target3) tar_manifest() #&gt; # A tibble: 3 × 3 #&gt; name command pattern #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 head_mtcars head(mtcars, 1) &lt;NA&gt; #&gt; 2 tail_mtcars tail(mtcars, 1) &lt;NA&gt; #&gt; 3 combined_target vctrs::vec_c(head_mtcars = head_mtcars, tail_mtcars =… &lt;NA&gt; tar_visnetwork(targets_only = TRUE) tar_make() #&gt; • start target head_mtcars #&gt; • built target head_mtcars #&gt; • start target tail_mtcars #&gt; • built target tail_mtcars #&gt; • start target combined_target #&gt; • built target combined_target #&gt; • end pipeline: 2.204 seconds tar_read(combined_target) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21.0 6 160 110 3.90 2.62 16.46 0 1 4 4 #&gt; Volvo 142E 21.4 4 121 109 4.11 2.78 18.60 1 1 4 2 To use tar_combine() and tar_map() together in more complicated situations, you may need to supply unlist = FALSE to tar_map(). That way, tar_map() will return a nested list of target objects, and you can combine the ones you want. The pipeline extends our previous tar_map() example by combining just the summaries, omitting the analyses from tar_combine(). Also note the use of bind_rows(!!!.x) below. This is how you supply custom code to combine the return values of other targets. .x is a placeholder for the return values, and !!! is the “unquote-splice” operator from the rlang package. # _targets.R file: library(targets) library(tarchetypes) library(tibble) random_seed &lt;- tar_target(random_seed, seq_len(10)) mapped &lt;- tar_map( unlist = FALSE, # Return a nested list from tar_map() values = tibble(method_function = rlang::syms(c(&quot;method1&quot;, &quot;method2&quot;))), tar_target( analysis, method_function(&quot;NIH&quot;, seed = random_seed), pattern = map(random_seed) ), tar_target( summary, summarize_analysis(analysis), pattern = map(analysis) ) ) combined &lt;- tar_combine( combined_summaries, mapped[[2]], command = dplyr::bind_rows(!!!.x, .id = &quot;method&quot;) ) list(random_seed, mapped, combined) tar_manifest() #&gt; # A tibble: 6 × 3 #&gt; name command pattern #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 random_seed &quot;seq_len(10)&quot; &lt;NA&gt; #&gt; 2 analysis_method1 &quot;method1(\\&quot;NIH\\&quot;, seed = random_seed)&quot; map(ra… #&gt; 3 analysis_method2 &quot;method2(\\&quot;NIH\\&quot;, seed = random_seed)&quot; map(ra… #&gt; 4 summary_method1 &quot;summarize_analysis(analysis_method1)&quot; map(an… #&gt; 5 summary_method2 &quot;summarize_analysis(analysis_method2)&quot; map(an… #&gt; 6 combined_summaries &quot;dplyr::bind_rows(summary_method1 = summary_method… &lt;NA&gt; tar_visnetwork(targets_only = TRUE) 12.6 Metaprogramming Custom metaprogramming is a more flexible alternative to tar_map() and tar_combine(). tar_eval() from tarchetypes accepts an arbitrary expression and iteratively plugs in symbols. Below, we use it to branch over datasets. # _targets.R library(rlang) library(targets) library(tarchetypes) string &lt;- c(&quot;gapminder&quot;, &quot;who&quot;, &quot;imf&quot;) symbol &lt;- syms(string) tar_eval( tar_target(symbol, get_data(string)), values = list(string = string, symbol = symbol) ) tar_eval() has fewer guardrails than tar_map() or tar_combine(), so tar_manifest() is especially important for checking the correctness of your metaprogramming. tar_manifest(fields = command) #&gt; # A tibble: 3 × 2 #&gt; name command #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 imf &quot;get_data(\\&quot;imf\\&quot;)&quot; #&gt; 2 gapminder &quot;get_data(\\&quot;gapminder\\&quot;)&quot; #&gt; 3 who &quot;get_data(\\&quot;who\\&quot;)&quot; 12.7 Hooks Hooks are supported in tarchtypes version 0.2.0 and above, and they allow you to prepend or wrap code in multiple targets at a time. For example, tar_hook_before() is a robust way to invoke the conflicted package to resolve namespace conflicts that works with distributed computing and does not require a project-level .Rprofile file. # _targets.R file library(tarchetypes) library(magrittr) tar_option_set(packages = c(&quot;conflicted&quot;, &quot;dplyr&quot;)) source(&quot;R/functions.R&quot;) list( tar_target(data, get_time_series_data()), tar_target(analysis1, analyze_months(data)), tar_target(analysis2, analyze_weeks(data)) ) %&gt;% tar_hook_before( hook = conflicted_prefer(&quot;filter&quot;, &quot;dplyr&quot;), names = starts_with(&quot;analysis&quot;) ) # R console targets::tar_manifest(fields = command) #&gt; # A tibble: 3 × 2 #&gt; name command #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 data &quot;get_time_series_data()&quot; #&gt; 2 analysis1 &quot;{ \\\\n conflicted_prefer(\\&quot;filter\\&quot;, \\&quot;dplyr\\&quot;) \\\\n analyze… #&gt; 3 analysis2 &quot;{ \\\\n conflicted_prefer(\\&quot;filter\\&quot;, \\&quot;dplyr\\&quot;) \\\\n analyze… Similarly, tar_hook_outer() wraps expressions around target commands, and tar_hook_inner() wraps expressions around target dependencies. These hooks could potentially help encrypt targets before storage in _targets/ and decrypt targets before retrieval, as demonstrated in the sketch below. Data security is the sole responsibility of the user and not the responsibility of targets, tarchetypes, or related pipeline packages. You as the user are responsible for validating your own target specifications and custom code and applying additional security precautions as appropriate for the situation. # _targets.R file library(tarchetypes) library(magrittr) list( tar_target(data1, get_data1()), tar_target(data2, get_data2()), tar_target(analysis, analyze(data1, data2)) ) %&gt;% tar_hook_outer(encrypt(.x, threads = 2)) %&gt;% tar_hook_inner(decrypt(.x)) # R console targets::tar_manifest(fields = command) #&gt; # A tibble: 3 × 2 #&gt; name command #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 data1 encrypt(get_data1(), threads = 2) #&gt; 2 data2 encrypt(get_data2(), threads = 2) #&gt; 3 analysis encrypt(analyze(decrypt(data1), decrypt(data2)), threads = 2) "],["hpc.html", "Chapter 13 High-performance computing 13.1 Clustermq 13.2 Future 13.3 Advanced 13.4 Cloud computing", " Chapter 13 High-performance computing targets supports high-performance computing with the tar_make_clustermq() and tar_make_future() functions. These functions are like tar_make(), but they allow multiple targets to run simultaneously over parallel workers. These workers can be processes on your local machine, or they can be jobs on a computing cluster. The main process automatically sends a target to a worker as soon as The worker is available, and All the target’s upstream dependency targets have been checked or built. Practical real-world examples of high-performance computing in targets can be found at the examples linked from here. But for the purposes of explaining the mechanics of the package, consider the following sketch of a pipeline. # _targets.R library(targets) list( tar_target(data, get_data()), tar_target(fast_fit, fit_small_model(data)), tar_target(slow_fit, fit_slow_model(data)), tar_target(plot_1, make_plot(fast_fit)), tar_target(plot_2, make_plot(slow_fit)) ) # R console tar_visnetwork() When we run this pipeline with high-performance computing, targets automatically knows to wait for data to finish running before moving on to the other targets. Once data is finished, it moves on to targets fast_fit and slow_fit. If fast_fit finishes before slow_fit, target plot_1 begins even as slow_fit is still running. Unlike drake, targets applies this behavior not only to stem targets, but also to branches of patterns. The following sections cover the mechanics and configuration details of high-performance computing in targets. 13.1 Clustermq tar_make_clustermq() uses the clustermq package, and prior familiarity with clustermq is extremely helpful for configuring targets and diagnosing errors. So before you use tar_make_clustermq(), please read the documentation at https://mschubert.github.io/clustermq/ and try out clustermq directly. If you plan to use a scheduler like SLURM or SGE, please configure and experiment with clustermq on your scheduler without targets. And if you later experience issues with tar_make_clustermq(), try to isolate the problem by creating a reproducible example that uses clustermq and not targets. Peeling back layers can help isolate problems and point toward specific solutions, and targets is usually one of the outer layers. 13.1.1 Persistent workers tar_make_clustermq() uses persistent workers. That means all the parallel processes launch together as soon as there is a target to build, and all the processes keep running until the pipeline winds down. The video clip below visualizes the concept. If use_targets() function configures clustermq automatically (if clustermq is installed). use_targets() tries to detect the cluster you are using and write the appropriate _targets.R settings clustermq configuration files commensurate with your system resources. On most systems, you should be able to run e.g. tar_make_clustermq(workers = 2) right away. In case you have specific configuration needs, however, read on to learn how to set it up yourself. 13.1.2 Clustermq installation Persistent workers require the clustermq R package, which in turn requires ZeroMQ. Please refer to the clustermq installation guide for specific instructions. 13.1.3 Clustermq local configuration If you manually write clustermq settings in _targets.R, be sure to set the clustermq.scheduler global option to a a local scheduler like \"multiprocess\". Many of the supported schedulers and their configuration details are listed here. # _targets.R options(clustermq.scheduler = &quot;multiprocess&quot;) list( tar_target(data, get_data()), tar_target(fast_fit, fit_small_model(data)), tar_target(slow_fit, fit_slow_model(data)), tar_target(plot_1, make_plot(fast_fit)), tar_target(plot_2, make_plot(slow_fit)) ) Then, run tar_make_clustermq() with the appropriate number of workers. # R console tar_make_clustermq(workers = 2) 13.1.4 Clustermq remote configuration For parallel computing on a cluster, Choose a scheduler listed here that corresponds to your cluster’s resource manager. Create a template file that configures the computing requirements and other settings for the cluster. Supply the scheduler option and template file to the clustermq.scheduler and clustermq.template global options in your target script file (default: _targets.R). # _targets.R options(clustermq.scheduler = &quot;sge&quot;, clustermq.template = &quot;sge.tmpl&quot;) list( tar_target(data, get_data()), tar_target(fast_fit, fit_small_model(data)), tar_target(slow_fit, fit_slow_model(data)), tar_target(plot_1, make_plot(fast_fit)), tar_target(plot_2, make_plot(slow_fit)) ) Above, sge_tmpl refers to a template file like the one below. ## From https://github.com/mschubert/clustermq/wiki/SGE #$ -N {{ job_name }} # Worker name. #$ -t 1-{{ n_jobs }} # Submit workers as an array. #$ -j y # Combine stdout and stderr into one worker log file. #$ -o /dev/null # Worker log files. #$ -cwd # Use project root as working directory. #$ -V # Use environment variables. module load R/3.6.3 # Needed if R is an environment module on the cluster. CMQ_AUTH={{ auth }} R --no-save --no-restore -e &#39;clustermq:::worker(&quot;{{ main }}&quot;)&#39; # Leave alone. Then, run tar_make_clustermq() as before. # R console tar_make_clustermq(workers = 2) See the examples linked from here to see how this setup works in real-world projects. 13.1.5 Clustermq template file configuration In addition to configuration options hard-coded in the template file, you can supply custom computing resources with the resources argument of tar_option_set(). As an example, let’s use a wildcard for the number of cores per worker on an SGE cluster. In the template file, supply {{ num_cores }} wildcard to the -pe smp flag. #$ -pe smp {{ num_cores }} # Number of cores per worker #$ -N {{ job_name | 1 }} #$ -t 1-{{ n_jobs }} #$ -j y #$ -o /dev/null #$ -cwd #$ -V module load R/3.6.3 CMQ_AUTH={{ auth }} R --no-save --no-restore -e &#39;clustermq:::worker(&quot;{{ main }}&quot;)&#39; Then, supply the value of num_cores to the resources option from within the target script file (default: _targets.R). In older version of targets, resources was a named list. In targets 0.5.0.9000 and above, please create the resources argument with helpers tar_resources() and tar_resources_clustermq(). # _targets.R # With older versions of targets: # tar_option_set(resources = list(num_cores = 2)) # With targets &gt;= 0.5.0.9000: tar_option_set( resources = tar_resources( clustermq = tar_resources_clustermq(template = list(num_cores = 2)) ) ) list( tar_target(...), ... # more targets ) Finally, call tar_make_clustermq() normally. # R console tar_make_clustermq(workers = 2) This particular use case comes up when you have custom parallel computing within targets and need to take advantage of multiple cores. 13.2 Future tar_make_future() uses the future package, and prior familiarity with future is extremely helpful for configuring targets and diagnosing errors. So before you use tar_make_future(), please read the documentation at https://future.futureverse.org/ and try out future directly, ideally with backends like future.callr and possibly future.batchtools. If you plan to use a scheduler like SLURM or SGE, please configure and experiment with future on your scheduler without targets. And if you later experience issues with tar_make_future(), try to isolate the problem by creating a reproducible example that uses future and not targets. Same goes for future.batchtools if applicable. Peeling back layers can help isolate problems and point toward specific solutions, and targets is usually one of the outer layers. The use_targets() function configures future and/or future.batchtools automatically (if those packages are installed). It tries to detect the cluster you are using, if any, and write the appropriate _targets.R settings clustermq configuration files commensurate with your system resources. On most systems, you should be able to run e.g. tar_make_future(workers = 2) right away. In case you have specific configuration needs, however, read on to learn how to set it up yourself. 13.2.1 Transient workers tar_make_future() runs transient workers. That means each target gets its own worker which initializes when the target begins and terminates when the target ends. The following video clip demonstrates the concept. 13.2.2 Future installation Install the future package. install.packages(&quot;future&quot;) If you intend to use a cluster, be sure to install the future.batchtools package too. install.packages(&quot;future.batchtools&quot;) The future ecosystem contains even more packages that extend future’s parallel computing functionality, such as future.callr. install.packages(&quot;future.callr&quot;) 13.2.3 Future locally To parallelize targets over multiple processes on your local machine, declare a future plan in your target script file (default: _targets.R). The callr plan from the future.callr package is recommended.25 It is crucial that future::plan() is called in the target script file itself - defining a plan interactively before invoking tar_make_future() does not leverage the future package. # _targets.R library(future) library(future.callr) plan(callr) list( tar_target(data, get_data()), tar_target(fast_fit, fit_small_model(data)), tar_target(slow_fit, fit_slow_model(data)), tar_target(plot_1, make_plot(fast_fit)), tar_target(plot_2, make_plot(slow_fit)) ) Then, run tar_make_future() with the desired number of workers. Here, the workers argument specifies the maximum number of transient workers to allow at a given time. Some future plans also have optional workers arguments that set their own caps. # R console tar_make_future(workers = 2) 13.2.4 Future remotely To run transient workers on a cluster, first install the future.batchtools package. Then, set one of these plans in your target script file (default: _targets.R). # _targets.R library(future) library(future.batchtools) plan(batchtools_sge, template = &quot;sge.tmpl&quot;) list( tar_target(data, get_data()), tar_target(fast_fit, fit_small_model(data)), tar_target(slow_fit, fit_slow_model(data)), tar_target(plot_1, make_plot(fast_fit)), tar_target(plot_2, make_plot(slow_fit)) ) Here, our template file sge.tmpl is configured for batchtools. #!/bin/bash #$ -cwd # Run in the current working directory. #$ -j y # Direct stdout and stderr to the same file. #$ -o &lt;%= log.file %&gt; # log file #$ -V # Use environment variables. #$ -N &lt;%= job.name %&gt; # job name module load R/3.6.3 # Uncomment and adjust if R is an environment module. Rscript -e &#39;batchtools::doJobCollection(&quot;&lt;%= uri %&gt;&quot;)&#39; # Leave alone. exit 0 # Leave alone. 13.2.5 Future configuration The tar_target(), tar_target_raw(), and tar_option_set() functions accept a resources argument.26 For example, if our batchtools template file has a wildcard for the number of cores for a job, #!/bin/bash #$ -pe smp &lt;%= resources[[&quot;num_cores&quot;]] | 1 %&gt; # Wildcard for cores per job. #$ -cwd #$ -j y #$ -o &lt;%= log.file %&gt; #$ -V #$ -N &lt;%= job.name %&gt; module load R/3.6.3 Rscript -e &#39;batchtools::doJobCollection(&quot;&lt;%= uri %&gt;&quot;)&#39; exit 0 then you can set the number of cores for an individual target using a target-specific future plan. In the case below, maybe the slow model needs 2 cores to run fast enough. Because of the resources[[\"num_cores\"]] placeholder in the above template file, we can control the number of cores in each target through its local plan.27 # _targets.R library(future) library(future.batchtools) plan(batchtools_sge, template = &quot;sge.tmpl&quot;) list( tar_target(data, get_data()), tar_target(fast_fit, fit_small_model(data)), # With older version of targets: # tar_target(slow_fit, fit_slow_model(data), resources = list(num_cores = 2)), # With targets &gt;= 0.5.0.9000: tar_target( slow_fit, fit_slow_model(data), resources = tar_resources( future = tar_resources_future( plan = tweak( batchtools_sge, template = &quot;sge.tmpl&quot;, resources = list(num_cores = 2) ) ) ) ), tar_target(plot_1, make_plot(fast_fit)), tar_target(plot_2, make_plot(slow_fit)) ) Then, run tar_make_future() as usual. # R console tar_make_future(workers = 2) 13.3 Advanced Functions tar_target(), tar_target_raw(), and tar_option_set() support advanced configuration options for heavy-duty pipelines that require high-performance computing. deployment: With the deployment argument, you can choose to run some targets locally on the main process instead of on a high-performance computing worker. This options is suitable for lightweight targets such as R Markdown reports where runtime is quick and a cluster would be excessive. memory: Choose whether to retain a target in memory or remove it from memory whenever it is not needed at the moment. This is a tradeoff between memory consumption and storage read speeds, and like all of the options listed here, you can set it on a target-by-target basis. The default settings consume a lot of memory to avoid frequently reading from storage. To keep memory usage down to a minimum, set memory = \"transient\" and garbage_collection = TRUE in tar_target() or tar_option_set(). For cloud-based dynamic files such as format = \"aws_file\", this memory policy applies to temporary local copies of the file in _targets/scratch/: \"persistent\" means they remain until the end of the pipeline, and \"transient\" means they get deleted from the file system as soon as possible. The former conserves bandwidth, and the latter conserves local storage. garbage_collection: Choose whether to run base::gc() just before running the target. storage: Choose whether the parallel workers or the main process is responsible for saving the target’s value. For slow network file systems on clusters, storage = \"main\" is often faster for small numbers of targets. For large numbers of targets or low-bandwidth connections between the main and workers, storage = \"worker\" is often faster. Always choose storage = \"main\" if the workers do not have access to the file system with the _targets/ data store. retrieval: Choose whether the parallel workers or the main process is responsible for reading dependency targets from disk. Should usually be set to whatever you choose for storage (default). Always choose retrieval = \"main\" if the workers do not have access to the file system with the _targets/ data store. format: If your pipeline has large computation, it may also have large data. Consider setting the format argument to help targets store and retrieve your data faster. error: Set error to \"continue\" to let the rest of the pipeline keep running even if a target encounters an error. 13.4 Cloud computing Right now, targets does not have built-in cloud-based distributed computing support. However, future development plans include seamless integration with AWS Batch. As a temporary workaround, it is possible to deploy a burstable SLURM cluster using AWS ParallelCluster and leverage targets’ existing support for traditional schedulers. Some alternative local future plans are listed here.↩︎ The resources of tar_target() defaults to tar_option_get(\"resources\"). You can set the default value for all targets using tar_option_set().↩︎ In older version of targets, resources was a named list. In targets version 0.5.0.9000 and above, please create the resources argument with helpers tar_resources() and tar_resources_future().↩︎ "],["drake.html", "Chapter 14 What about drake? 14.1 Why is drake superseded? 14.2 Transitioning to targets 14.3 Advantages of targets over drake", " Chapter 14 What about drake? targets is the successor of drake, an older pipeline tool. As of 2021-01-21, drake is superseded, which means there are no plans for new features or discretionary enhancements, but basic maintenance and support will continue indefinitely. Existing projects that use drake can safely continue to use drake, and there is no need to retrofit targets. New projects should use targets because it is friendlier and more robust. 14.1 Why is drake superseded? Nearly four years of community feedback have exposed major user-side limitations regarding data management, collaboration, dynamic branching, and parallel efficiency. Unfortunately, these limitations are permanent. Solutions in drake itself would make the package incompatible with existing projects that use it, and the internal architecture is too copious, elaborate, and mature for such extreme refactoring. That is why targets was created. The targets package borrows from past learnings, user suggestions, discussions, complaints, success stories, and feature requests, and it improves the user experience in ways that will never be possible in drake. 14.2 Transitioning to targets If you know drake, then you already almost know targets. The programming style is similar, and most functions in targets have counterparts in drake. Functions in drake Counterparts in targets use_drake(), drake_script() tar_script() drake_plan() tar_manifest(), tarchetypes::tar_plan() target() tar_target(), tar_target_raw() drake_config() tar_option_set() outdated(), r_outdated() tar_outdated() vis_drake_graph(), r_vis_drake_graph() tar_visnetwork(), tar_glimpse() drake_graph_info(), r_drake_graph_info() tar_network() make(), r_make() tar_make(), tar_make_clustermq(), tar_make_future() loadd() tar_load() readd() tar_read() diagnose(), build_times(), cached(), drake_cache_log() tar_meta() drake_progress(), drake_running(), drake_done(), drake_failed(), drake_cancelled() tar_progress() clean() tar_deduplicate(), tar_delete(), tar_destroy(), tar_invalidate() drake_gc() tar_prune() id_chr() tar_name(), tar_path() knitr_in() tarchetypes::tar_render() cancel(), cancel_if() tar_cancel() trigger() tar_cue() drake_example(), drake_example(), load_mtcars_example(), clean_mtcars_example() Unsupported. Example targets pipelines are in individual repositories linked from here. drake_build() Unsupported in targets to ensure coherence with dynamic branching. drake_debug() See the debugging chapter. drake_history(), recoverable() Unsupported in targets. Instead of trying to manage history and data recovery directly, targets maintains a much lighter/friendlier data store to make it easier to use external data versioning tools instead. missed(), tracked(), deps_code(), deps_target(), deps_knitr(), deps_profile() Unsupported in targets because dependency detection is easier to understand than in drake. drake_hpc_template_file(), drake_hpc_template_files() Deemed out of scope for targets. drake_cache(), new_cache(), find_cache(). Unsupported because targets is far more strict and paternalistic about data/file management. rescue_cache(), which_clean(), cache_planned(), cache_unplanned() Unsupported due to the simplified data management system and storage cleaning functions. drake_get_session_info() Deemed superfluous and a potential bottleneck. Discarded for targets. read_drake_seed() Superfluous because targets always uses the same global seed. tar_meta() shows all the target-level seeds. show_source() Deemed superfluous. Discarded in targets to conserve storage space in _targets/meta/meta. drake_tempfile() Superfluous in targets because there is no special disk.frame storage format. (Dynamic file targets are much better for managing disk.frames.) file_store() Superfluous in targets because all files are dynamic files and there is no longer a need to Base32-encode any file names. Likewise, many make() arguments have equivalent arguments elsewhere. Argument of drake::make() Counterparts in targets targets names in tar_make() etc. envir envir in tar_option_set() verbose reporter in tar_make() etc. parallelism Choice of function: tar_make() vs tar_make_clustermq() vs tar_make_future() jobs workers in tar_make_clustermq() and tar_make_future() packages packages in tar_target() and tar_option_set() lib_loc library in tar_target() and tar_option_set() trigger cue in tar_target() and tar_option_set() caching storage and retrieval in tar_target() and tar_option_set() keep_going error in tar_target() and tar_option_set() memory_strategy memory in tar_target() and tar_option_set() garbage_collection garbage_collection in tar_target() and tar_option_set() template resources in tar_target() and tar_option_set(), along with helpers like tar_resources(). curl_handles handle element of resources argument of tar_target() and tar_option_set() format format in tar_target() and tar_option_set() seed Superfluous because targets always uses the same global seed. [tar_meta()] shows all the target-level seeds. In addition, many optional columns of drake plans are expressed differently in targets. Optional column of drake plans Feature in targets format format argument of tar_target() and tar_option_set() dynamic pattern argument of tar_target() and tar_option_set() transform static branching functions in tarchetypes such as tar_map() and tar_combine() trigger cue argument of tar_target() and tar_option_set() hpc deployment argument of tar_target() and tar_option_set() resources resources argument of tar_target() and tar_option_set() caching storage and retrieval arguments of tar_target() and tar_option_set() 14.3 Advantages of targets over drake 14.3.1 Better guardrails by design drake leaves ample room for user-side mistakes, and some of these mistakes require extra awareness or advanced knowledge of R to consistently avoid. The example behaviors below are too systemic to solve and still preserve back-compatibility. By default, make() looks for functions and global objects in the parent environment of the calling R session. Because the global environment is often old and stale in practical situations, which causes targets to become incorrectly invalidated. Users need to remember to restart the session before calling make(). The issue is discussed here, and the discussion led to functions like r_make() which always create a fresh session to do the work. However, r_make() is not a complete replacement for make(), and beginner users still run into the original problems. Similar to the above, make() does not find the intended functions and global objects if it is called in a different environment. Edge cases like this one and this one continue to surprise users. drake is extremely flexible about the location of the .drake/ cache. When a user calls readd(), loadd(), make(), and similar functions, drake searches up through the parent directories until it finds a .drake/ folder. This flexibility seldom helps, and it creates uncertainty and inconsistency when it comes to initializing and accessing projects, especially if there are multiple projects with nested file systems. The targets package solves all these issues by design. Functions tar_make(), tar_make_clustermq(), and tar_make_future() all create fresh new R sessions by default. They all require a _targets.R configuration file in the project root (working directory of the tar_make() call) so that the functions, global objects, and settings are all populated in the exact same way each session, leading to less frustration, greater consistency, and greater reproducibility. In addition, the _targets/ data store always lives in the project root. 14.3.2 Enhanced debugging support targets has enhanced debugging support. With the workspaces argument to tar_option_set(), users can locally recreate the conditions under which a target runs. This includes packages, global functions and objects, and the random number generator seed. Similarly, tar_option_set(error = \"workspace\") automatically saves debugging workspaces for targets that encounter errors. The debug option lets users enter an interactive debugger for a given target while the pipeline is running. And unlike drake, all debugging features are fully compatible with dynamic branching. 14.3.3 Improved tracking of package functions By default, targets ignores changes to functions inside external packages. However, if a workflow centers on a custom package with methodology under development, users can make targets automatically watch the package’s functions for changes. Simply supply the names of the relevant packages to the imports argument of tar_option_set(). Unlike drake, targets can track multiple packages this way, and the internal mechanism is much safer. 14.3.4 Lighter, friendlier data management drake’s cache is an intricate file system in a hidden .drake folder. It contains multiple files for each target, and those names are not informative. (See the files in the data/ folder in the diagram below.) Users often have trouble understanding how drake manages data, resolving problems when files are corrupted, placing the data under version control, collaborating with others on the same pipeline, and clearing out superfluous data when the cache grows large in storage. .drake/ ├── config/ ├── data/ ├───── 17bfcef645301416.rds ├───── 21935c86f12692e2.rds ├───── 37caf5df2892cfc4.rds ├───── ... ├── drake/ ├───── history/ ├───── return/ ├───── tmp/ ├── keys/ # A surprisingly large number of tiny text files live here. ├───── memoize/ ├───── meta/ ├───── objects/ ├───── progress/ ├───── recover/ ├───── session/ └── scratch/ # This folder should be temporary, but it gets egregiously large. The targets takes a friendlier, more transparent, less mysterious approach to data management. Its data store is a visible _targets folder, and it contains far fewer files: a spreadsheet of metadata, a spreadsheet of target progress, and one informatively named data file for each target. It is much easier to understand the data management process, identify and diagnose problems, place projects under version control, and avoid consuming unnecessary storage resources. Sketch: _targets/ ├── meta/ ├───── meta ├───── process ├───── progress ├── objects/ ├───── target_name_1 ├───── target_name_2 ├───── target_name_3 ├───── ... ├── scratch/ # tar_make() deletes this folder after it finishes. └── user/ # gittargets users can put custom files here for data version control. 14.3.5 Cloud storage Thanks to the simplified data store and simplified internals, targets can automatically upload data to the Amazon S3 bucket of your choice. Simply configure aws.s3, create a bucket, and select one of the AWS-powered storage formats. Then, targets will automatically upload the return values to the cloud. # _targets.R tar_option_set(resources = list(bucket = &quot;my-bucket-name&quot;)) list( tar_target(dataset, get_large_dataset(), format = &quot;aws_fst_tbl&quot;), tar_target(analysis, analyze_dataset(dataset), format = &quot;aws_qs&quot;) ) Data retrieval is still super easy. tar_read(dataset) 14.3.6 Show status of functions and global objects drake has several utilities that inform users which targets are up to date and which need to rerun. However, those utilities are limited by how drake manages functions and other global objects. Whenever drake inspects globals, it stores their values in its cache and loses track of their previous state from the last run of the pipeline. As a result, it has trouble informing users exactly why a given target is out of date. And because the system for tracking global objects is tightly coupled with the cache, this limitation is permanent. In targets, the metadata management system only updates information on global objects when the pipeline actually runs. This makes it possible to understand which specific changes to your code could have invalided your targets. In large projects with long runtimes, this feature contributes significantly to reproducibility and peace of mind. 14.3.7 Dynamic branching with dplyr::group_by() Dynamic branching was an architecturally difficult fit in drake, and it can only support one single (vctrs-based) method of slicing and aggregation for processing sub-targets. This limitation has frustrated members of the community, as discussed here and here. targets, on the other hand, is more flexible regarding slicing and aggregation. When it branches over an object, it can iterate over vectors, lists, and even data frames grouped with dplyr::group_by(). To branch over chunks of a data frame, our data frame target needs to have a special tar_group column. We can create this column in our target’s return value with the tar_group() function. library(dplyr) library(targets) library(tibble) tibble( x = seq_len(6), id = rep(letters[seq_len(3)], each = 2) ) %&gt;% group_by(id) %&gt;% tar_group() #&gt; # A tibble: 6 × 3 #&gt; x id tar_group #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 1 a 1 #&gt; 2 2 a 1 #&gt; 3 3 b 2 #&gt; 4 4 b 2 #&gt; 5 5 c 3 #&gt; 6 6 c 3 Our actual target has the command above and iteration = \"group\". tar_target( data, tibble( x = seq_len(6), id = rep(letters[seq_len(3)], each = 2) ) %&gt;% group_by(id) %&gt;% tar_group(), iteration = &quot;group&quot; ) Now, any target that maps over data is going to define one branch for each group in the data frame. The following target creates three branches when run in a pipeline: one returning 3, one returning 7, and one returning 11. tar_target( sums, sum(data$x), pattern = map(data) ) 14.3.8 Composable dynamic branching Because the design of targets is fundamentally dynamic, users can create complicated dynamic branching patterns that are never going to be possible in drake. Below, target z creates six branches, one for each combination of w and tuple (x, y). The pattern cross(w, map(x, y)) is equivalent to tidyr::crossing(w, tidyr::nesting(x, y)). # _targets.R library(targets) list( tar_target(w, seq_len(2)), tar_target(x, head(letters, 3)), tar_target(y, head(LETTERS, 3)), tar_target( z, data.frame(w = w, x = x, y = y), pattern = cross(w, map(x, y)) ) ) Thanks to glep and djbirke on GitHub for the idea. 14.3.9 Improved parallel efficiency Dynamic branching in drake is staged. In other words, all the sub-targets of a dynamic target must complete before the pipeline moves on to downstream targets. The diagram below illustrates this behavior in a pipeline with a dynamic target B that maps over another dynamic target A. For thousands of dynamic sub-targets with highly variable runtimes, this behavior consumes unnecessary runtime and computing resources. And because drake’s architecture was designed at a fundamental level for static branching only, this limitation is permanent. By contrast, the internal data structures in targets are dynamic by design, which allows for a dynamic branching model with more flexibility and parallel efficiency. Branches can always start as soon as their upstream dependencies complete, even if some of those upstream dependencies are branches. This behavior reduces runtime and reduces consumption of computing resources. 14.3.10 Metaprogramming In drake, pipelines are defined with the drake_plan() function. drake_plan() supports an elaborate domain specific language that diffuses user-supplied R expressions. This makes it convenient to assign commands to targets in the vast majority of cases, but it also obstructs custom metaprogramming by users (example here). Granted, it is possible to completely circumvent drake_plan() and create the whole data frame from scratch, but this is hardly ideal and seldom done in practice. The targets package tries to make customization easier. Relative to drake, targets takes a decentralized approach to setting up pipelines, moving as much custom configuration as possible to the target level rather than the whole pipeline level. In addition, the tar_target_raw() function avoids non-standard evaluation while mirroring tar_target() in all other respects. All this makes it much easier to create custom metaprogrammed pipelines and target archetypes while avoiding an elaborate domain specific language for static branching, which was extremely difficult to understand and error prone in drake. The R Targetopia is an emerging ecosystem of workflow frameworks that take full advantage of this customization and democratize reproducible pipelines. "],["markdown.html", "Chapter 15 Literate programming", " Chapter 15 Literate programming This chapter has moved to https://books.ropensci.org/targets/literate-programming.html. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
