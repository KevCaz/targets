[
["index.html", "The targets R Package User Manual Chapter 1 Introduction 1.1 Motivation 1.2 Pipeline toolkits 1.3 The targets package 1.4 What about drake?", " The targets R Package User Manual Will Landau Copyright Eli Lilly and Company Chapter 1 Introduction The targets package is a Make-like pipeline toolkit for Statistics and data science in R. With targets, you can maintain a reproducible workflow without repeating yourself. targets learns how your pipeline fits together, skips costly runtime for tasks that are already up to date, runs only the necessary computation, supports implicit parallel computing, abstracts files as R objects, and shows tangible evidence that the results match the underlying code and data. The current chapter elaborates on the role and benefits of targets, and subsequent chapters walk through the major functionality. The documentation website, is a companion resource with installation instructions, links to example projects, and a reference page with all user-side functions. 1.1 Motivation Data analysis can be slow. A round of scientific computation can take several minutes, hours, or even days to complete. After it finishes, if you update your code or data, your hard-earned results may no longer be valid. Unchecked, this invalidation creates chronic Sisyphean loop: Launch the code. Wait while it runs. Discover an issue. Restart from scratch. 1.2 Pipeline toolkits Pipeline toolkits like GNU Make break the cycle. They watch the dependency graph of the whole workflow and skip steps, or “targets”, whose code, data, and upstream dependencies have not changed since the last run of the pipeline. When all targets are up to date, this is evidence that the results match the underlying code and data, which helps us trust the results and confirm the computation is reproducible. 1.3 The targets package Unlike most pipeline toolkits, which are language agnostic or Python-focused, the targets package allows data scientists and researchers to work entirely within R. targets implicitly nudges users toward a clean, function-oriented programming style that fits the intent of the R language and helps practitioners maintain their data analysis projects. 1.4 What about drake? The drake package is an older and more established R-focused pipeline toolkit. It is has become a key piece of the R ecosystem, and development will continue. However, years of community feedback have exposed major user-side limitations regarding data management, collaboration, parallel efficiency, and pipeline archetypes. Unfortunately, these limitations are permanent. Solutions in drake itself would make the package incompatible with existing projects that use it. That is why targets was created. The targets package borrows from past learnings and attempts to advance the user experience beyond drake’s potential capabilities. Please see the statement of need for technical details. If you know drake, then you already almost know targets. The programming style is similar, and most functions in targets have counterparts in drake. Functions in drake Similar functions in targets use_drake(), drake_script() tar_script() drake_plan() tar_pipeline(), tar_manifest() target() tar_target(), tar_target_raw() drake_config() tar_options() outdated() tar_outdated() vis_drake_graph(), r_vis_drake_graph() tar_visnetwork(), tar_glimpse() drake_graph_info(), r_drake_graph_info() tar_network() make(), r_make() tar_make(), tar_make_clustermq(), tar_make_future() loadd() tar_load() readd() tar_read() diagnose(), build_times(), cached() tar_meta() drake_progress(), drake_running(), drake_done(), drake_failed(), drake_cancelled() tar_progress() clean() tar_deduplicate(), tar_delete(), tar_destroy(), tar_invalidate(), tar_prune() id_chr() tar_name() knitr_in() tar_knitr() cancel(), cancel_if() tar_cancel() trigger() tar_cue() "],
["walkthrough.html", "Chapter 2 Walkthrough 2.1 Goal 2.2 Files 2.3 Inspect the pipeline 2.4 Run the pipeline 2.5 Changes 2.6 Read your data 2.7 Read metadata", " Chapter 2 Walkthrough This chapter walks through a minimal example of a targets-powered data analysis project. The source code is here, and other examples are linked from the documentation website. 2.1 Goal The this minimal workflow is to assess the relationship among ozone, wind, and temperature in base R’s airquality dataset. We get the data from a file, preprocess it, visualize it, fit a regression model, and generate an R Markdown report to communicate the results. 2.2 Files The file structure of the project looks like this. ├── _targets.R ├── R/ ├──── functions.R ├── data/ └──── raw_data.csv raw_data.csv contains the data we want to analyze. Ozone,Solar.R,Wind,Temp,Month,Day 36,118,8.0,72,5,2 12,149,12.6,74,5,3 ... functions.R contains our custom user-defined functions. (See the best practices chapter for a discussion of function-oriented workflows.) # functions.R create_plot &lt;- function(data) { ggplot(data) + geom_histogram(aes(x = Ozone)) + theme_gray(24) } Both raw_data.csv and functions.R are custom components. You as the user decide whether to use them, what to name them, and where to put them. However, the “target script” _targets.R file is special. (So special, in fact, that there is a special tar_script() function to help you create one). Every targets project requires a _targets.R script in the project’s root directory to declare and configure the pipeline. Ours looks like this. library(targets) source(&quot;R/functions.R&quot;) options(tidyverse.quiet = TRUE) tar_options(packages = c(&quot;biglm&quot;, &quot;rmarkdown&quot;, &quot;tidyverse&quot;)) tar_pipeline( tar_target( raw_data_file, &quot;data/raw_data.csv&quot;, format = &quot;file&quot; ), tar_target( raw_data, read_csv(raw_data_file, col_types = cols()) ), tar_target( data, raw_data %&gt;% mutate(Ozone = replace_na(Ozone, mean(Ozone, na.rm = TRUE))) ), tar_target(hist, create_plot(data)), tar_target(fit, biglm(Ozone ~ Wind + Temp, data)) ) All _targets.R scripts have these requirements. Load the targets package itself. (_targets.R scripts created with tar_script() automatically insert a library(targets) line at the top by default.) Load your custom functions and global objects into the R session. In our case, our only such object is the create_plot() function, and we load it into the session by calling source(\"R/functions.R\"). Call tar_options() to set the default settings for all you targets, such as the names of required packages and the data storage format. Individual targets can override these settings. Define individual targets with the tar_target() function. Each target is an intermediate step of the workflow. At minimum, a target must have a name and an R expression. This expression runs when the pipeline builds the target, and the return value is saved as a file in the _targets/objects/ folder. The only targets not stored in _/targets/objects/ are dynamic files such as raw_data_file. Here, format = \"file\" makes raw_data_file a dynamic file. That means targets watches the data at the file paths returned from the expression (in this case, \"data/raw_data.csv\"). Gather the targets in a pipeline with the tar_pipeline() function. tar_pipeline() is flexible: it can accept individual tar_target() objects or nested lists of such objects. Every _targets.R script must end with a tar_pipeline() object, which usually means there is an explicit call to tar_pipeline() at the very bottom. 2.3 Inspect the pipeline Before you run the pipeline, it is good practice to inspect it for errors. tar_manifest() shows you a data frame information about the targets, and it has functionality to specify the targets and columns returned. tar_manifest(fields = &quot;command&quot;) #&gt; # A tibble: 5 x 2 #&gt; name command #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 raw_data &quot;expression(read_csv(raw_data_file, col_types = cols()))&quot; #&gt; 2 fit &quot;expression(biglm(Ozone ~ Wind + Temp, data))&quot; #&gt; 3 hist &quot;expression(create_plot(data))&quot; #&gt; 4 raw_data_fi… &quot;expression(\\&quot;data/raw_data.csv\\&quot;)&quot; #&gt; 5 data &quot;expression(raw_data %&gt;% mutate(Ozone = replace_na(Ozone, mean(O… There are also graphical displays with tar_glimpse() tar_glimpse() and tar_visnetwork(). tar_visnetwork() Both graphing functions visualize the underlying directed acyclic graph (DAG) and tell you how targets are connected. This DAG is indifferent to the order you write targets in tar_pipeline(). You will still get the same graph even if you rearrange them. This is because targets uses static code analysis with codetools::findGlobals() to detect dependency relationships. For example, target raw_data depends on target raw_data_file because the R expression for raw_data mentions the symbol raw_data_file. This process can be a bit hard to understand sometimes, so you should always look at the graph to verify that targets understands the flow of your pipeline. 2.4 Run the pipeline tar_make() runs the workflow. It creates a fresh clean external R process, reads _targets.R to learn about the pipeline, runs the correct targets in the correct order given by the graph, and saves the necessary data to the _targets/ data store. tar_make() #&gt; ● run target raw_data_file #&gt; ● run target raw_data #&gt; ● run target data #&gt; ● run target fit #&gt; ● run target hist The next time you run tar_make(), targets skips everything that is already up to date, which saves a lot of time in large projects with long runtimes. tar_make() #&gt; ✔ skip target raw_data_file #&gt; ✔ skip target raw_data #&gt; ✔ skip target data #&gt; ✔ skip target fit #&gt; ✔ skip target hist #&gt; ✔ Already up to date. You can use tar_visnetwork() and tar_outdated() to check ahead of time which targets are up to date. tar_visnetwork() tar_outdated() #&gt; character(0) 2.5 Changes The targets package notices when you make changes to your workflow, and tar_make() only runs the targets that need to build. There are custom rules called “cues” that targets uses to decide whether a target needs to rerun. For the full details on cues, read the “Details” section of the tar_cue() help file. (Enter ?targets::tar_cue into your R console.) 2.5.1 Change code If you change one of your functions, the targets that depend on it will no longer be up to date, and tar_make() will rebuild them. For example, let’s set the number of bins in our histogram. # Edit functions.R. create_plot &lt;- function(data) { ggplot(data) + geom_histogram(aes(x = Ozone), bins = 10) + # Set number of bins. theme_gray(24) } targets detects the change. hist is outdated (as would be any targets downstream of hist) and the others are still up to date. tar_visnetwork() tar_outdated() #&gt; [1] &quot;hist&quot; That means tar_make() reruns hist and nothing else. tar_make() #&gt; ✔ skip target raw_data_file #&gt; ✔ skip target raw_data #&gt; ✔ skip target data #&gt; ✔ skip target fit #&gt; ● run target hist We would see similar behavior if we changed the R expressions in any tar_target() calls in _targets.R. 2.5.2 Change data If we change the data file raw_data.csv, targets notices the change. This is because raw_data_file is a dynamic file (i.e. tar_target(format = \"file\")) that returned \"raw_data.csv\". Let’s try it out. Below, let’s use only the first 100 rows of the airquality dataset. write_csv(head(airquality, n = 100), &quot;data/raw_data.csv&quot;) Sure enough, raw_data_file and everything downstream is out of date, so all our targets are outdated. tar_visnetwork() tar_outdated() #&gt; [1] &quot;raw_data&quot; &quot;fit&quot; &quot;hist&quot; &quot;raw_data_file&quot; #&gt; [5] &quot;data&quot; tar_make() #&gt; ● run target raw_data_file #&gt; ● run target raw_data #&gt; ● run target data #&gt; ● run target fit #&gt; ● run target hist 2.6 Read your data targets has a convenient functions tar_read() to read your data from the _targets/ data store. tar_read(hist) There is also a tar_load() function, which supports tidyselect verbs like starts_with() tar_load(starts_with(&quot;fit&quot;)) library(biglm) #&gt; Loading required package: DBI fit #&gt; Large data regression model: biglm(Ozone ~ Wind + Temp, data) #&gt; Sample size = 100 The purpose of tar_read() and tar_load() is to make exploratory data analysis easy and convenient. Use these functions to verify the correctness of the output from the pipeline and come up with ideas for new targets if needed. 2.7 Read metadata To read the build progress of your targets while tar_make() is running, you can open a new R session and run tar_progress(). It reads the spreadsheet in _targets/meta/progress and tells you which targets are running, built, errored, or cancelled. tar_progress() #&gt; # A tibble: 5 x 2 #&gt; name progress #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 raw_data_file built #&gt; 2 raw_data built #&gt; 3 data built #&gt; 4 fit built #&gt; 5 hist built Likewise, the tar_meta() function reads _targets/meta/meta and tells you high-level information about the target’s settings, data, and results. The warnings, error, and traceback columns give you diagnostic information about targets with problems. tar_meta() #&gt; # A tibble: 6 x 16 #&gt; name type data command depend seed path bytes time format iteration #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;lis&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 crea… func… 658f… &lt;NA&gt; &lt;NA&gt; NA &lt;chr… NA NA &lt;NA&gt; &lt;NA&gt; #&gt; 2 raw_… stem 2b16… b6df0c… ef46d… 2.11e9 &lt;chr… 1884 1.59e9 file vector #&gt; 3 raw_… stem b155… 000ed0… 6ef08… -9.80e8 &lt;chr… 1156 1.59e9 rds vector #&gt; 4 data stem 7596… df3101… 3380d… 1.59e9 &lt;chr… 1155 1.59e9 rds vector #&gt; 5 fit stem 7422… aa0df6… 63f82… 1.78e9 &lt;chr… 578 1.59e9 rds vector #&gt; 6 hist stem c81a… 688771… 7fffc… -1.03e9 &lt;chr… 44321 1.59e9 rds vector #&gt; # … with 5 more variables: parent &lt;lgl&gt;, children &lt;list&gt;, seconds &lt;dbl&gt;, #&gt; # warnings &lt;lgl&gt;, error &lt;lgl&gt; The _targets/meta/meta spreadsheet file is critically important. Although targets can still work properly if files are missing from _targets/objects, the pipeline will error out if _targets/meta/meta is corrupted. If tar_meta() works, the project should be fine. "],
["practice.html", "Chapter 3 Best practices for targets-powered projects 3.1 A move away from imperative scripts 3.2 Functions 3.3 Targets 3.4 Debugging 3.5 Performance", " Chapter 3 Best practices for targets-powered projects The targets package espouses a clean, organized, modular, function-oriented style of programming. Many data scientists and researchers initially find this style uncomfortable, not only because it can be unfamiliar, but also because may requires us to postpone our thinking about actual results and conclusions while we focus on the process of creating a new workflow. However, this up-front cost pays off. Data science projects become easier to extend, maintain, and share with collabotors, and they have longer shelf lives than otherwise. In other words, we increase reproducibility while decreasing technical debt. Once these patterns become habits, the pace of development quickens while still lending more trust and credibility to the results. 3.1 A move away from imperative scripts Traditional data analysis projects are usually coded as collections of imperative scripts, often with numeric prefixes. 01-data.R 02-preprocess.R 03-analysis.R 04-summaries.R This approach does not scale well in large, cumbersome projects. Code gets tangled, messy, and difficult to test. Results fail to keep pace with rapid development, and reproducibility suffers. 3.2 Functions Functions are the most idiomatic way to express the modularity we need. They are custom shorthand that makes code easier to read and easier to test. A good pure function has An informative name that describes what the function does. Input arguments that are easy to generate. A return value that is convenient to introspect and meaningful to the project. is the shorthand, and (2) and (3) make testing easier. In data science workflows, we typically write individual functions to Retrieve or generate a dataset. Preprocess a dataset. Fit a model to a preprocessed dataset. Generate machine-readable summaries of the model fit (tables of summary statistics). Generate human-readable summaries of the model fit (plots and reports). Each of the functions in (1)-(5) typically calls custom inner functions to increase modularity even further. For examples, see the functions.R files in the example projects listed here. 3.3 Targets Targets are imperative high-level steps of the workflow that run the work you define in your functions. Like functions, targets generally focus on datasets, analyses, and summaries. The targets package automatically skips targets that are already up to date, so you should strive to define targets that maximize time savings. Good targets usually Are large enough to subtract a decent amount of runtime when skipped. Are small enough that some targets can be skipped even if others need to run. Invoke no side effects such as modifications to the global environment. (But targets with tar_target(format = \"file\") can save files.) Return a single value that is Easy to understand and introspect. Meaningful to the project. Easy to save as a file, e.g. with readRDS(). Regarding the last point above, it is possible to customize the storage format of the target. For details, enter ?tar_target in the console and scroll down to the description of the format argument. 3.4 Debugging If one of your targets errors out because of a user-side mistake, look up the error message in tar_meta(). If that does not help, run the target in interactive debugging mode: In _targets.R, write a call to tar_options() with debug equal to the target name. It may also be convenient to set cue equal to tar_cue(mode = \"never\") so tar_make() reaches the target you want to debug more quickly. Launch a fresh clean new interactive R session with the _targets.R script in your working directory. Run targets::tar_make() (or targets::tar_make_clustermq(), or targets::tar_make_future()) with callr_function = FALSE. When targets reaches the target you selected to debug, your R session will start an interactive debugger, and you should see Browse[1]&gt; in your console. Run targets::tar_name() to verify that you are debugging the correct target. Interactively run any R code that helps you troubleshoot the problem.1 To try it out yourself, write the following _targets.R file and then run tar_make(callr_function = NULL). When the debugger launches, run print(tar_name()) and print(a) to be sure things are set up properly. library(targets) tar_options(debug = &quot;b&quot;) tar_pipeline(tar_target(a, &quot;a&quot;), tar_target(b, a)) For more on debugging R code, visit this page. 3.5 Performance If your pipeline has several thousand targets, functions like tar_make(), tar_outdated(), and tar_vis_drake_graph() may take longer to run. There is an inevitable per-target runtime cost because package needs to check the code and data of each target individually. If this overhead becomes too much, consider batching your work into a smaller group of heavier targets. Using your custom functions, you can make each target perform multiple iterations of a task that was previously given to targets one at a time. Alternatively, if you see slowness in your project, you can contribute to the package with a profiling study. These contributions are great because they help improve the package. Here are the recommended steps. Install the proffer R package and its dependencies. Run proffer::pprof(tar_make(callr_function = NULL)) on your project. When a web browser pops up with pprof, select the flame graph and screenshot it. Post the flame graph, along with any code and data you can share, to the targets package issue tracker. The maintainer will have a look and try to make the package faster for your use case if speedups are possible. Because of the way targets manages environments, ls() will not report all your global objects, but you should still be able to access them.↩︎ "],
["branching.html", "Chapter 4 Branching: patterns and metaprogramming 4.1 Patterns 4.2 Metaprogramming", " Chapter 4 Branching: patterns and metaprogramming In targets, it is possible to write shorthand for large collections of targets. 4.1 Patterns The pattern argument of tar_target() allows you to dynamically branch over subsets of upstream targets. The following minimal example explores the mechanics of patterns and branching (and examples of branching in real-world projects are linked from here). library(targets) library(tidyverse) tar_script({ options(crayon.enabled = FALSE, tidyverse.quiet = TRUE) tar_pipeline( tar_target(w, c(1, 2)), tar_target(x, c(10, 20)), tar_target(y, w + x, pattern = map(w, x)), tar_target(z, sum(y)), tar_target(z2, length(y), pattern = map(y)) ) }) tar_visnetwork() tar_make() #&gt; ● run target w #&gt; ● run target x #&gt; ● run branch y_61ced14d #&gt; ● run branch y_4096b6a8 #&gt; ● run branch z2_275d234a #&gt; ● run branch z2_8c730b35 #&gt; ● run target z Above, targets w, x, and z are called stems because they provide values for other targets to branch over. Target y is a pattern becuase it defines multiple sub-targets, or branches, based on the return values of the targets named inside map() or cross(). If we read target y into memory, all the branches will load and get aggregated according to the iteration argument of tar_target(). tar_read(y) #&gt; [1] 11 22 Target z accepts this entire aggregate of y and sums it. tar_read(z) #&gt; [1] 33 Target z2 maps over y, so each each branch of z2 accepts a branch of y. tar_read(z2) #&gt; [1] 1 1 4.1.1 Pattern types As we see above, the map() pattern creates one branch for each tuple of slices of the arguments. For example, map(x, y) creates one branch corresponding to x[1] and y[1] and another branch corresponding to x[2] and y[2]. The cross() pattern instead creates a target for combination of arguments. Here, cross(x, y) creates 4 targets: One for x[1] and y[1], one for x[1] and y[2], one for x[2] and y[1], and one for x[2] and y[2]. Since we already ran the equivalent map() pattern, the following run only builds the (x[1], y[2]) and (x[2], y[1]) branches y. tar_script({ options(crayon.enabled = FALSE, tidyverse.quiet = TRUE) tar_pipeline( tar_target(w, c(1, 2)), tar_target(x, c(10, 20)), tar_target(y, w + x, pattern = cross(w, x)) ) }) tar_make() #&gt; ✔ skip target w #&gt; ✔ skip target x #&gt; ✔ skip branch y_61ced14d #&gt; ● run branch y_4b87fe9a #&gt; ● run branch y_6f1bdd19 #&gt; ✔ skip branch y_4096b6a8 tar_read(y) #&gt; [1] 11 21 12 22 4.1.2 Iteration There are many ways to slice up a stem for branching, and there are many ways to aggregate the branches of a pattern.2 The iteration argument of tar_target() controls the splitting and aggregation protocol on a target-by-target basis, and you can set the default for all targets with the analogous argument of tar_options(). 4.1.2.1 Vector targets uses vector iteration by default, and you can opt into this behavior by setting iteration = \"vector\" in tar_target(). In vector iteration, targets uses the vctrs package to split stems and aggregate banches. That means vctrs::vec_slice() slices up stems like x for mapping, and vctrs::vec_c() aggregates patterns like y for operations like tar_read(). For atomic vectors like in the example above, this behavior is already intuitive. But if we map over a data frame, each branch will get a row of the data frame due to vector iteration. tar_script({ options(crayon.enabled = FALSE, tidyverse.quiet = TRUE) print_and_return &lt;- function(x) { print(x) x } tar_pipeline( tar_target(x, data.frame(a = c(1, 2), b = c(&quot;a&quot;, &quot;b&quot;))), tar_target(y, print_and_return(x), pattern = map(x)) ) }) tar_make() #&gt; ● run target x #&gt; ● run branch y_2e131731 #&gt; a b #&gt; 1 1 a #&gt; ● run branch y_6c3b7977 #&gt; a b #&gt; 1 2 b And since y also has iteration = \"vector\", the aggregate of y is a single data frame of all the rows. tar_read(y) #&gt; a b #&gt; 1 1 a #&gt; 2 2 b 4.1.2.2 List List iteration splits and aggregates targets as simple lists. If target x has \"list\" iteration, all branches of downstream patterns will get x[[1]], x[[2]], and so on. (vctrs::vec_slice() behaves more like [] than [[]].) tar_script({ options(crayon.enabled = FALSE, tidyverse.quiet = TRUE) print_and_return &lt;- function(x) { print(x) x } tar_pipeline( tar_target( x, data.frame(a = c(1, 2), b = c(&quot;a&quot;, &quot;b&quot;)), iteration = &quot;list&quot; ), tar_target(y, print_and_return(x), pattern = map(x)), tar_target(z, x, pattern = map(x), iteration = &quot;list&quot;) ) }) tar_make() #&gt; ● run target x #&gt; ● run branch y_651b13c7 #&gt; [1] 1 2 #&gt; ● run branch y_1c8c7726 #&gt; [1] &quot;a&quot; &quot;b&quot; #&gt; ● run branch z_651b13c7 #&gt; ● run branch z_1c8c7726 Aggregation also happens differently. In this case, the vector iteration in y is not ideal, and the list iteration in z gives us more sensible output. tar_read(y) #&gt; Error: Can&#39;t combine `..1` &lt;double&gt; and `..2` &lt;character&gt;. tar_read(z) #&gt; [[1]] #&gt; [1] 1 2 #&gt; #&gt; [[2]] #&gt; [1] &quot;a&quot; &quot;b&quot; 4.1.2.3 Group Group iteration brings dplyr::group_by() functionality to patterns. This way, we can map or cross over custom subsets of rows. Consider the following data frame. object &lt;- data.frame( x = seq_len(6), id = rep(letters[seq_len(3)], each = 2) ) object #&gt; x id #&gt; 1 1 a #&gt; 2 2 a #&gt; 3 3 b #&gt; 4 4 b #&gt; 5 5 c #&gt; 6 6 c To map over the groups of rows defined by the id column, we Use group_by() and tar_group() to define the groups of rows, and Use iteration = \"group\" in tar_target() to tell downstream patterns to use the row groups. Put together, the pipeline looks like this. tar_script({ library(dplyr) print_and_return &lt;- function(x) { print(x) x } tar_pipeline( tar_target( data, data.frame( x = seq_len(6), id = rep(letters[seq_len(3)], each = 2) ) %&gt;% group_by(id) %&gt;% tar_group(), iteration = &quot;group&quot; ), tar_target( sums, sum(print_and_return(data)$x), pattern = map(data), iteration = &quot;vector&quot; ) ) }) tar_make() #&gt; #&gt; Attaching package: ‘dplyr’ #&gt; #&gt; The following objects are masked from ‘package:stats’: #&gt; #&gt; filter, lag #&gt; #&gt; The following objects are masked from ‘package:base’: #&gt; #&gt; intersect, setdiff, setequal, union #&gt; #&gt; ● run target data #&gt; ● run branch sums_9c27c3ec #&gt; # A tibble: 2 x 3 #&gt; # Groups: id [1] #&gt; x id tar_group #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 1 a 1 #&gt; 2 2 a 1 #&gt; ● run branch sums_8c545d52 #&gt; # A tibble: 2 x 3 #&gt; # Groups: id [1] #&gt; x id tar_group #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 3 b 2 #&gt; 2 4 b 2 #&gt; ● run branch sums_2babc46e #&gt; # A tibble: 2 x 3 #&gt; # Groups: id [1] #&gt; x id tar_group #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 5 c 3 #&gt; 2 6 c 3 tar_read(sums) #&gt; [1] 3 7 11 Row groups are defined in the special tar_group column created by tar_group(). tar_group() creates this column based on the attributes created by dplyr::group_by(). targets avoids using those attributes directly because some custom storage formats, particularly format = \"fst\" in tar_target(), drop all attributes. data.frame( x = seq_len(6), id = rep(letters[seq_len(3)], each = 2) ) %&gt;% dplyr::group_by(id) %&gt;% tar_group() #&gt; # A tibble: 6 x 3 #&gt; # Groups: id [3] #&gt; x id tar_group #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 1 a 1 #&gt; 2 2 a 1 #&gt; 3 3 b 2 #&gt; 4 4 b 2 #&gt; 5 5 c 3 #&gt; 6 6 c 3 4.2 Metaprogramming Metaprogramming is an alternative to patterns for large collections of targets. Metaprogramming can help you understand of the targets you start with, create informative target names, and construct complicated pipelines when layered on top of patterns. In the example below, we define a target for each data source in a mock workflow. We use tar_target_raw() instead of tar_target() to avoid the non-standard evaluation, which lets us supply target names, expressions, and pattern specifications programmatically. tar_script({ library(purrr) datasets &lt;- c(&quot;gapminder&quot;, &quot;who&quot;, &quot;imf&quot;) data_target_list &lt;- map( datasets, ~tar_target_raw(.x, substitute(get_data(x), env = list(x = .x))) ) tar_pipeline(data_target_list) }) tar_manifest() is especially important for checking the correctness of your metaprogramming. tar_manifest(fields = &quot;command&quot;) #&gt; # A tibble: 3 x 2 #&gt; name command #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 imf &quot;expression(get_data(\\&quot;imf\\&quot;))&quot; #&gt; 2 gapminder &quot;expression(get_data(\\&quot;gapminder\\&quot;))&quot; #&gt; 3 who &quot;expression(get_data(\\&quot;who\\&quot;))&quot; We can define symbols with rlang::sym() and insert them into expressions with substitute(). It is straightforward to define entire chains of targets this way. The example below demonstrates how to work with symbols and use metaprogramming and patterns simultaneously. tar_script({ library(purrr) library(rlang) target_a &lt;- tar_target(a, seq_len(2)) target_list &lt;- map( seq(2, 4), ~tar_target_raw( letters[.x], substitute(identity(y), env = list(y = sym(letters[.x - 1]))), pattern = substitute(map(y), env = list(y = sym(letters[.x - 1]))) ) ) tar_pipeline(target_a, target_list) }) library(dplyr) tar_manifest() %&gt;% mutate(dimensions = as.character(dimensions)) #&gt; #&gt; Attaching package: ‘rlang’ #&gt; #&gt; The following objects are masked from ‘package:purrr’: #&gt; #&gt; %@%, as_function, flatten, flatten_chr, flatten_dbl, flatten_int, #&gt; flatten_lgl, flatten_raw, invoke, list_along, modify, prepend, #&gt; splice #&gt; # A tibble: 4 x 4 #&gt; name command type dimensions #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 a expression(seq_len(2)) stem character(0) #&gt; 2 b expression(identity(a)) map a #&gt; 3 c expression(identity(b)) map b #&gt; 4 d expression(identity(c)) map c tar_glimpse() #&gt; #&gt; Attaching package: ‘rlang’ #&gt; #&gt; The following objects are masked from ‘package:purrr’: #&gt; #&gt; %@%, as_function, flatten, flatten_chr, flatten_dbl, flatten_int, #&gt; flatten_lgl, flatten_raw, invoke, list_along, modify, prepend, #&gt; splice Slicing is always the same when we branch over an existing pattern. If we have tar_target(y, x, pattern = map(x)) and x is another pattern, then each branch of y always gets a branch of x regardless of the iteration method. Likewise, the aggregation of stems does not depend on the iteration method because every stem is already aggregated.↩︎ "],
["files.html", "Chapter 5 External files and literate programming 5.1 Internal files 5.2 External input files 5.3 External output files 5.4 Literate programming", " Chapter 5 External files and literate programming The targets package automatically stores data and automatically responds to changed files to keep your targets up to date. The chapter below explains how to leverage this reproducibility for external datasets, external output files, and literate programming artifacts such as R Markdown reports. Real-world applications of these techniques are linked from here. 5.1 Internal files Each project’s data lives in the _targets/ folder in the root directory (where you call tar_make()). The files in the _targets/ look like this: _targets/ ├── meta/ ├────── progress ├────── meta ├── objects/ ├────── target1 ├────── target2 ├────── branching_target_c7bcb4bd ├────── branching_target_285fb6a9 ├────── branching_target_874ca381 └── scratch/ # tar_make() deletes this folder after it finishes. Spreadsheets _targets/meta/meta and _targets/meta_progress keep track of target metadata, and the scratch/ directory contains temporary files which can be safely deleted after tar_make() finishes. The _targets/objects/ folder contains the return values of the targets themselves. A typical target returns an R object: for example, a dataset with tar_target(dataset, data.frame(x = rnorm(1000)), format = \"fst\") or a fitted model tar_target(model, biglm(ozone ~ temp + wind), format = \"qs\"). When you run the pipeline, targets computes this object and saves it as a file in _targets/objects/. The file name in _targets/objects/ is always the target name, and type of the file is determined by the format argument of tar_target(), and formats \"fst\" and \"qs\" are two of many choices explained in the help file of tar_target(). No matter what format you pick, targets watches the file for changes and recomputes the target in tar_make() if the the file gets corrupted (unless you suppress the file cue with tar_target(cue = tar_cue(file = FALSE))). 5.2 External input files To reproducibly track an external input file, you need to define a new target and choose format = \"file\" in tar_target(). Targets with the \"file\" format are called dynamic files, and these targets are unusual because their return values do not get saved in the _targets/objects/ folder. Rather, they expect you to return a character vector files and directories, which gets stored in the _targets/meta spreadsheet, and then targets watches the data at those paths for changes. The first two targets of the minimal example demonstrate this technique. tar_script({ path_to_data &lt;- function() { &quot;data/raw_data.csv&quot; } tar_pipeline( tar_target( raw_data_file, path_to_data(), format = &quot;file&quot; ), tar_target( raw_data, read_csv(raw_data_file, col_types = cols()) ) ) }) Above, raw_data_file is the dynamic file target. The file data/raw_data.csv exists before we ever run the pipeline, and the R expresion for the target returns the character vector \"data/raw_data.csv\". (We use the path_to_data() function to demonstrate that you need not literally write \"data/raw_data.csv\" as long as the path is returned somehow.) All subsequent targets that depend on the file must reference the file using the symbol raw_data_file. This allows targets’ automatic static code analysis routines to detect which targets depend on the file. Because the raw_data target literally mentions the symbol raw_data_file, targets knows raw_data depends on raw_data_file. This ensures that raw_data_file gets processed before raw_data, and tar_make() automatically reruns raw_data if raw_data_file or \"data/raw_data.csv\" change. tar_visnetwork() If we were to omit the symbol raw_data_file from the R expression of raw_data, those targets would be disconnected in the graph and tar_make() would make incorrect decisions. tar_script({ path_to_data &lt;- function() { &quot;data/raw_data.csv&quot; } tar_pipeline( tar_target( raw_data_file, path_to_data(), format = &quot;file&quot; ), tar_target( raw_data, read_csv(&quot;data/raw_data.csv&quot;, col_types = cols()) # incorrect ) ) }) tar_visnetwork() 5.3 External output files We can generate and track custom external files too, and the mechanics are similar. We still return a file path and use format = \"file\", but this time, our R command writes a file before it returns a path. For an external plot file, our target might look like this. tar_target( plot_file, save_plot_and_return_path(), format = &quot;file&quot; ) where our custom save_plot_and_return_path() function does exactly what the name describes. save_plot_and_return_path &lt;- function() { plot &lt;- ggplot(mtcars) + geom_point(aes(x = wt, y = mpg)) ggsave(&quot;plot_file.png&quot;, plot, width = 7, height = 7) return(&quot;plot_file.png&quot;) } 5.4 Literate programming R Markdown reports are a little more challenging because they often depend on upstream targets. To integrate an R Markdown report with a targets pipeline, you must use tar_read() and tar_load() in active code chunks to explicitly name the targets that the report depends on. The report from the minimal example looks like this. Above, the report depends on targets fit and hist. The use of tar_read() and tar_load() allows us to run the report outside the pipeline. As long as _targets/ folder has data on the required targets from a previous tar_make(), you can open the RStudio IDE, edit the report, and click the Knit button like you would for any other R Markdown report. To connect the target with the pipeline, we define a special kind of target with 4 requirements: format = \"file\" in tar_target(). tidy_eval = TRUE in tar_target() (default). An R expression that returns the paths to the report’s source file and the rendered output file. Explicit mentions of the symbols fit and hist in the command. The target definition looks like this. tar_target( report, { render(&quot;report.Rmd&quot;, quiet = TRUE) c(!!tar_knitr(&quot;report.Rmd&quot;), &quot;report.html&quot;) }, format = &quot;file&quot; ) #&gt; &lt;stem target&gt; #&gt; name: report #&gt; command: #&gt; { #&gt; render(&quot;report.Rmd&quot;, quiet = TRUE) #&gt; c(list(deps = list(fit, hist), path = &quot;report.Rmd&quot;)[[&quot;path&quot;]], #&gt; &quot;report.html&quot;) #&gt; } #&gt; format: file #&gt; iteration method: vector #&gt; error mode: stop #&gt; memory mode: persistent #&gt; storage mode: local #&gt; retrieval mode: local #&gt; deploy to: remote #&gt; template (clustermq): #&gt; list() #&gt; resources (future): #&gt; list() #&gt; cue: #&gt; depend: TRUE #&gt; file: TRUE #&gt; iteration: TRUE #&gt; command: TRUE #&gt; mode: thorough #&gt; format: TRUE #&gt; packages: #&gt; biglm #&gt; DBI #&gt; withr #&gt; forcats #&gt; stringr #&gt; dplyr #&gt; purrr #&gt; readr #&gt; tidyr #&gt; tibble #&gt; ggplot2 #&gt; tidyverse #&gt; targets #&gt; fs #&gt; stats #&gt; graphics #&gt; grDevices #&gt; utils #&gt; datasets #&gt; methods #&gt; base #&gt; library: #&gt; NULL The tar_knitr() function searches reports for mentions of tar_read() and tar_load() and returns code that mentions the target names as symbols. The tidy evaluation unquote operator !! tells targets to evaluate tar_knitr() right now instead of in tar_make(). From the print output above, you can see that the R expression of the target returns the correct file paths and mentions the symbols fit and hist. As a result, targets knows the report depends on fit and hist, and it knows that report.Rmd and report.html are the files associated with this dynamic file target. tar_visnetwork() "],
["hpc.html", "Chapter 6 High-performance computing 6.1 Clustermq 6.2 Future 6.3 Advanced", " Chapter 6 High-performance computing targets supports high-performance computing with the tar_make_clustermq() and tar_make_future() functions. These functions are like tar_make(), but they allow multiple targets to run simultaneously over parallel workers. These workers can be processes on your local machine, or they can be jobs on a cluster. The master process automatically sends a target to a worker as soon as The worker is available, and All the target’s upstream dependency targets have been checked or built. Practical real-world examples of high-performance computing in targets can be found at the examples linked from here. But for the purposes of explaining the mechanics of the package, consider the following sketch of a pipeline. tar_visnetwork() When we run this pipeline with high-performance computing, targets automatically knows to wait for data to finish running before moving on to the other targets. Once data is finished, it moves on to targets fast_fit and slow_fit. If fast_fit finishes before slow_fit, target plot_1 begins even as slow_fit is still running. Unlike drake, targets applies this behavior not only to stem targets, but also to branches of patterns. The following sections cover the mechanics and configuration details of high-performance computing in targets. 6.1 Clustermq tar_make_clustermq() uses persistent workers. That means all the parallel processes launch together as soon as there is a target to build, and all the processes keep running until the pipeline winds down. The video clip below visualizes the concept. 6.1.1 Clustermq installation Persistent workers require the clustermq R package, which in turn requires ZeroMQ. Please refer to the clustermq installation guide for specific instructions. 6.1.2 Clustermq locally When you write your _targets.R script, be sure to set the clustermq.scheduler global option to a a local scheduler like \"multicore\". Many of the supported schedulers and their configuration details are listed here. # _targets.R options(clustermq.scheduler = &quot;multicore&quot;) tar_pipeline( tar_target(data, get_data()), tar_target(fast_fit, fit_small_model(data)), tar_target(slow_fit, fit_slow_model(data)), tar_target(plot_1, make_plot(fast_fit)), tar_target(plot_2, make_plot(slow_fit)) ) Then, run tar_make_clustermq() with the appropriate number of workers. tar_make_clustermq(workers = 2) 6.1.3 Clustermq remotely For parallel computing on a cluster, Choose a scheduler listed here that corresponds to your cluster’s resource manager. Create a template file that configures the computing requirements and other settings for the cluster. Supply the scheduler option and template file to the clustermq.scheduler and clustermq.template global options in your _targets.R file. # _targets.R options(clustermq.scheduler = &quot;sge&quot;, clustermq.template = &quot;sge.tmpl&quot;) tar_pipeline( tar_target(data, get_data()), tar_target(fast_fit, fit_small_model(data)), tar_target(slow_fit, fit_slow_model(data)), tar_target(plot_1, make_plot(fast_fit)), tar_target(plot_2, make_plot(slow_fit)) ) Above, sge_tmpl refers to a template file like the one below. ## From https://github.com/mschubert/clustermq/wiki/SGE #$ -N {{ job_name }} # Worker name. #$ -t 1-{{ n_jobs }} # Submit workers as an array. #$ -j y # Combine stdout and stderr into one worker log file. #$ -o /dev/null # Worker log files. #$ -cwd # Use project root as working directory. #$ -V # Use environment variables. module load R/3.6.3 # Needed if R is an environment module on the cluster. CMQ_AUTH={{ auth }} R --no-save --no-restore -e &#39;clustermq:::worker(&quot;{{ master }}&quot;)&#39; # Leave alone. Then, run tar_make_clustermq() as before. tar_make_clustermq(workers = 2) See the examples linked from here to see how this setup works in real-world projects. 6.1.4 Clustermq configuration In addition to configuration options hard-coded in the template file, you can supply computing resources to individual targets with the template argument of tar_options(). For example, if your template has a wildcard num_cores for the number of cores per worker, #$ -pe smp {{ num_cores }} # Number of cores per worker #$ -N {{ job_name | 1 }} #$ -t 1-{{ n_jobs }} #$ -j y #$ -o /dev/null #$ -cwd #$ -V module load R/3.6.3 CMQ_AUTH={{ auth }} R --no-save --no-restore -e &#39;clustermq:::worker(&quot;{{ master }}&quot;)&#39; then you can call tar_make_clustermq() like this: tar_make_clustermq(workers = 2, template = list(num_cores = 2)) This particular use case comes up when you have custom parallel computing within targets and need to take advantage of multiple cores. 6.2 Future tar_make_future() runs transient workers. That means each target gets its own worker which initializes when the target begins and terminates when the target ends. The following video clip demonstrates the concept. 6.2.1 Future installation Install the future package. install.packages(&quot;future&quot;) # CRAN release # Alternatively, install the GitHub development version. devtools::install_github(&quot;HenrikBengtsson/future&quot;, ref = &quot;develop&quot;) If you intend to use a cluster, be sure to install the future.batchtools package too. The future ecosystem contains even more packages that extend future’s parallel computing functionality, such as future.callr. 6.2.2 Future locally To parallelize targets over multiple processes on your local machine, declare one of the future plans listed here in your _targets.R file. # _targets.R library(future) plan(multisession) tar_pipeline( tar_target(data, get_data()), tar_target(fast_fit, fit_small_model(data)), tar_target(slow_fit, fit_slow_model(data)), tar_target(plot_1, make_plot(fast_fit)), tar_target(plot_2, make_plot(slow_fit)) ) Then, run tar_make_future() with the desired number of workers. Here, the workers argument specifies the maximum number of transient workers to allow at a given time. Some future plans also have optional workers arguments that set their own caps. tar_make_future(workers = 2) 6.2.3 Future remotely To run transient workers on a cluster, first install the future.batchtools package. Then, set one of these plans in your _targets.R file. # _targets.R library(future) library(future.batchtools) plan(batchtools_sge, template = &quot;sge.tmpl&quot;) tar_pipeline( tar_target(data, get_data()), tar_target(fast_fit, fit_small_model(data)), tar_target(slow_fit, fit_slow_model(data)), tar_target(plot_1, make_plot(fast_fit)), tar_target(plot_2, make_plot(slow_fit)) ) Here, our template file sge.tmpl is configured for batchtools. #!/bin/bash #$ -cwd # Run in the current working directory. #$ -j y # Direct stdout and stderr to the same file. #$ -o &lt;%= log.file %&gt; # log file #$ -V # Use environment variables. #$ -N &lt;%= job.name %&gt; # job name module load R/3.6.3 # Uncomment and adjust if R is an environment module. Rscript -e &#39;batchtools::doJobCollection(&quot;&lt;%= uri %&gt;&quot;)&#39; # Leave alone. exit 0 # Leave alone. 6.2.4 Future configuration The tar_target(), tar_target_raw(), and tar_options() functions accept a resources argument similar to the template argument of tar_make_clustermq(). For example, if our batchtools template file has a wildcard for the number of cores for a job, #!/bin/bash #$ -pe smp &lt;%= resources[[&quot;num_cores&quot;]] | 1 %&gt; # Wildcard for cores per job. #$ -cwd #$ -j y #$ -o &lt;%= log.file %&gt; #$ -V #$ -N &lt;%= job.name %&gt; module load R/3.6.3 Rscript -e &#39;batchtools::doJobCollection(&quot;&lt;%= uri %&gt;&quot;)&#39; exit 0 then you can set the number of cores for individual targets. In the case below, maybe the slow model needs 2 cores to run fast enough. # _targets.R library(future) library(future.batchtools) plan(batchtools_sge, template = &quot;sge.tmpl&quot;) tar_pipeline( tar_target(data, get_data()), tar_target(fast_fit, fit_small_model(data)), tar_target(slow_fit, fit_slow_model(data), resources = list(num_cores = 2)), tar_target(plot_1, make_plot(fast_fit)), tar_target(plot_2, make_plot(slow_fit)) ) Then, run tar_make_future() as usual. tar_make_future(workers = 2) 6.3 Advanced Functions tar_target(), tar_target_raw(), and tar_options() support advanced configuration options for heavy-duty pipelines that require high-performance computing. deployment: With the deployment argument, you can choose to run some targets locally on the master process instead of on a high-performance computing worker. This options is suitable for lightweight targets such as R Markdown reports where runtime is quick and a cluster would be excessive. memory: Choose whether to retain a target in memory or remove it from memory whenever it is not needed at the moment. This is a tradeoff between memory consumption and storage read speeds, and like all of the options listed here, you can set it on a target-by-target basis. The default settings consume a lot of memory to avoid frequently reading from storage. To keep memory usage down to a minimum, set memory = \"transient\" in tar_options() and set garbage_collection = TRUE in your tar_make*() function. storage: Choose whether the parallel workers or the master process is responsible for saving the target’s value. For slow network file systems on clusters, storage = \"local\" is often faster for small numbers of targets. For large numbers of targets or low-bandwidth connections between the master and workers, storage = \"remote\" is often faster. Always choose storage = \"local\" if the workers do no have access to the file system with the _targets/ data store. retrieval: Choose whether the parallel workers or the master process is responsible for reading dependency targets from disk. Should usually be set to whatever you choose for storage (default). Always choose retrieval = \"local\" if the workers do no have access to the file system with the _targets/ data store. format: If your pipeline has large computation, it may also have large data. Consider setting the format argument to help targets store and retrieve your data faster. error: Set error to \"continue\" to let the rest of the pipeline keep running even if a target encounters an error. "]
]
