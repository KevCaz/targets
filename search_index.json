[["index.html", "The targets R Package User Manual Chapter 1 Introduction 1.1 Motivation 1.2 Pipeline toolkits 1.3 The targets package 1.4 About this manual 1.5 What about drake?", " The targets R Package User Manual Will Landau Copyright Eli Lilly and Company Chapter 1 Introduction The targets package is a Make-like pipeline toolkit for Statistics and data science in R. With targets, you can maintain a reproducible workflow without repeating yourself. targets learns how your pipeline fits together, skips costly runtime for tasks that are already up to date, runs only the necessary computation, supports implicit parallel computing, abstracts files as R objects, and shows tangible evidence that the results match the underlying code and data. 1.1 Motivation Data analysis can be slow. A round of scientific computation can take several minutes, hours, or even days to complete. After it finishes, if you update your code or data, your hard-earned results may no longer be valid. Unchecked, this invalidation creates chronic Sisyphean loop: Launch the code. Wait while it runs. Discover an issue. Restart from scratch. 1.2 Pipeline toolkits Pipeline toolkits like GNU Make break the cycle. They watch the dependency graph of the whole workflow and skip steps, or “targets”, whose code, data, and upstream dependencies have not changed since the last run of the pipeline. When all targets are up to date, this is evidence that the results match the underlying code and data, which helps us trust the results and confirm the computation is reproducible. 1.3 The targets package Unlike most pipeline toolkits, which are language agnostic or Python-focused, the targets package allows data scientists and researchers to work entirely within R. targets implicitly nudges users toward a clean, function-oriented programming style that fits the intent of the R language and helps practitioners maintain their data analysis projects. 1.4 About this manual This manual is a step-by-step user guide to targets. It walks through basic usage, outlines general best practices, dives deep into advanced features like high-performance computing, and helps drake users transition to targets. See the documentation website for most other major resources, including installation instructions, links to example projects, and a reference page with all user-side functions. 1.5 What about drake? The drake is an older R-focused pipeline toolkit, and targets is drake’s long-term successor. There is a special chapter to explain why targets was created, what this means for drake’s future, advice for drake users transitioning to the targets, and the main technical advantages of targets over drake. "],["walkthrough.html", "Chapter 2 Walkthrough 2.1 About this minimal example 2.2 File structure 2.3 Inspect the pipeline 2.4 Run the pipeline 2.5 Changes 2.6 Read your data 2.7 Read metadata", " Chapter 2 Walkthrough This chapter walks through a minimal example of a targets-powered data analysis project. The source code is available here, and it has a free RStudio Cloud workspace where you can try the code in your web browser. The documentation website links to other examples. 2.1 About this minimal example The goal of this minimal workflow is to assess the relationship among ozone, wind, and temperature in base R’s airquality dataset. We read the data from a file, preprocess it, visualize some of the variables, fit a regression model, and generate an R Markdown report to communicate the results. 2.2 File structure The file structure of the project looks like this. ├── _targets.R ├── R/ ├──── functions.R ├── data/ └──── raw_data.csv raw_data.csv contains the data we want to analyze. Ozone,Solar.R,Wind,Temp,Month,Day 36,118,8.0,72,5,2 12,149,12.6,74,5,3 ... functions.R contains our custom user-defined functions. (See the best practices chapter for a discussion of function-oriented workflows.) # functions.R create_plot &lt;- function(data) { ggplot(data) + geom_histogram(aes(x = Ozone)) + theme_gray(24) } Whereas files raw_data.csv and functions.R are typical user-defined components of a project-oriented workflow, _targets.R file is special. Every targets workflow needs a file called _targets.R in the project’s root directory. Functions tar_script() and tar_edit() can help you create one. Ours looks looks like this: # _targets.R library(targets) source(&quot;R/functions.R&quot;) options(tidyverse.quiet = TRUE) tar_option_set(packages = c(&quot;biglm&quot;, &quot;tidyverse&quot;)) list( tar_target( raw_data_file, &quot;data/raw_data.csv&quot;, format = &quot;file&quot; ), tar_target( raw_data, read_csv(raw_data_file, col_types = cols()) ), tar_target( data, raw_data %&gt;% mutate(Ozone = replace_na(Ozone, mean(Ozone, na.rm = TRUE))) ), tar_target(hist, create_plot(data)), tar_target(fit, biglm(Ozone ~ Wind + Temp, data)) ) All _targets.R scripts have these requirements. Load the targets package itself. (_targets.R scripts created with tar_script() automatically insert a library(targets) line at the top by default.) Load your custom functions and global objects into the R session. In our case, our only such object is the create_plot() function, and we load it into the session by calling source(\"R/functions.R\"). Call tar_option_set() to set the default settings for all you targets, such as the names of required packages and the data storage format. Individual targets can override these settings. Define individual targets with the tar_target() function. Each target is an intermediate step of the workflow. At minimum, a target must have a name and an R expression. This expression runs when the pipeline builds the target, and the return value is saved as a file in the _targets/objects/ folder. The only targets not stored in _/targets/objects/ are dynamic files such as raw_data_file. Here, format = \"file\" makes raw_data_file a dynamic file. That means targets watches the data at the file paths returned from the expression (in this case, \"data/raw_data.csv\"). Every _targets.R script must end with a list of your tar_target() objects. Those objects can be nested, i.e. lists within lists. 2.3 Inspect the pipeline Before you run the pipeline for real, you should always inspect the manifest and the graph for errors. tar_manifest() shows you a data frame information about the targets, and it has functionality to specify the targets and columns returned. tar_manifest(fields = &quot;command&quot;) #&gt; # A tibble: 5 x 2 #&gt; name command #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 raw_data_fi… &quot;\\&quot;data/raw_data.csv\\&quot;&quot; #&gt; 2 raw_data &quot;read_csv(raw_data_file, col_types = cols())&quot; #&gt; 3 data &quot;raw_data %&gt;% mutate(Ozone = replace_na(Ozone, mean(Ozone, \\\\n … #&gt; 4 fit &quot;biglm(Ozone ~ Wind + Temp, data)&quot; #&gt; 5 hist &quot;create_plot(data)&quot; There are also graphical displays with tar_glimpse() tar_glimpse() and tar_visnetwork(). tar_visnetwork() Both graphing functions above visualize the underlying directed acyclic graph (DAG) and tell you how targets are connected. This DAG is indifferent to the order of targets in your pipeline. You will still get the same graph even if you rearrange them. This is because targets uses static code analysis to detect the dependencies of each target, and this process does not depend on target order. For details, visit the dependency detection section of the best practices guide. 2.4 Run the pipeline tar_make() runs the workflow. It creates a fresh clean external R process, reads _targets.R to learn about the pipeline, runs the correct targets in the correct order given by the graph, and saves the necessary data to the _targets/ data store. tar_make() #&gt; ● run target raw_data_file #&gt; ● run target raw_data #&gt; ● run target data #&gt; ● run target fit #&gt; ● run target hist #&gt; ● end pipeline The next time you run tar_make(), targets skips everything that is already up to date, which saves a lot of time in large projects with long runtimes. tar_make() #&gt; ✔ skip target raw_data_file #&gt; ✔ skip target raw_data #&gt; ✔ skip target data #&gt; ✔ skip target fit #&gt; ✔ skip target hist #&gt; ✔ skip pipeline You can use tar_visnetwork() and tar_outdated() to check ahead of time which targets are up to date. tar_visnetwork() tar_outdated() #&gt; character(0) 2.5 Changes The targets package notices when you make changes to your workflow, and tar_make() only runs the targets that need to build. There are custom rules called “cues” that targets uses to decide whether a target needs to rerun. 1 2.5.1 Change code If you change one of your functions, the targets that depend on it will no longer be up to date, and tar_make() will rebuild them. For example, let’s set the number of bins in our histogram. # Edit functions.R. create_plot &lt;- function(data) { ggplot(data) + geom_histogram(aes(x = Ozone), bins = 10) + # Set number of bins. theme_gray(24) } targets detects the change. hist is outdated (as would be any targets downstream of hist) and the others are still up to date. tar_visnetwork() tar_outdated() #&gt; [1] &quot;hist&quot; That means tar_make() reruns hist and nothing else. tar_make() #&gt; ✔ skip target raw_data_file #&gt; ✔ skip target raw_data #&gt; ✔ skip target data #&gt; ✔ skip target fit #&gt; ● run target hist #&gt; ● end pipeline We would see similar behavior if we changed the R expressions in any tar_target() calls in _targets.R. 2.5.2 Change data If we change the data file raw_data.csv, targets notices the change. This is because raw_data_file is a dynamic file (i.e. tar_target(format = \"file\")) that returned \"raw_data.csv\". Let’s try it out. Below, let’s use only the first 100 rows of the airquality dataset. write_csv(head(airquality, n = 100), &quot;data/raw_data.csv&quot;) Sure enough, raw_data_file and everything downstream is out of date, so all our targets are outdated. tar_visnetwork() tar_outdated() #&gt; [1] &quot;raw_data&quot; &quot;fit&quot; &quot;hist&quot; &quot;raw_data_file&quot; #&gt; [5] &quot;data&quot; tar_make() #&gt; ● run target raw_data_file #&gt; ● run target raw_data #&gt; ● run target data #&gt; ● run target fit #&gt; ● run target hist #&gt; ● end pipeline 2.6 Read your data targets has a convenient functions tar_read() to read your data from the _targets/ data store. tar_read(hist) There is also a tar_load() function, which supports tidyselect verbs like starts_with() tar_load(starts_with(&quot;fit&quot;)) library(biglm) #&gt; Loading required package: DBI fit #&gt; Large data regression model: biglm(Ozone ~ Wind + Temp, data) #&gt; Sample size = 100 The purpose of tar_read() and tar_load() is to make exploratory data analysis easy and convenient. Use these functions to verify the correctness of the output from the pipeline and come up with ideas for new targets if needed. 2.7 Read metadata To read the build progress of your targets while tar_make() is running, you can open a new R session and run tar_progress(). It reads the spreadsheet in _targets/meta/progress and tells you which targets are running, built, errored, or cancelled. tar_progress() #&gt; # A tibble: 5 x 2 #&gt; name progress #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 raw_data_file built #&gt; 2 raw_data built #&gt; 3 data built #&gt; 4 fit built #&gt; 5 hist built Likewise, the tar_meta() function reads _targets/meta/meta and tells you high-level information about the target’s settings, data, and results. The warnings, error, and traceback columns give you diagnostic information about targets with problems. as.data.frame(tar_meta()) #&gt; name type data command depend #&gt; 1 create_plot function 658f2a44b31f63bb &lt;NA&gt; &lt;NA&gt; #&gt; 2 raw_data_file stem 2b16f490030787ce b6df0c34fc22d1b9 ef46db3751d8e999 #&gt; 3 raw_data stem 81f24e414a848be0 000ed0cc054f0d35 6ef08fe7f06c82ac #&gt; 4 data stem 1e1d6c4923db1fa9 df3101ce91c63e21 1e037ea6a50b00ac #&gt; 5 fit stem d01f8b0736dde4fe aa0df6c5dbd10537 fb68e10410f0f3c8 #&gt; 6 hist stem 53d24f4f6d889d9d 68877181ab74e51e 39b2e88285ec367d #&gt; seed path time size bytes #&gt; 1 NA NA &lt;NA&gt; &lt;NA&gt; NA #&gt; 2 2110307107 data/raw_data.csv 7de8f5bef68df803 9fb65f8dffe0153b 1884 #&gt; 3 -979620141 _targets/objects/raw_data 78f1b9b024e8007e 029e3f18865b911c 1152 #&gt; 4 1588979285 _targets/objects/data 52af38dd5fafad02 a744f0f8f0be9d70 1156 #&gt; 5 1780184594 _targets/objects/fit 8ac441e60dd8920a 31f6a3e6158ce3a3 1608 #&gt; 6 -1026346201 _targets/objects/hist 75e02784616080f8 631abafa35c3f47a 44325 #&gt; format iteration parent children seconds warnings error #&gt; 1 &lt;NA&gt; &lt;NA&gt; NA NA NA NA NA #&gt; 2 file vector NA NA 1.316 NA NA #&gt; 3 rds vector NA NA 0.033 NA NA #&gt; 4 rds vector NA NA 0.016 NA NA #&gt; 5 rds vector NA NA 0.003 NA NA #&gt; 6 rds vector NA NA 0.014 NA NA The _targets/meta/meta spreadsheet file is critically important. Although targets can still work properly if files are missing from _targets/objects, the pipeline will error out if _targets/meta/meta is corrupted. If tar_meta() works, the project should be fine. For the full details on cues, read the “Details” section of the tar_cue() help file. (Enter ?targets::tar_cue into your R console.)↩︎ "],["debugging.html", "Chapter 3 Debugging 3.1 Workspaces 3.2 Interactive debugging", " Chapter 3 Debugging If one of your targets fails, first look up the error message in tar_meta(). If that does not help, try one of the following techniques. 3.1 Workspaces Workspaces are special lightweight reference files that allow tar_workspace() to recreate the runtime environment of a target. This lets you troubleshoot issues outside the pipeline in an interactive session. There are two ways to save a workspace file: Set error = \"workspace\" in tar_option_set() or tar_target(). Then, tar_make() and friends will save a workspace file for every target that errors out. In the workspaces argument of tar_option_set(), specify the targets for which you want to save workspaces. Then, run tar_make() or similar. A workspace file will be saved for each existing target, regardless of whether the target runs or gets skipped in the pipeline. Here is an example of (1). # _targets.R file: options(tidyverse.quiet = TRUE) library(targets) library(tidyverse) options(crayon.enabled = FALSE) tar_option_set(error = &quot;workspace&quot;) f &lt;- function(arg, value) { stopifnot(arg &lt; 4) } list( tar_target(x, seq_len(4)), tar_target( y, f(arg = x, value = &quot;succeeded&quot;, a = 1, b = 2, key = &quot;my_api_key&quot;), pattern = map(x) # The branching chapter describes patterns. ) ) # R console: tar_make() #&gt; ● run target x #&gt; ● run branch y_29239c8a #&gt; ● run branch y_7cc32924 #&gt; ● run branch y_bd602d50 #&gt; ● run branch y_05f206d7 #&gt; x error branch y_05f206d7 #&gt; ● save workspace y_05f206d7 #&gt; Error : x &lt; 4 is not TRUE . #&gt; Error: callr subprocess failed: x &lt; 4 is not TRUE . One of the y_******* targets errored out. failed &lt;- tar_meta(fields = error) %&gt;% na.omit() %&gt;% pull(name) print(failed) #&gt; [1] &quot;y_05f206d7&quot; tar_workspace() reads the special metadata in the workspace file and then loads the target’s dependencies from various locations in _targets/objects and/or the cloud. It also sets the random number generator seed to the seed of the target, loads the required packages, and runs _targets.R to load other global object dependencies such as functions. tar_workspace(y_05f206d7) We now have the dependencies of y_05f206d7 in memory, which allows you to try out any failed function calls in your local R session. 2 3 print(x) #&gt; [1] 4 f(arg = 0, value = &quot;my_value&quot;, a = 1, b = 2, key = &quot;my_api_key&quot;) #&gt; [1] &quot;my_value&quot; f(arg = x, value = &quot;my_value&quot;, a = 1, b = 2, key = &quot;my_api_key&quot;) #&gt; Error in f(x) : x &lt; 4 is not TRUE Keep in mind that that although the dependencies of y_05f206d7 are in memory, the arguments of f() are not. arg #&gt; Error: object &#39;arg&#39; not found value #&gt; Error: object &#39;value&#39; not found 3.2 Interactive debugging Interactive debugging offers a browser()-like debugging experience. Unlike workspaces, which most users access after the pipline finishes, interactive debugging lets you step through a target’s command while the target is running. This approach is not feasible if the pipeline is running in a non-interactive process or requires resources that are only available on a remote compute node of a cluster. However, when possible, it may be more convenient than workspaces. In our example above, tar_workspace() automatically loaded a branch of the dependency x, but the user still needed to manually supply the rest of the arguments of f(). During interactive debugging, you will instead be able to call debug(f) and then press c to immediately drop into the function environment where the remaining variables are already defined. Follow these steps to use interactive debugging. In _targets.R, write a call to tar_option_set() with debug equal to the target name. Consider also setting cue equal to tar_cue(mode = \"never\") so tar_make() reaches the target you want to debug more quickly. Launch a fresh clean new interactive R session with the _targets.R script in your working directory. Run targets::tar_make() (or targets::tar_make_clustermq(), or targets::tar_make_future()) with callr_function = NULL. When targets reaches the target you selected to debug, your R session will start an interactive debugger, and you should see Browse[1]&gt; in your console. Run targets::tar_name() to verify that you are debugging the correct target. Interactively run any R code that helps you troubleshoot the problem. For example, if the target invokes a function f(), enter debug(f) and then c to immediately enter the function’s calling environment where all its arguments are defined. To try it out yourself, write the following _targets.R file. # _targets.R library(targets) tar_option_set(debug = &quot;b&quot;) f &lt;- function(x, another_arg = 123) x + another_arg list( tar_target(a, 1), tar_target(b, f(a)) ) Then, call tar_make(callr_function = NULL) to drop into a debugger at the command of b. # R console tar_make(callr_function = NULL) #&gt; ● run target a #&gt; ● run target b #&gt; Called from: eval(expr, envir) Browse[1]&gt; When the debugger launches, run targets::tar_name() to confirm you are running the correct target. Browse[1]&gt; targets::tar_name() #&gt; [1] &quot;b&quot; In the debugger, the dependency targets of b are available in the current environment, and the global objects and functions are available in the parent environment. Browse[1]&gt; ls() #&gt; [1] &quot;a&quot; Browse[1]&gt; a #&gt; [1] 1 Browse[1]&gt; ls(parent.env(environment())) #&gt; [1] &quot;f&quot; Browse[1]&gt; f(1) #&gt; [1] 124 Enter debug(f) to debug the function f(), and press c to enter the function’s calling environment where another_arg is defined. Browse[1]&gt; debug(f) Browse[1]&gt; c #&gt; debugging in: f(a) #&gt; debug at _targets.R#3: x + another_arg Browse[2]&gt; ls() #&gt; [1] &quot;another_arg&quot; &quot;x&quot; Browse[2]&gt; another_arg #&gt; [1] 123 Visit this page for more information on debugging R code. In addition, current random number generator seed (.Random.seed) is also the value y_05f206d7 started with.↩︎ When you are finished debugging, you can remove all workspace files with tar_destroy(destroy = \"workspaces\").↩︎ "],["functions.html", "Chapter 4 Functions 4.1 Problems with script-based workflows 4.2 Functions 4.3 Writing functions 4.4 Functions in pipelines", " Chapter 4 Functions targets expects users to adopt a function-oriented style of programming. User-defined R functions are essential to express the complexities of data generation, analysis, and reporting. This chapter explains what makes functions useful and how to leverage them in your pipelines. 4.1 Problems with script-based workflows Traditional data analysis projects consist of imperative scripts, often with with numeric prefixes. 01-data.R 02-model.R 03-plot.R To run the project, the user runs each of the scripts in order. source(&quot;01-data.R&quot;) source(&quot;02-model.R&quot;) source(&quot;03-plot.R&quot;) Each script executes a different part of the workflow. # 01-data.R library(tidyverse) raw_data &lt;- read_csv(&quot;data/raw_data.csv&quot;, col_types = cols()) data &lt;- raw_data %&gt;% mutate(Ozone = replace_na(Ozone, mean(Ozone, na.rm = TRUE))) write_csv(data, &quot;data/data.csv&quot;) # 02-model.R library(biglm) library(tidyverse) data &lt;- read_csv(&quot;data/data.csv&quot;, col_types = cols()) fit &lt;- biglm(Ozone ~ Wind + Temp, data) saveRDS(fit, &quot;fit.rds&quot;) # 03-plot.R library(tidyverse) data &lt;- read_csv(&quot;data/data.csv&quot;, col_types = cols()) hist &lt;- ggplot(data) + geom_histogram(aes(x = Ozone)) + theme_gray(24) ggsave(&quot;hist.png&quot;, hist) Although this approach may feel convenient at first, it scales poorly for medium-sized workflows. These imperative scripts are monolithic, and they grow too large and complicated to understand or maintain. 4.2 Functions Functions are the building blocks of most computer code. They make code easier to think about, and they break down complicated ideas into small manageable pieces. Out of context, you can develop and test a function in isolation without mentally juggling the rest of the project. In the context of the whole workflow, functions are convenient shorthand to make your work easier to read. In addition, functions are a nice mental model to express data science. A data analysis workflow is a sequence of transformations: datasets map to analyses, and analyses map to summaries. In fact, a function for data science typically falls into one of three categories: Process a dataset. Analyze a dataset. Summarize an analysis. 4.3 Writing functions Let us begin with our imperative code for data processing. Every time you look at it, you need to read it carefully and relearn what it does. And test it, you need to copy the entire block into the R console. raw_data &lt;- read_csv(&quot;data/raw_data.csv&quot;, col_types = cols()) data &lt;- raw_data %&gt;% mutate(Ozone = replace_na(Ozone, mean(Ozone, na.rm = TRUE))) It is better to encapsulate this code in a function. read_and_clean &lt;- function(path) { raw_data &lt;- read_csv(path, col_types = cols()) raw_data %&gt;% mutate(Ozone = replace_na(Ozone, mean(Ozone, na.rm = TRUE))) } Now, instead of invoking a whole block of text, all you need to do is type a small reusable command. The function name speaks for itself, so you can recall what it does without having to mentally process all the details again. read_and_clean(&quot;data/raw_data.csv&quot;) As with the data, we can write a function to fit a model, fit_model &lt;- function(data) { biglm(Ozone ~ Wind + Temp, data) } and another function to plot the data. create_plot &lt;- function(data) { ggplot(data) + geom_histogram(aes(x = Ozone)) + theme_gray(24) } 4.4 Functions in pipelines Without those functions, our pipeline in the walkthrough chapter would look long, complicated, and difficult to digest. # _targets.R library(targets) source(&quot;R/functions.R&quot;) options(tidyverse.quiet = TRUE) tar_option_set(packages = c(&quot;biglm&quot;, &quot;rmarkdown&quot;, &quot;tidyverse&quot;)) list( tar_target(raw_data_file, &quot;data/raw_data.csv&quot;, format = &quot;file&quot;), tar_target(raw_data, read_csv(raw_data_file, col_types = cols())), tar_target( data, raw_data %&gt;% mutate(Ozone = replace_na(Ozone, mean(Ozone, na.rm = TRUE))) ), tar_target(fit, biglm(Ozone ~ Wind + Temp, data)), tar_target( hist, ggplot(data) + geom_histogram(aes(x = Ozone)) + theme_gray(24) ) ) But if we write our functions in R/functions.R and source() them into _targets.R, the pipeline becomes much easier to read. We can even condense out raw_data and data targets together without creating a large command. # _targets.R library(targets) source(&quot;R/functions.R&quot;) options(tidyverse.quiet = TRUE) tar_option_set(packages = c(&quot;biglm&quot;, &quot;tidyverse&quot;)) list( tar_target(raw_data_file, &quot;data/raw_data.csv&quot;,format = &quot;file&quot;), tar_target(data, read_and_clean(raw_data_file)), tar_target(fit, fit_model(data)), tar_target(hist, create_plot(data)) ) "],["practices.html", "Chapter 5 Best practices 5.1 How to define good targets 5.2 Dependencies 5.3 Workflows as R packages 5.4 Working with tools outside R 5.5 Monitoring the pipeline 5.6 Performance 5.7 Cleaning up", " Chapter 5 Best practices This chapter describes additional best practices for developing and maintaining targets-powered projects. 5.1 How to define good targets Targets are imperative high-level steps of the workflow that run the work you define in your functions. Like functions, targets generally focus on datasets, analyses, and summaries. The targets package automatically skips targets that are already up to date, so you should strive to define targets that maximize time savings. Good targets usually Are large enough to subtract a decent amount of runtime when skipped. Are small enough that some targets can be skipped even if others need to run. Invoke no side effects such as modifications to the global environment. (But targets with tar_target(format = \"file\") can save files.) Return a single value that is Easy to understand and introspect. Meaningful to the project. Easy to save as a file, e.g. with readRDS(). Regarding the last point above, it is possible to customize the storage format of the target. For details, enter ?tar_target in the console and scroll down to the description of the format argument. 5.2 Dependencies Adept pipeline construction requires an understanding of dependency detection. To identify the targets and global objects that each target depends on, the targets package uses static code analysis with codetools, and you can emulate this process with tar_deps(). Let us look at the dependencies of the raw_data target. tar_deps(function() { read_csv(raw_data_file, col_types = cols()) }) The raw_data target depends on target raw_data_file because the command for raw_data mentions the symbol raw_data_file. Similarly, if we were to create a user-defined read_csv() function, the raw_data target would also depend on read_csv() and any other user-defined global functions and objects nested inside read_csv(). Changes to any of these objects would cause the raw_data target to rerun on the next tar_make(). Not all of the objects from tar_deps() actually register as dependencies. When it comes to detecting dependencies, targets only recognizes Other targets (such as raw_data_file). Functions and objects in the main environment. This environment is almost always the global environment of the R process that runs _targets.R, so these dependencies are usually going to be the custom functions and objects you write yourself. This process excludes many objects from dependency detection. For example, both { and cols() are excluded because they are defined in the environments of packages (base and readr, respectively). Functions and objects from packages are ignored unless you supply a package environment to the envir argument of tar_option_set() when you call it in _targets.R, e.g. tar_option_set(envir = getNamespace(\"packageName\")). You should only set envir if you write your own package to contain your whole data analysis project. 5.3 Workflows as R packages When it comes time to decide which targets to rerun or skip, the default behavior is to ignore changes to external R packages. Usually, local package libraries do not need to change very often, and it is best to maintain a reproducible project library using renv. However, there are some situations where it makes sense to watch a package for changes. For example, you could be in the middle of developing a methodology package that serves as the focus of the pipeline, or you could implement the workflow itself as a package. In either case, you can tell targets to track changes using the imports argument to tar_option_set(). If you write tar_option_set(imports = \"package1\") in _targets.R, then targets will analyze the R objects in package1 and automatically rerun the dependent targets when these objects change. These tracked objects include unexported functions internal to the package. You can track multiple packages this way, e.g. tar_option_set(imports = c(\"package1\", \"package2\")). In this case, the contents of package1 override those of package2 when there are name conflicts. Likewise, tar_option_get(\"envir\") (usually the global environment) overrides both. 5.4 Working with tools outside R targets lives and operates entirely within the R interpreter, so working with outside tools is a matter of finding the right functionality in R itself. system2() and processx can invoke system commands outside R, and you can include them in your targets’ R commands to run shell scripts, Python scripts, etc. There are also specialized R packages to retrieve data from remote sources and invoke web APIs, including rnoaa, ots, and aws.s3. 5.5 Monitoring the pipeline If you are using targets, then you probably have an intense computation like Bayesian data analysis or machine learning. These tasks take a long time to run, and it is a good idea to monitor them. If you are running the work on your local machine, you can monitor parallel workers with a utility like top or htop. If you are using a traditional HPC scheduler like SLURM or SGE, you can check the status of the workers with squeue, qstat, or similar. But those tools do not always give you a high-level view of what the pipeline has done and which targets are going to run next. For that information, targets has options: tar_progress() returns a data frame describing the targets that started, finished running successfully, got canceled, or errored out. tar_visnetwork() shows this progress information in an interactive visNetwork widget. Set outdated to FALSE to get slightly more detailed progress information. tar_watch() launches an Shiny app that automatically refreshes the graph every few seconds. Try it out in the example below. # Define an example _targets.R file with a slow pipeline. library(targets) tar_script({ sleep_run &lt;- function(...) { Sys.sleep(10) } list( tar_target(settings, sleep_run()), tar_target(data1, sleep_run(settings)), tar_target(data2, sleep_run(settings)), tar_target(data3, sleep_run(settings)), tar_target(model1, sleep_run(data1)), tar_target(model2, sleep_run(data2)), tar_target(model3, sleep_run(data3)), tar_target(figure1, sleep_run(model1)), tar_target(figure2, sleep_run(model2)), tar_target(figure3, sleep_run(model3)), tar_target(conclusions, sleep_run(c(figure1, figure2, figure3))) ) }) # Launch the app in a background process. # You may need to refresh the browser if the app is slow to start. # The graph automatically refreshes every 10 seconds tar_watch(seconds = 10, outdated = FALSE, targets_only = TRUE) # Now run the pipeline and watch the graph change. px &lt;- tar_make() tar_watch_ui() and tar_watch_server() make this functionality available to other apps through a Shiny module. 5.6 Performance If your pipeline has several thousand targets, functions like tar_make(), tar_outdated(), and tar_visnetwork() may take longer to run. There is an inevitable per-target runtime cost because package needs to check the code and data of each target individually. If this overhead becomes too much, consider batching your work into a smaller group of heavier targets. Using your custom functions, you can make each target perform multiple iterations of a task that was previously given to targets one at a time. For details and an example, please see the discussion on batching at the bottom of the dynamic branching chapter. Alternatively, if you see slowness in your project, you can contribute to the package with a profiling study. These contributions are great because they help improve the package. Here are the recommended steps. Install the proffer R package and its dependencies. Run proffer::pprof(tar_make(callr_function = NULL)) on your project. When a web browser pops up with pprof, select the flame graph and screenshot it. Post the flame graph, along with any code and data you can share, to the targets package issue tracker. The maintainer will have a look and try to make the package faster for your use case if speedups are possible. 5.7 Cleaning up There are multiple functions to help you manually remove data or force targets to rerun. tar_destroy() is by far the most commonly used cleaning function. It removes the _targets/ data store completely, deleting all the results from tar_make() except for external files. Use it if you intend to start the pipeline from scratch without any trace of a previous run. tar_prune() deletes the data and metadata of all the targets no longer present in your current _targets.R file. This is useful if you recently worked through multiple changes to your project and are now trying to discard irrelevant data while keeping the results that still matter. tar_delete() is more selective than tar_destroy() and tar_prune(). It removes the individual data files of a given set of targets from _targets/objects/ while leaving the metadata in _targets/meta/meta alone. If you have a small number of data-heavy targets you need to discard to conserve storage, this function can help. tar_invalidate() is the opposite of tar_delete(): for the selected targets, it deletes the metadata in _targets/meta/meta but keeps the return values in _targets/objects/. After invalidation, you will still be able to locate the data files with tar_path() and manually salvage them in an emergency. However, tar_load() and tar_read() will not be able to read the data into R, and subesequent calls to tar_make() will attempt to rebuild those targets. "],["files.html", "Chapter 6 External files and literate programming 6.1 Internal files 6.2 External input files 6.3 External output files 6.4 Literate programming 6.5 Parameterized R Markdown", " Chapter 6 External files and literate programming The targets package automatically stores data and automatically responds to changed files to keep your targets up to date. The chapter below explains how to leverage this reproducibility for external datasets, external output files, and literate programming artifacts such as R Markdown reports. Real-world applications of these techniques are linked from here. 6.1 Internal files Each project’s data lives in the _targets/ folder in the root directory (where you call tar_make()). The files in the _targets/ look like this: _targets/ ├── meta/ ├────── progress ├────── meta ├── objects/ ├────── target1 ├────── target2 ├────── branching_target_c7bcb4bd ├────── branching_target_285fb6a9 ├────── branching_target_874ca381 └── scratch/ # tar_make() deletes this folder after it finishes. Spreadsheets _targets/meta/meta and _targets/meta/progress keep track of target metadata, and the scratch/ directory contains temporary files which can be safely deleted after tar_make() finishes. The _targets/objects/ folder contains the return values of the targets themselves. A typical target returns an R object: for example, a dataset with tar_target(dataset, data.frame(x = rnorm(1000)), format = \"fst\") or a fitted model tar_target(model, biglm(ozone ~ temp + wind), format = \"qs\"). When you run the pipeline, targets computes this object and saves it as a file in _targets/objects/. The file name in _targets/objects/ is always the target name, and type of the file is determined by the format argument of tar_target(), and formats \"fst\" and \"qs\" are two of many choices explained in the help file of tar_target(). No matter what format you pick, targets watches the file for changes and recomputes the target in tar_make() if the the file gets corrupted (unless you suppress the file cue with tar_target(cue = tar_cue(file = FALSE))). 6.2 External input files To reproducibly track an external input file, you need to define a new target that has A command that returns the file path as a character vector, and format = \"file\" in tar_target(). When the target runs in the pipeline, the returned character vector gets recorded in _targets/meta, and targets watches the data file and invalidates the target when that file changes. To track multiple files or directories this way, simply define a multi-element character vector where each element is a path. The first two targets of the minimal example demonstrate how to track an input file. # _targets.R library(targets) path_to_data &lt;- function() { &quot;data/raw_data.csv&quot; } list( tar_target( raw_data_file, path_to_data(), format = &quot;file&quot; ), tar_target( raw_data, read_csv(raw_data_file, col_types = cols()) ) ) Above, raw_data_file is the dynamic file target. The file data/raw_data.csv exists before we ever run the pipeline, and the R expression for the target returns the character vector \"data/raw_data.csv\". (We use the path_to_data() function to demonstrate that you need not literally write \"data/raw_data.csv\" as long as the path is returned somehow.) All subsequent targets that depend on the file must reference the file using the symbol raw_data_file. This allows targets’ automatic static code analysis routines to detect which targets depend on the file. Because the raw_data target literally mentions the symbol raw_data_file, targets knows raw_data depends on raw_data_file. This ensures that raw_data_file gets processed before raw_data, and tar_make() automatically reruns raw_data if raw_data_file or \"data/raw_data.csv\" change. tar_visnetwork() If we were to omit the symbol raw_data_file from the R expression of raw_data, those targets would be disconnected in the graph and tar_make() would make incorrect decisions. # _targets.R library(targets) path_to_data &lt;- function() { &quot;data/raw_data.csv&quot; } list( tar_target( raw_data_file, path_to_data(), format = &quot;file&quot; ), tar_target( raw_data, read_csv(&quot;data/raw_data.csv&quot;, col_types = cols()) # incorrect ) ) tar_visnetwork() 6.3 External output files We can generate and track custom external files too, and the mechanics are similar. We still return a file path and use format = \"file\", but this time, our R command writes a file before it returns a path. For an external plot file, our target might look like this. tar_target( plot_file, save_plot_and_return_path(), format = &quot;file&quot; ) where our custom save_plot_and_return_path() function does exactly what the name describes. save_plot_and_return_path &lt;- function() { plot &lt;- ggplot(mtcars) + geom_point(aes(x = wt, y = mpg)) ggsave(&quot;plot_file.png&quot;, plot, width = 7, height = 7) return(&quot;plot_file.png&quot;) } 6.4 Literate programming R Markdown reports are a little more challenging because they often depend on upstream targets. To integrate an R Markdown report with a targets pipeline, you must use tar_read() and tar_load() in active code chunks to explicitly name the targets that the report depends on. (Do not use tar_read_raw() or tar_load_raw() for this.) The report from the minimal example looks like this. Above, the report depends on targets fit and hist. The use of tar_read() and tar_load() allows us to run the report outside the pipeline. (Again, do not use tar_read_raw() or tar_load_raw() for this.) As long as _targets/ folder has data on the required targets from a previous tar_make(), you can open the RStudio IDE, edit the report, and click the Knit button like you would for any other R Markdown report. To connect the target with the pipeline, we define a special kind of target using tar_render() from the tarchetypes package instead of the usual tar_target(), which Finds all the tar_load()/tar_read() dependencies in the report and inserts them into the target’s command. This enforces the proper dependency relationships. (tar_load_raw() and tar_read_raw() are ignored because those dependencies cannot be resolved with static code analysis.) Sets format = \"file\" (see tar_target()) so targets watches the files at the returned paths. Configures the target’s command to return both the output report files and the input source file. All these file paths are relative paths so the project stays portable. Forces the report to run in the user’s current working directory instead of the working directory of the report. Sets convenient default options such as deployment = \"main\" in tar_target() and quiet = TRUE in rmarkdown::render(). The target definition looks like this. library(tarchetypes) tar_render(report, &quot;report.Rmd&quot;) Because symbols fit and hist appear in the command, targets knows that report depends on fit and hist. When we put the report target in the pipeline, these dependency relationships show up in the graph. # _targets.R library(targets) library(tarchetypes) source(&quot;R/functions.R&quot;) list( tar_target( raw_data_file, &quot;data/raw_data.csv&quot;, format = &quot;file&quot; ), tar_target( raw_data, read_csv(raw_data_file, col_types = cols()) ), tar_target( data, raw_data %&gt;% mutate(Ozone = replace_na(Ozone, mean(Ozone, na.rm = TRUE))) ), tar_target(hist, create_plot(data)), tar_target(fit, biglm(Ozone ~ Wind + Temp, data)), tar_render(report, &quot;report.Rmd&quot;) # Here is our call to tar_render(). ) tar_visnetwork() 6.5 Parameterized R Markdown Functions in tarchetypes make it straightforward to use parameterized R Markdown in a targets pipeline. The next two subsections walk through the major use cases. 6.5.1 Single parameter set In this scenario, the pipeline renders your parameterized R Markdown report one time using a single set of parameters. These parameters can be upstream targets, global objects, or fixed values. Simply pass a params argument to tarchetypes::tar_render(): # _targets.R library(targets) library(tarchetypes) list( tar_target(data, data.frame(x = seq_len(26), y = letters)) tar_render(report, &quot;report.Rmd&quot;, params = list(your_param = data)) ) the report target will run: rmarkdown::render(&quot;report.Rmd&quot;, params = list(your_param = your_target)) where report.Rmd has the following YAML front matter: --- title: report output_format: html_document params: your_param: &quot;default value&quot; --- and the following code chunk: print(params$your_param) See these examples for a demonstration. 6.5.2 Multiple parameter sets In this scenario, you still have a single report, but you render it multiple times over multiple sets of R Markdown parameters. This time, use tarchetypes::tar_render_rep() and write code to reference or generate a grid of parameters with one row per rendered report and one column per parameter. Optionally, you can also include an output_file column to control the file paths of the generated reports, and you can set the number of batches to reduce the overhead that would otherwise ensue from creating a large number of targets. # _targets.R library(targets) library(tarchetypes) library(tibble) list( tar_target(x, &quot;value_of_x&quot;), tar_render_rep( report, &quot;report.Rmd&quot;, params = tibble( par = c(&quot;par_val_1&quot;, &quot;par_val_2&quot;, &quot;par_val_3&quot;, &quot;par_val_4&quot;), output_file = c(&quot;f1.html&quot;, &quot;f2.html&quot;, &quot;f3.html&quot;, &quot;f4.html&quot;) ), batches = 2 ) ) where report.Rmd has the following YAML front matter: title: report output_format: html_document params: par: &quot;default value&quot; and the following R code chunk: print(params$par) print(tar_read(x)) tar_render_rep() creates multiple targets to set up the R Markdown part of the workflow, including a target for the grid of parameters and a dynamic branching target to iterate over the parameters in batches. In this case, we have two batches (dynamic branches) and each one renders the report twice. tar_make() #&gt; ● run target x #&gt; ● run target report_params #&gt; ● run branch report_9e7470a1 #&gt; ● run branch report_457829de The third output file f3.html is below, and the rest look similar. For more information, see these examples. "],["dynamic.html", "Chapter 7 Dynamic branching 7.1 Branching 7.2 Abort dynamic branching 7.3 Patterns 7.4 Pattern construction 7.5 Branch provenance 7.6 Testing patterns 7.7 Dynamic branching over files 7.8 Iteration 7.9 Batching", " Chapter 7 Dynamic branching 7.1 Branching Sometimes, a pipeline contains more targets than a user can comfortably type by hand. For projects with hundreds of targets, branching can make the _targets.R file more concise and easier to read and maintain. targets supports two types of branching: dynamic branching and static branching. Some projects are better suited to dynamic branching, while others benefit more from static branching or a combination of both. Some users understand dynamic branching more easily because it avoids metaprogramming, while others prefer static branching because tar_manifest() and tar_visnetwork() provide immediate feedback. Except for the section on dynamic-within-static branching, you can read the two chapters on branching in any order (or skip them) depending on your needs. 7.2 Abort dynamic branching Dynamic branching is the act of defining new targets (i.e. branches) while the pipeline is running. Prior to launching the pipeline, the user does not necessarily know which branches will spawn or how many branches there will be, and each branch’s inputs are determined at the last minute. Relative to static branching, dynamic branching is better suited to iterating over a larger number of very similar tasks (but can act as an inner layer inside static branching, as the next chapter demonstrates). 7.3 Patterns To use dynamic branching, set the pattern argument of tar_target(). A pattern is a dynamic branching specification expressed in terms of functional programming. The following minimal example explores the mechanics of patterns (and examples of branching in real-world projects are linked from here). # _targets.R library(targets) library(tidyverse) list( tar_target(w, c(1, 2)), tar_target(x, c(10, 20)), tar_target(y, w + x, pattern = map(w, x)), tar_target(z, sum(y)), tar_target(z2, length(y), pattern = map(y)) ) tar_visnetwork() tar_make() Above, targets w, x, and z are called stems because they provide values for other targets to branch over. Target y is a pattern because it defines multiple sub-targets, or branches, based on the return values of the targets named inside map() or cross(). If we read target y into memory, all the branches will load and get aggregated according to the iteration argument of tar_target(). tar_read(y) Target z accepts this entire aggregate of y and sums it. tar_read(z) Target z2 maps over y, so each each branch of z2 accepts a branch of y. tar_read(z2) 7.4 Pattern construction targets supports the following pattern types. map(): iterate over one or more targets in sequence. cross(): iterate over combinations of slices of targets. head(): restrict branching to the first few elements. tail(): restrict branching to the last few elements. sample(): restrict branching to a random subset of elements. These patterns are composable. Below, target z creates six branches, one for each combination of w and (x, y) pair. The pattern cross(w, map(x, y)) is equivalent to tidyr::crossing(w, tidyr::nesting(x, y)). # _targets.R library(targets) list( tar_target(w_comp, seq_len(2)), tar_target(x_comp, head(letters, 3)), tar_target(y_comp, head(LETTERS, 3)), tar_target( z_comp, data.frame(w = w_comp, x = x_comp, y = y_comp), pattern = cross(w_comp, map(x_comp, y_comp)) ) ) tar_make() tar_read(z_comp) 7.5 Branch provenance The tar_branches() function identifies dependency relationships among individual branches. In the example pipeline below, we can find out the branch of y that each branch of z depends on. # _targets.R library(targets) list( tar_target(x, seq_len(3)), tar_target(y, x + 1, pattern = map(x)), tar_target(z, y + 1, pattern = map(y)) ) tar_make() branches &lt;- tar_branches(z, map(y)) branches tar_read_raw(branches$y[2]) However, tar_branches() is not always helpful: for example, if we look at how y branches over x. x does not use dynamic branching, so tar_branches() does not return meaningful branch names. branches &lt;- tar_branches(y, map(x)) branches tar_read_raw(branches$x[2]) In situations like this, it is best to proactively write targets that keep track of information about their upstream branches. Data frames and tibbles are useful for this. # _targets.R library(targets) library(tibble) list( tar_target(x, seq_len(3)), tar_target(y, tibble(x = x, y = x + 1), pattern = map(x)) ) tar_make() tar_read(y) 7.6 Testing patterns To check the correctness of a pattern without running the pipeline, use tar_pattern(). Simply supply the pattern itself and the length of each dependency target. The branch names in the data frames below are made up, but they convey a high-level picture of the branching structure. tar_pattern( cross(w_comp, map(x_comp, y_comp)), w_comp = 2, x_comp = 3, y_comp = 3 ) tar_pattern( head(cross(w_comp, map(x_comp, y_comp)), n = 2), w_comp = 2, x_comp = 3, y_comp = 3 ) tar_pattern( cross(w_comp, sample(map(x_comp, y_comp), n = 2)), w_comp = 2, x_comp = 3, y_comp = 3 ) 7.7 Dynamic branching over files Dynamic branching over files is tricky. A target with format = \"file\" treats the entire set of files as an irreducible bundle. That means in order to branch over files downstream, each file must already have its own branch. # _targets.R library(targets) list( tar_target(paths, c(&quot;a.csv&quot;, &quot;b.csv&quot;)), tar_target(files, paths, format = &quot;file&quot;, pattern = map(paths)), tar_target(data, read_csv(files), pattern = map(files)) ) The tar_files() function from the tarchetypes package is shorthand for the first two targets above. # _targets.R library(targets) library(tarchetypes) list( tar_files(files, c(&quot;a.csv&quot;, &quot;b.csv&quot;)), tar_target(data, read_csv(files), pattern = map(files)) ) 7.8 Iteration There are many ways to slice up a stem for branching, and there are many ways to aggregate the branches of a pattern.4 The iteration argument of tar_target() controls the splitting and aggregation protocol on a target-by-target basis, and you can set the default for all targets with the analogous argument of tar_option_set(). 7.8.1 Vector iteration targets uses vector iteration by default, and you can opt into this behavior by setting iteration = \"vector\" in tar_target(). In vector iteration, targets uses the vctrs package to split stems and aggregate branches. That means vctrs::vec_slice() slices up stems like x for mapping, and vctrs::vec_c() aggregates patterns like y for operations like tar_read(). For atomic vectors like in the example above, this behavior is already intuitive. But if we map over a data frame, each branch will get a row of the data frame due to vector iteration. library(targets) print_and_return &lt;- function(x) { print(x) x } list( tar_target(x, data.frame(a = c(1, 2), b = c(&quot;a&quot;, &quot;b&quot;))), tar_target(y, print_and_return(x), pattern = map(x)) ) tar_make() And since y also has iteration = \"vector\", the aggregate of y is a single data frame of all the rows. tar_read(y) 7.8.2 List iteration List iteration splits and aggregates targets as simple lists. If target x has \"list\" iteration, all branches of downstream patterns will get x[[1]], x[[2]], and so on. (vctrs::vec_slice() behaves more like [] than [[]].) # _targets.R library(targets) print_and_return &lt;- function(x) { print(x) x } list( tar_target( x, data.frame(a = c(1, 2), b = c(&quot;a&quot;, &quot;b&quot;)), iteration = &quot;list&quot; ), tar_target(y, print_and_return(x), pattern = map(x)), tar_target(z, x, pattern = map(x), iteration = &quot;list&quot;) ) tar_make() Aggregation also happens differently. In this case, the vector iteration in y is not ideal, and the list iteration in z gives us more sensible output. tar_read(y) tar_read(z) 7.8.3 Group iteration Group iteration brings dplyr::group_by() functionality to patterns. This way, we can map or cross over custom subsets of rows. Consider the following data frame. object &lt;- data.frame( x = seq_len(6), id = rep(letters[seq_len(3)], each = 2) ) object To map over the groups of rows defined by the id column, we Use group_by() and tar_group() to define the groups of rows, and Use iteration = \"group\" in tar_target() to tell downstream patterns to use the row groups. Put together, the pipeline looks like this. # _targets.R library(targets) tar_option_set(packages = &quot;tidyverse&quot;) list( tar_target( data, data.frame( x = seq_len(6), id = rep(letters[seq_len(3)], each = 2) ) %&gt;% group_by(id) %&gt;% tar_group(), iteration = &quot;group&quot; ), tar_target( subsets, data, pattern = map(data), iteration = &quot;list&quot; ) ) tar_make() lapply(tar_read(subsets), as.data.frame) Row groups are defined in the special tar_group column created by tar_group(). data.frame( x = seq_len(6), id = rep(letters[seq_len(3)], each = 2) ) %&gt;% dplyr::group_by(id) %&gt;% tar_group() tar_group() creates this column based on the orderings of the grouping variables supplied to dplyr::group_by(), not the order of the rows in the data. flip_order &lt;- function(x) { ordered(x, levels = sort(unique(x), decreasing = TRUE)) } data.frame( x = seq_len(6), id = flip_order(rep(letters[seq_len(3)], each = 2)) ) %&gt;% dplyr::group_by(id) %&gt;% tar_group() The ordering in tar_group agrees with the ordering shown by dplyr::group_keys(). data.frame( x = seq_len(6), id = flip_order(rep(letters[seq_len(3)], each = 2)) ) %&gt;% dplyr::group_by(id) %&gt;% dplyr::group_keys() Branches are arranged in increasing order with respect to the integers in tar_group. # _targets.R library(targets) tar_option_set(packages = &quot;tidyverse&quot;) flip_order &lt;- function(x) { ordered(x, levels = sort(unique(x), decreasing = TRUE)) } list( tar_target( data, data.frame( x = seq_len(6), id = flip_order(rep(letters[seq_len(3)], each = 2)) ) %&gt;% group_by(id) %&gt;% tar_group(), iteration = &quot;group&quot; ), tar_target( subsets, data, pattern = map(data), iteration = &quot;list&quot; ) ) tar_make() lapply(tar_read(subsets), as.data.frame) 7.9 Batching With dynamic branching, it is super easy to create an enormous number of targets. But when the number of targets starts to exceed a couple hundred, tar_make() slows down, and graphs from tar_visnetwork() start to become unmanageable. If that happens to you, consider batching your work into a smaller number of targets. Targetopia packages usually have functions that support batching for various use cases. In stantargets, tar_stan_mcmc_rep_summary() and friends automatically use batching behind the scenes. The user simply needs to select the number of batches and number of reps per batch. Each batch is a dynamic branch with multiple reps, and each rep fits the user’s model once and computes summary statistics. In tarchetypes, tar_rep()` is a general-use target factory for dynamic branching. It allows you to repeat arbitrary code over multiple reps split into multiple batches. Each batch gets its own reproducible random number seed generated from the target name (as do all targets) and reps run sequentially within each batch, so the results are reproducible. The targets-stan repository has an example of batching implemented from scratch. The goal of the pipeline is to validate a Bayesian model by simulating thousands of dataset, analyzing each with a Bayesian model, and assessing the overall accuracy of the inference. Rather than define a target for each dataset in model, the pipeline breaks up the work into batches, where each batch has multiple datasets or multiple analyses. Here is a version of the pipeline with 40 batches and 25 simulation reps per batch (1000 reps total in a pipeline of 82 targets). list( tar_target(model_file, compile_model(&quot;stan/model.stan&quot;), format = &quot;file&quot;), tar_target(index_batch, seq_len(40)), tar_target(index_sim, seq_len(25)), tar_target( data_continuous, purrr::map_dfr(index_sim, ~simulate_data_continuous()), pattern = map(index_batch) ), tar_target( fit_continuous, map_sims(data_continuous, model_file = model_file), pattern = map(data_continuous) ) ) Slicing is always the same when we branch over an existing pattern. If we have tar_target(y, x, pattern = map(x)) and x is another pattern, then each branch of y always gets a branch of x regardless of the iteration method. Likewise, the aggregation of stems does not depend on the iteration method because every stem is already aggregated.↩︎ "],["static.html", "Chapter 8 Static branching 8.1 Branching 8.2 When to use static branching 8.3 Map 8.4 Dynamic-within-static branching 8.5 Combine 8.6 Metaprogramming", " Chapter 8 Static branching 8.1 Branching Sometimes, a pipeline contains more targets than a user can comfortably type by hand. For projects with hundreds of targets, branching can make the _targets.R file more concise and easier to read and maintain. targets supports two types of branching: dynamic branching and static branching. Some projects are better suited to dynamic branching, while others benefit more from static branching or a combination of both. Some users understand dynamic branching more easily because it avoids metaprogramming, while others prefer static branching because tar_manifest() and tar_visnetwork() provide immediate feedback. Except for the section on dynamic-within-static branching, you can read the two chapters on branching in any order (or skip them) depending on your needs. 8.2 When to use static branching Static branching is the act of defining a group of targets in bulk before the pipeline starts. Whereas dynamic branching uses last-minute dependency data to define the branches, static branching uses metaprogramming to modify the code of the pipeline up front. Whereas dynamic branching excels at creating a large number of very similar targets, static branching is most useful for smaller number of heterogeneous targets. Some users find it more convenient because they can use tar_manifest() and tar_visnetwork() to check the correctness of static branching before launching the pipeline. 8.3 Map tar_map() from the tarchetypes package creates copies of existing target objects, where each new command is a variation on the original. In the example below, we have a data analysis workflow that iterates over datasets and analysis methods. The values data frame has the operational parameters of each data analysis, and tar_map() creates one new target per row. # _targets.R file: library(targets) library(tarchetypes) library(tibble) values &lt;- tibble( method_function = rlang::syms(c(&quot;method1&quot;, &quot;method2&quot;)), data_source = c(&quot;NIH&quot;, &quot;NIAID&quot;) ) targets &lt;- tar_map( values = values, tar_target(analysis, method_function(data_source, reps = 10)), tar_target(summary, summarize_analysis(analysis, data_source)) ) list(targets) tar_manifest() #&gt; # A tibble: 4 x 3 #&gt; name command pattern #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 analysis_method2_NI… &quot;method2(\\&quot;NIAID\\&quot;, reps = 10)&quot; &lt;NA&gt; #&gt; 2 analysis_method1_NIH &quot;method1(\\&quot;NIH\\&quot;, reps = 10)&quot; &lt;NA&gt; #&gt; 3 summary_method2_NIA… &quot;summarize_analysis(analysis_method2_NIAID, \\&quot;NI… &lt;NA&gt; #&gt; 4 summary_method1_NIH &quot;summarize_analysis(analysis_method1_NIH, \\&quot;NIH\\… &lt;NA&gt; tar_visnetwork(targets_only = TRUE) For shorter target names, use the names argument of tar_map(). And for more combinations of settings, use tidyr::expand_grid() on values. # _targets.R file: library(targets) library(tarchetypes) library(tidyr) values &lt;- expand_grid( # Use all possible combinations of input settings. method_function = rlang::syms(c(&quot;method1&quot;, &quot;method2&quot;)), data_source = c(&quot;NIH&quot;, &quot;NIAID&quot;) ) targets &lt;- tar_map( values = values, names = &quot;data_source&quot;, # Select columns from `values` for target names. tar_target(analysis, method_function(data_source, reps = 10)), tar_target(summary, summarize_analysis(analysis, data_source)) ) list(targets) tar_manifest() #&gt; # A tibble: 8 x 3 #&gt; name command pattern #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 analysis_NIAID_1 &quot;method2(\\&quot;NIAID\\&quot;, reps = 10)&quot; &lt;NA&gt; #&gt; 2 analysis_NIAID &quot;method1(\\&quot;NIAID\\&quot;, reps = 10)&quot; &lt;NA&gt; #&gt; 3 analysis_NIH_1 &quot;method2(\\&quot;NIH\\&quot;, reps = 10)&quot; &lt;NA&gt; #&gt; 4 analysis_NIH &quot;method1(\\&quot;NIH\\&quot;, reps = 10)&quot; &lt;NA&gt; #&gt; 5 summary_NIAID_1 &quot;summarize_analysis(analysis_NIAID_1, \\&quot;NIAID\\&quot;)&quot; &lt;NA&gt; #&gt; 6 summary_NIAID &quot;summarize_analysis(analysis_NIAID, \\&quot;NIAID\\&quot;)&quot; &lt;NA&gt; #&gt; 7 summary_NIH_1 &quot;summarize_analysis(analysis_NIH_1, \\&quot;NIH\\&quot;)&quot; &lt;NA&gt; #&gt; 8 summary_NIH &quot;summarize_analysis(analysis_NIH, \\&quot;NIH\\&quot;)&quot; &lt;NA&gt; # You may need to zoom out on this interactive graph to see all 8 targets. tar_visnetwork(targets_only = TRUE) 8.4 Dynamic-within-static branching You can even combine together static and dynamic branching. The static tar_map() is an excellent outer layer on top of targets with patterns. The following is a sketch of a pipeline that runs each of two data analysis methods 10 times, once per random seed. Static branching iterates over the method functions, while dynamic branching iterates over the seeds. tar_map() creates new patterns as well as new commands. So below, the summary methods map over the analysis methods both statically and dynamically. # _targets.R file: library(targets) library(tarchetypes) library(tibble) random_seed_target &lt;- tar_target(random_seed, seq_len(10)) targets &lt;- tar_map( values = tibble(method_function = rlang::syms(c(&quot;method1&quot;, &quot;method2&quot;))), tar_target( analysis, method_function(&quot;NIH&quot;, seed = random_seed), pattern = map(random_seed) ), tar_target( summary, summarize_analysis(analysis), pattern = map(analysis) ) ) list(random_seed_target, targets) tar_manifest() #&gt; # A tibble: 5 x 3 #&gt; name command pattern #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 random_seed &quot;seq_len(10)&quot; &lt;NA&gt; #&gt; 2 analysis_method1 &quot;method1(\\&quot;NIH\\&quot;, seed = random_seed)&quot; map(random_seed) #&gt; 3 analysis_method2 &quot;method2(\\&quot;NIH\\&quot;, seed = random_seed)&quot; map(random_seed) #&gt; 4 summary_method1 &quot;summarize_analysis(analysis_method1)&quot; map(analysis_method1) #&gt; 5 summary_method2 &quot;summarize_analysis(analysis_method2)&quot; map(analysis_method2) tar_visnetwork(targets_only = TRUE) 8.5 Combine tar_combine() from the tarchetypes package creates a new target to aggregate the results of upstream targets. In the simple example below, our combined target simply aggregates the rows returned from two other targets. # _targets.R file: library(targets) library(tarchetypes) library(tibble) options(crayon.enabled = FALSE) target1 &lt;- tar_target(head, head(mtcars, 1)) target2 &lt;- tar_target(tail, tail(mtcars, 1)) target3 &lt;- tar_combine(combined_target, target1, target2) list(target1, target2, target3) tar_manifest() #&gt; # A tibble: 3 x 3 #&gt; name command pattern #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 head_mtcars head(mtcars, 1) &lt;NA&gt; #&gt; 2 tail_mtcars tail(mtcars, 1) &lt;NA&gt; #&gt; 3 combined_targ… vctrs::vec_c(head_mtcars = head_mtcars, tail_mtcars = … &lt;NA&gt; tar_visnetwork(targets_only = TRUE) tar_make() #&gt; ● run target head_mtcars #&gt; ● run target tail_mtcars #&gt; ● run target combined_target #&gt; ● end pipeline tar_read(combined_target) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21.0 6 160 110 3.90 2.62 16.46 0 1 4 4 #&gt; Volvo 142E 21.4 4 121 109 4.11 2.78 18.60 1 1 4 2 To use tar_combine() and tar_map() together in more complicated situations, you may need to supply unlist = FALSE to tar_map(). That way, tar_map() will return a nested list of target objects, and you can combine the ones you want. The pipeline extends our previous tar_map() example by combining just the summaries, omitting the analyses from tar_combine(). Also note the use of bind_rows(!!!.x) below. This is how you supply custom code to combine the return values of other targets. .x is a placeholder for the return values, and !!! is the “unquote-splice” operator from the rlang package. # _targets.R file: library(targets) library(tarchetypes) library(tibble) random_seed &lt;- tar_target(random_seed, seq_len(10)) mapped &lt;- tar_map( unlist = FALSE, # Return a nested list from tar_map() values = tibble(method_function = rlang::syms(c(&quot;method1&quot;, &quot;method2&quot;))), tar_target( analysis, method_function(&quot;NIH&quot;, seed = random_seed), pattern = map(random_seed) ), tar_target( summary, summarize_analysis(analysis), pattern = map(analysis) ) ) combined &lt;- tar_combine( combined_summaries, mapped[[2]], command = dplyr::bind_rows(!!!.x, .id = &quot;method&quot;) ) list(random_seed, mapped, combined) tar_manifest() #&gt; Warning message: #&gt; Targets and globals must have unique names. Ignoring global objects that conflict with target names: random_seed. Suppress this warning with Sys.setenv(TAR_WARN = &quot;false&quot;) in _targets.R. #&gt; # A tibble: 6 x 3 #&gt; name command pattern #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 random_seed &quot;seq_len(10)&quot; &lt;NA&gt; #&gt; 2 analysis_met… &quot;method1(\\&quot;NIH\\&quot;, seed = random_seed)&quot; map(random_see… #&gt; 3 analysis_met… &quot;method2(\\&quot;NIH\\&quot;, seed = random_seed)&quot; map(random_see… #&gt; 4 summary_meth… &quot;summarize_analysis(analysis_method1)&quot; map(analysis_m… #&gt; 5 summary_meth… &quot;summarize_analysis(analysis_method2)&quot; map(analysis_m… #&gt; 6 combined_sum… &quot;dplyr::bind_rows(summary_method1 = summary_met… &lt;NA&gt; tar_visnetwork(targets_only = TRUE) #&gt; Warning message: #&gt; Targets and globals must have unique names. Ignoring global objects that conflict with target names: random_seed. Suppress this warning with Sys.setenv(TAR_WARN = &quot;false&quot;) in _targets.R. 8.6 Metaprogramming Custom metaprogramming is a more flexible alternative to tar_map() and tar_combine(). tar_eval() from tarchetypes accepts an arbitrary expression and iteratively plugs in symbols. Below, we use it to branch over datasets. # _targets.R library(rlang) library(targets) library(tarchetypes) string &lt;- c(&quot;gapminder&quot;, &quot;who&quot;, &quot;imf&quot;) symbol &lt;- syms(string) tar_eval( tar_target(symbol, get_data(string)), values = list(string = string, symbol = symbol) ) tar_eval() has fewer guardrails than tar_map() or tar_combine(), so tar_manifest() is especially important for checking the correctness of your metaprogramming. tar_manifest(fields = command) #&gt; # A tibble: 3 x 2 #&gt; name command #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 imf &quot;get_data(\\&quot;imf\\&quot;)&quot; #&gt; 2 gapminder &quot;get_data(\\&quot;gapminder\\&quot;)&quot; #&gt; 3 who &quot;get_data(\\&quot;who\\&quot;)&quot; "],["hpc.html", "Chapter 9 High-performance computing 9.1 Clustermq 9.2 Future 9.3 Advanced", " Chapter 9 High-performance computing targets supports high-performance computing with the tar_make_clustermq() and tar_make_future() functions. These functions are like tar_make(), but they allow multiple targets to run simultaneously over parallel workers. These workers can be processes on your local machine, or they can be jobs on a computing cluster. The main process automatically sends a target to a worker as soon as The worker is available, and All the target’s upstream dependency targets have been checked or built. Practical real-world examples of high-performance computing in targets can be found at the examples linked from here. But for the purposes of explaining the mechanics of the package, consider the following sketch of a pipeline. # _targets.R library(targets) list( tar_target(data, get_data()), tar_target(fast_fit, fit_small_model(data)), tar_target(slow_fit, fit_slow_model(data)), tar_target(plot_1, make_plot(fast_fit)), tar_target(plot_2, make_plot(slow_fit)) ) # R console tar_visnetwork() When we run this pipeline with high-performance computing, targets automatically knows to wait for data to finish running before moving on to the other targets. Once data is finished, it moves on to targets fast_fit and slow_fit. If fast_fit finishes before slow_fit, target plot_1 begins even as slow_fit is still running. Unlike drake, targets applies this behavior not only to stem targets, but also to branches of patterns. The following sections cover the mechanics and configuration details of high-performance computing in targets. 9.1 Clustermq tar_make_clustermq() uses persistent workers. That means all the parallel processes launch together as soon as there is a target to build, and all the processes keep running until the pipeline winds down. The video clip below visualizes the concept. 9.1.1 Clustermq installation Persistent workers require the clustermq R package, which in turn requires ZeroMQ. Please refer to the clustermq installation guide for specific instructions. 9.1.2 Clustermq locally When you write your _targets.R script, be sure to set the clustermq.scheduler global option to a a local scheduler like \"multicore\". Many of the supported schedulers and their configuration details are listed here. # _targets.R options(clustermq.scheduler = &quot;multicore&quot;) list( tar_target(data, get_data()), tar_target(fast_fit, fit_small_model(data)), tar_target(slow_fit, fit_slow_model(data)), tar_target(plot_1, make_plot(fast_fit)), tar_target(plot_2, make_plot(slow_fit)) ) Then, run tar_make_clustermq() with the appropriate number of workers. tar_make_clustermq(workers = 2) 9.1.3 Clustermq remotely For parallel computing on a cluster, Choose a scheduler listed here that corresponds to your cluster’s resource manager. Create a template file that configures the computing requirements and other settings for the cluster. Supply the scheduler option and template file to the clustermq.scheduler and clustermq.template global options in your _targets.R file. # _targets.R options(clustermq.scheduler = &quot;sge&quot;, clustermq.template = &quot;sge.tmpl&quot;) list( tar_target(data, get_data()), tar_target(fast_fit, fit_small_model(data)), tar_target(slow_fit, fit_slow_model(data)), tar_target(plot_1, make_plot(fast_fit)), tar_target(plot_2, make_plot(slow_fit)) ) Above, sge_tmpl refers to a template file like the one below. ## From https://github.com/mschubert/clustermq/wiki/SGE #$ -N {{ job_name }} # Worker name. #$ -t 1-{{ n_jobs }} # Submit workers as an array. #$ -j y # Combine stdout and stderr into one worker log file. #$ -o /dev/null # Worker log files. #$ -cwd # Use project root as working directory. #$ -V # Use environment variables. module load R/3.6.3 # Needed if R is an environment module on the cluster. CMQ_AUTH={{ auth }} R --no-save --no-restore -e &#39;clustermq:::worker(&quot;{{ main }}&quot;)&#39; # Leave alone. Then, run tar_make_clustermq() as before. tar_make_clustermq(workers = 2) See the examples linked from here to see how this setup works in real-world projects. 9.1.4 Clustermq configuration In addition to configuration options hard-coded in the template file, you can supply custom computing resources with the resources argument of tar_option_set(). As an example, let’s use a wildcard for the number of cores per worker on an SGE cluster. In the template file, supply {{ num_cores }} wildcard to the -pe smp flag. #$ -pe smp {{ num_cores }} # Number of cores per worker #$ -N {{ job_name | 1 }} #$ -t 1-{{ n_jobs }} #$ -j y #$ -o /dev/null #$ -cwd #$ -V module load R/3.6.3 CMQ_AUTH={{ auth }} R --no-save --no-restore -e &#39;clustermq:::worker(&quot;{{ main }}&quot;)&#39; Then, supply the value of num_cores to the resources option from within _targets.R. # _targets.R tar_option_set(resources = list(num_cores = 2)) list( tar_target(...), ... # more targets ) Finally, call tar_make_clustermq() normally. tar_make_clustermq(workers = 2) This particular use case comes up when you have custom parallel computing within targets and need to take advantage of multiple cores. 9.2 Future tar_make_future() runs transient workers. That means each target gets its own worker which initializes when the target begins and terminates when the target ends. The following video clip demonstrates the concept. 9.2.1 Future installation Install the future package. install.packages(&quot;future&quot;) # CRAN release # Alternatively, install the GitHub development version. devtools::install_github(&quot;HenrikBengtsson/future&quot;, ref = &quot;develop&quot;) If you intend to use a cluster, be sure to install the future.batchtools package too. The future ecosystem contains even more packages that extend future’s parallel computing functionality, such as future.callr. 9.2.2 Future locally To parallelize targets over multiple processes on your local machine, declare one of the future plans listed here in your _targets.R file. # _targets.R library(future) plan(multisession) list( tar_target(data, get_data()), tar_target(fast_fit, fit_small_model(data)), tar_target(slow_fit, fit_slow_model(data)), tar_target(plot_1, make_plot(fast_fit)), tar_target(plot_2, make_plot(slow_fit)) ) Then, run tar_make_future() with the desired number of workers. Here, the workers argument specifies the maximum number of transient workers to allow at a given time. Some future plans also have optional workers arguments that set their own caps. tar_make_future(workers = 2) 9.2.3 Future remotely To run transient workers on a cluster, first install the future.batchtools package. Then, set one of these plans in your _targets.R file. # _targets.R library(future) library(future.batchtools) plan(batchtools_sge, template = &quot;sge.tmpl&quot;) list( tar_target(data, get_data()), tar_target(fast_fit, fit_small_model(data)), tar_target(slow_fit, fit_slow_model(data)), tar_target(plot_1, make_plot(fast_fit)), tar_target(plot_2, make_plot(slow_fit)) ) Here, our template file sge.tmpl is configured for batchtools. #!/bin/bash #$ -cwd # Run in the current working directory. #$ -j y # Direct stdout and stderr to the same file. #$ -o &lt;%= log.file %&gt; # log file #$ -V # Use environment variables. #$ -N &lt;%= job.name %&gt; # job name module load R/3.6.3 # Uncomment and adjust if R is an environment module. Rscript -e &#39;batchtools::doJobCollection(&quot;&lt;%= uri %&gt;&quot;)&#39; # Leave alone. exit 0 # Leave alone. 9.2.4 Future configuration The tar_target(), tar_target_raw(), and tar_option_set() functions accept a resources argument. For example, if our batchtools template file has a wildcard for the number of cores for a job, #!/bin/bash #$ -pe smp &lt;%= resources[[&quot;num_cores&quot;]] | 1 %&gt; # Wildcard for cores per job. #$ -cwd #$ -j y #$ -o &lt;%= log.file %&gt; #$ -V #$ -N &lt;%= job.name %&gt; module load R/3.6.3 Rscript -e &#39;batchtools::doJobCollection(&quot;&lt;%= uri %&gt;&quot;)&#39; exit 0 then you can set the number of cores for individual targets. In the case below, maybe the slow model needs 2 cores to run fast enough. # _targets.R library(future) library(future.batchtools) plan(batchtools_sge, template = &quot;sge.tmpl&quot;) list( tar_target(data, get_data()), tar_target(fast_fit, fit_small_model(data)), tar_target(slow_fit, fit_slow_model(data), resources = list(num_cores = 2)), tar_target(plot_1, make_plot(fast_fit)), tar_target(plot_2, make_plot(slow_fit)) ) Then, run tar_make_future() as usual. tar_make_future(workers = 2) The resources of tar_target() defaults to tar_option_get(\"resources\"). You can set the default value for all targets using tar_option_set(). 9.3 Advanced Functions tar_target(), tar_target_raw(), and tar_option_set() support advanced configuration options for heavy-duty pipelines that require high-performance computing. deployment: With the deployment argument, you can choose to run some targets locally on the main process instead of on a high-performance computing worker. This options is suitable for lightweight targets such as R Markdown reports where runtime is quick and a cluster would be excessive. memory: Choose whether to retain a target in memory or remove it from memory whenever it is not needed at the moment. This is a tradeoff between memory consumption and storage read speeds, and like all of the options listed here, you can set it on a target-by-target basis. The default settings consume a lot of memory to avoid frequently reading from storage. To keep memory usage down to a minimum, set memory = \"transient\" and garbage_collection = TRUE in tar_target() or tar_option_set(). For cloud-based dynamic files such as format = \"aws_file\", this memory policy applies to temporary local copies of the file in _targets/scratch/: \"persistent\" means they remain until the end of the pipeline, and \"transient\" means they get deleted from the file system as soon as possible. The former conserves bandwidth, and the latter conserves local storage. garbage_collection: Choose whether to run base::gc() just before running the target. storage: Choose whether the parallel workers or the main process is responsible for saving the target’s value. For slow network file systems on clusters, storage = \"main\" is often faster for small numbers of targets. For large numbers of targets or low-bandwidth connections between the main and workers, storage = \"worker\" is often faster. Always choose storage = \"main\" if the workers do no have access to the file system with the _targets/ data store. retrieval: Choose whether the parallel workers or the main process is responsible for reading dependency targets from disk. Should usually be set to whatever you choose for storage (default). Always choose retrieval = \"main\" if the workers do no have access to the file system with the _targets/ data store. format: If your pipeline has large computation, it may also have large data. Consider setting the format argument to help targets store and retrieve your data faster. error: Set error to \"continue\" to let the rest of the pipeline keep running even if a target encounters an error. "],["cloud.html", "Chapter 10 Cloud integration 10.1 Compute 10.2 Storage", " Chapter 10 Cloud integration targets has built-in cloud capabilities to help scale pipelines up and out. Cloud storage solutions are already available, and cloud computing computing solutions are in the works. Before getting started, please familiarize yourself with the pricing model and cost management and monitoring tools of Amazon Web Services. Everything has a cost, from virtual instances to web API calls. Free tier accounts give you a modest monthly budget for some services for the first year, but it is easy to exceed the limits. Developers of reusable software should consider applying for promotional credit using this application form. 10.1 Compute Right now, targets does not have built-in cloud-based distributed computing support. However, future development plans include seamless integration with AWS Batch. As a temporary workaround, it is possible to deploy a burstable SLURM cluster using AWS ParallelCluster and leverage targets’ existing support for traditional schedulers. 10.2 Storage targets supports cloud storage on a target-by-target basis using Amazon Simple Storage Service, or S3. After a target completes, the return value is uploaded to a user-defined S3 bucket S3 bucket. Follow these steps to get started. 10.2.1 Get started with the Amazon S3 web console If you do not already have an Amazon Web Services account, sign up for the free tier at https://aws.amazon.com/free. Then, follow these step-by-step instructions to practice using Amazon S3 through the web console at https://console.aws.amazon.com/s3/. 10.2.2 Configure your local machine targets uses the aws.s3 package behind the scenes. It is not a strict dependency of targets, so you will need to install it yourself. install.packages(&quot;aws.s3&quot;) Next, aws.s3 needs an access ID, secret access key, and default region. Follow these steps to generate the keys, and choose a region from this table of endpoints. Then, open the .Renviron file in your home directory with usethis::edit_r_environ() and store this information in special environment variables. Here is an example .Renviron file. # Example .Renviron file AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY AWS_DEFAULT_REGION=us-east-1 Restart your R session so the changes take effect. Your keys are sensitive personal information. You can print them in your private console to verify correctness, but otherwise please avoid saving them to any persistent documents other than .Renviron. Sys.getenv(&quot;AWS_ACCESS_KEY_ID&quot;) #&gt; [1] &quot;AKIAIOSFODNN7EXAMPLE&quot; Sys.getenv(&quot;AWS_SECRET_ACCESS_KEY&quot;) #&gt; [1] &quot;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&quot; Sys.getenv(&quot;AWS_DEFAULT_REGION&quot;) #&gt; [1] &quot;us-east-1&quot; 10.2.3 Create S3 buckets Now, you are ready to create one or more S3 buckets for your targets pipeline. Each pipeline should have its own set of buckets. Create one through the web console or with aws.s3::put_bucket(). library(aws.s3) put_bucket(&quot;my-test-bucket-25edb4956460647d&quot;) #&gt; [1] TRUE Sign in to https://s3.console.aws.amazon.com/s3 to verify that the bucket exists. 10.2.4 Configure the pipeline To connect your pipeline with S3, Supply your bucket name to resources in tar_option_set(). To use different buckets for different targets, set resources directly in tar_target(). Supply AWS-powered storage formats to tar_option_set() and/or tar_target(). See the tar_target() help file for the full list of formats. Your _targets.R file will look something like this. # Example _targets.R library(targets) tar_option_set(resources = list(bucket = &quot;my-test-bucket-25edb4956460647d&quot;)) write_mean &lt;- function(data) { tmp &lt;- tempfile() writeLines(as.character(mean(data)), tmp) tmp } list( tar_target(data, rnorm(5), format = &quot;aws_qs&quot;), tar_target(mean_file, write_mean(data), format = &quot;aws_file&quot;) ) 10.2.5 Run the pipeline When you run the pipeline above with tar_make(), your local R session computes rnorm(5), saves it to a temporary qs file on disk, and then uploads it to a file called _targets/objects/data on your S3 bucket. Likewise for mean_file, but because the format is \"aws_file\", you are responsible for supplying the path to the file that gets uploaded to _targets/objects/mean_file. tar_make() #&gt; ● run target data #&gt; ● run target mean_file And of course, your targets stay up to date if you make no changes. tar_make() #&gt; ✓ skip target data #&gt; ✓ skip target mean_file #&gt; ✓ Already up to date. 10.2.6 Manage the data Log into https://s3.console.aws.amazon.com/s3. You should see objects _targets/objects/data and _targets/objects/mean_file in your bucket. To download this data locally, use tar_read() and tar_load() like before. These functions download the data from the bucket and load it into R. tar_read(data) #&gt; [1] -0.74654607 -0.59593497 -1.57229983 0.40915323 0.02579023 The \"aws_file\" format is different from the other AWS-powered formats. tar_read() and tar_load() download the object to a temporary file and return the path so you can process it yourself.5 tar_load(mean_file) mean_file #&gt; [1] &quot;_targets/scratch/mean_fileff086e70876d&quot; readLines(mean_file) #&gt; [1] &quot;-0.495967480886693&quot; When you are done with these temporary files and the pipeline is no longer running, you can safely remove everything in _targets/scratch/. unlink(&quot;_targets/scratch/&quot;, recursive = TRUE) Lastly, if you want to erase the whole project or start over from scratch, consider removing the S3 bucket to avoid incurring storage fees. The easiest way to do this is through the S3 console. You can alternatively call aws.s3::delete_bucket(), but you have to make sure the bucket is empty first. delete_bucket(&quot;my-test-bucket-25edb4956460647d&quot;) Non-“file” AWS formats also download temporary files, but they are immediately discarded after they are read into memory.↩︎ "],["drake.html", "Chapter 11 What about drake? 11.1 Transitioning to targets 11.2 Advantages of targets over drake", " Chapter 11 What about drake? The drake package is an older and more established R-focused pipeline toolkit. It is has become a key piece of the R ecosystem, and development and support will continue. Future efforts will focus on the issues that bring the most concrete value to users: requested features, performance improvements for known inefficiencies, bug fixes, documentation, education, and prompt one-on-one help. Existing drake-powered projects can safely continue to use drake, and there is no urgency to migrate to targets. However, nearly four years of community feedback have exposed major user-side limitations regarding data management, collaboration, dynamic branching, and parallel efficiency. Unfortunately, these limitations are permanent. Solutions in drake itself would make the package incompatible with existing projects that use it, and the internal architecture is too copious, elaborate, and mature for such extreme refactoring. That is why targets was created. The targets package borrows from past learnings, user suggestions, discussions, complaints, success stories, and feature requests, and it improves the user experience in ways that will never be possible in drake. 11.1 Transitioning to targets If you know drake, then you already almost know targets. The programming style is similar, and most functions in targets have counterparts in drake. Functions in drake Counterparts in targets use_drake(), drake_script() tar_script() drake_plan() tar_manifest(), tarchetypes::tar_plan() target() tar_target(), tar_target_raw() drake_config() tar_option_set() outdated(), r_outdated() tar_outdated() vis_drake_graph(), r_vis_drake_graph() tar_visnetwork(), tar_glimpse() drake_graph_info(), r_drake_graph_info() tar_network() make(), r_make() tar_make(), tar_make_clustermq(), tar_make_future() loadd() tar_load() readd() tar_read() diagnose(), build_times(), cached(), drake_cache_log() tar_meta() drake_progress(), drake_running(), drake_done(), drake_failed(), drake_cancelled() tar_progress() clean() tar_deduplicate(), tar_delete(), tar_destroy(), tar_invalidate() drake_gc() tar_prune() id_chr() tar_name(), tar_path() knitr_in() tarchetypes::tar_render() cancel(), cancel_if() tar_cancel() trigger() tar_cue() drake_example(), drake_example(), load_mtcars_example(), clean_mtcars_example() Unsupported. Example targets pipelines are in individual repositories linked from here. drake_build() Unsupported in targets to ensure coherence with dynamic branching. drake_debug() Read here to learn about interactive debugging in targets. drake_history(), recoverable() Unsupported in targets. Instead of trying to manage history and data recovery directly, targets maintains a much lighter/friendlier data store to make it easier to use external data versioning tools instead. missed(), tracked(), deps_code(), deps_target(), deps_knitr(), deps_profile() Unsupported in targets because dependency detection is far easier to understand than in drake. drake_hpc_template_file(), drake_hpc_template_files() Deemed out of scope for targets. drake_cache(), new_cache(), find_cache(). Unsupported because targets is far more strict and paternalistic about data/file management. rescue_cache(), which_clean(), cache_planned(), cache_unplanned() Unsupported due to the simplified data management system and storage cleaning functions. drake_get_session_info() Deemed superfluous and a potential bottleneck. Discarded for targets. read_drake_seed() Superfluous because targets always uses the same global seed. tar_meta() shows all the target-level seeds. show_source() Deemed superfluous. Discarded in targets to conserve storage space in _targets/meta/meta. drake_tempfile() Superfluous in targets because there is no special disk.frame storage format. (Dynamic file targets are much better for managing disk.frames.) file_store() Superfluous in targets because all files are dynamic files and there is no longer a need to Base32-encode any file names. Likewise, many make() arguments have equivalent arguments elsewhere. Argument of drake::make() Counterparts in targets targets names in tar_make() etc. envir envir in tar_option_set() verbose reporter in tar_make() etc. parallelism Choice of function: tar_make() vs tar_make_clustermq() vs tar_make_future() jobs workers in tar_make_clustermq() and tar_make_future() packages packages in tar_target() and tar_option_set() lib_loc library in tar_target() and tar_option_set() trigger cue in tar_target() and tar_option_set() caching storage and retrieval in tar_target() and tar_option_set() keep_going error in tar_target() and tar_option_set() memory_strategy memory in tar_target() and tar_option_set() garbage_collection garbage_collection in tar_target() and tar_option_set() template resources in tar_target() and tar_option_set() curl_handles handle element of resources argument of tar_target() and tar_option_set() format format in tar_target() and tar_option_set() In addition, many optional columns of drake plans are expressed differently in targets. Optional column of drake plans Feature in targets format format argument of tar_target() and tar_option_set() dynamic pattern argument of tar_target() and tar_option_set() transform static branching functions in tarchetypes such as tar_map() and tar_combine() trigger cue argument of tar_target() and tar_option_set() hpc deployment argument of tar_target() and tar_option_set() resources resources argument of tar_target() and tar_option_set() caching storage and retrieval arguments of tar_target() and tar_option_set() 11.2 Advantages of targets over drake 11.2.1 Better guardrails by design drake leaves ample room for user-side mistakes, and some of these mistakes require extra awareness or advanced knowledge of R to consistently avoid. The example behaviors below are too systemic to solve and still preserve back-compatibility. By default, make() looks for functions and global objects in the parent environment of the calling R session. Because the global environment is often old and stale in practical situations, which causes targets to become incorrectly invalidated. Users need to remember to restart the session before calling make(). The issue is discussed here, and the discussion led to functions like r_make() which always create a fresh session to do the work. However, r_make() is not a complete replacement for make(), and beginner users still run into the original problems. Similar to the above, make() does not find the intended functions and global objects if it is called in a different environment. Edge cases like this one and this one continue to surprise users. drake is extremely flexible about the location of the .drake/ cache. When a user calls readd(), loadd(), make(), and similar functions, drake searches up through the parent directories until it finds a .drake/ folder. This flexibility seldom helps, and it creates uncertainty and inconsistency when it comes to initializing and accessing projects, especially if there are multiple projects with nested file systems. The targets package solves all these issues by design. Functions tar_make(), tar_make_clustermq(), and tar_make_future() all create fresh new R sessions by default. They all require a _targets.R configuration file in the project root (working directory of the tar_make() call) so that the functions, global objects, and settings are all populated in the exact same way each session, leading to less frustration, greater consistency, and greater reproducibility. In addition, the _targets/ data store always lives in the project root. 11.2.2 Enhanced debugging support targets has enhanced debugging support. With the workspaces argument to tar_option_set(), users can locally recreate the conditions under which a target runs. This includes packages, global functions and objects, and the random number generator seed. Similarly, tar_option_set(error = \"workspace\") automatically saves debugging workspaces for targets that encounter errors. The debug option lets users enter an interactive debugger for a given target while the pipeline is running. And unlike drake, all debugging features are fully compatible with dynamic branching. 11.2.3 Improved tracking of package functions By default, targets ignores changes to functions inside external packages. However, if a workflow centers on a custom package with methodology under development, users can make targets automatically watch the package’s functions for changes. Simply supply the names of the relevant packages to the imports argument of tar_option_set(). Unlike drake, targets can track multiple packages this way, and the internal mechanism is much safer. 11.2.4 Lighter, friendlier data management drake’s cache is an intricate file system in a hidden .drake folder. It contains multiple files for each target, and those names are not informative. (See the files in the data/ folder in the diagram below.) Users often have trouble understanding how drake manages data, resolving problems when files are corrupted, placing the data under version control, collaborating with others on the same pipeline, and clearing out superfluous data when the cache grows large in storage. .drake/ ├── config/ ├── data/ ├───── 17bfcef645301416.rds ├───── 21935c86f12692e2.rds ├───── 37caf5df2892cfc4.rds ├───── ... ├── drake/ ├───── history/ ├───── return/ ├───── tmp/ ├── keys/ # A surprisingly large number of tiny text files live here. ├───── memoize/ ├───── meta/ ├───── objects/ ├───── progress/ ├───── recover/ ├───── session/ └── scratch/ # This folder should be temporary, but it gets egregiously large. The targets takes a friendlier, more transparent, less mysterious approach to data management. Its data store is a visible _targets folder, and it contains far fewer files: a spreadsheet of metadata, a spreadsheet of target progress, and one informatively named data file for each target. It is much easier to understand the data management process, identify and diagnose problems, place projects under version control, and avoid consuming unnecessary storage resources. Sketch: _targets/ ├── meta/ ├───── meta ├───── progress ├── objects/ ├───── target_name_1 ├───── target_name_2 ├───── target_name_3 ├───── ... └── scratch/ # Deleted when the pipeline finishes. 11.2.5 Cloud storage Thanks to the simplified data store and simplified internals, targets can automatically upload data to the Amazon S3 bucket of your choice. Simply configure aws.s3, create a bucket, and select one of the AWS-powered storage formats. Then, targets will automatically upload the return values to the cloud. # _targets.R tar_option_set(resources = list(bucket = &quot;my-bucket-name&quot;)) list( tar_target(dataset, get_large_dataset(), format = &quot;aws_fst_tbl&quot;), tar_target(analysis, analyze_dataset(dataset), format = &quot;aws_qs&quot;) ) Data retrieval is still super easy. tar_read(dataset) 11.2.6 Show status of functions and global objects drake has several utilities that inform users which targets are up to date and which need to rerun. However, those utilities are limited by how drake manages functions and other global objects. Whenever drake inspects globals, it stores their values in its cache and loses track of their previous state from the last run of the pipeline. As a result, it has trouble informing users exactly why a given target is out of date. And because the system for tracking global objects is tightly coupled with the cache, this limitation is permanent. In targets, the metadata management system only updates information on global objects when the pipeline actually runs. This makes it possible to understand which specific changes to your code could have invalided your targets. In large projects with long runtimes, this feature contributes significantly to reproducibility and peace of mind. 11.2.7 Dynamic branching with dplyr::group_by() Dynamic branching was an architecturally difficult fit in drake, and it can only support one single (vctrs-based) method of slicing and aggregation for processing sub-targets. This limitation has frustrated members of the community, as discussed here and here. targets, on the other hand, is more flexible regarding slicing and aggregation. When it branches over an object, it can iterate over vectors, lists, and even data frames grouped with dplyr::group_by(). To branch over chunks of a data frame, our data frame target needs to have a special tar_group column. We can create this column in our target’s return value with the tar_group() function. library(dplyr) library(targets) data.frame( x = seq_len(6), id = rep(letters[seq_len(3)], each = 2) ) %&gt;% group_by(id) %&gt;% tar_group() Our actual target has the command above and iteration = \"group\". tar_target( data, data.frame( x = seq_len(6), id = rep(letters[seq_len(3)], each = 2) ) %&gt;% group_by(id) %&gt;% tar_group(), iteration = &quot;group&quot; ) Now, any target that maps over data is going to define one branch for each group in the data frame. The following target creates three branches when run in a pipeline: one returning 3, one returning 7, and one returning 11. tar_target( sums, sum(data$x), pattern = map(data) ) 11.2.8 Composable dynamic branching Because the design of targets is fundamentally dynamic, users can create complicated dynamic branching patterns that are never going to be possible in drake. Below, target z creates six branches, one for each combination of w and tuple (x, y). The pattern cross(w, map(x, y)) is equivalent to tidyr::crossing(w, tidyr::nesting(x, y)). # _targets.R library(targets) list( tar_target(w, seq_len(2)), tar_target(x, head(letters, 3)), tar_target(y, head(LETTERS, 3)), tar_target( z, data.frame(w = w, x = x, y = y), pattern = cross(w, map(x, y)) ) ) Thanks to glep and djbirke on GitHub for the idea. 11.2.9 Improved parallel efficiency Dynamic branching in drake is staged. In other words, all the sub-targets of a dynamic target must complete before the pipeline moves on to downstream targets. The diagram below illustrates this behavior in a pipeline with a dynamic target B that maps over another dynamic target A. For thousands of dynamic sub-targets with highly variable runtimes, this behavior consumes unnecessary runtime and computing resources. And because drake’s architecture was designed at a fundamental level for static branching only, this limitation is permanent. By contrast, the internal data structures in targets are dynamic by design, which allows for a dynamic branching model with more flexibility and parallel efficiency. Branches can always start as soon as their upstream dependencies complete, even if some of those upstream dependencies are branches. This behavior reduces runtime and reduces consumption of computing resources. 11.2.10 Metaprogramming In drake, pipelines are defined with the drake_plan() function. drake_plan() supports an elaborate domain specific language that diffuses user-supplied R expressions. This makes it convenient to assign commands to targets in the vast majority of cases, but it also obstructs custom metaprogramming by users (example here). Granted, it is possible to completely circumvent drake_plan() and create the whole data frame from scratch, but this is hardly ideal and seldom done in practice. The targets package tries to make customization easier. Relative to drake, targets takes a decentralized approach to setting up pipelines, moving as much custom configuration as possible to the target level rather than the whole pipeline level. In addition, the tar_target_raw() function avoids non-standard evaluation while mirroring tar_target() in all other respects. All this makes it much easier to create custom metaprogrammed pipelines and target archetypes while avoiding an elaborate domain specific language for static branching, which was extremely difficult to understand and error prone in drake. The R Targetopia is an emerging ecosystem of workflow frameworks that take full advantage of this customization and democratize reproducible pipelines. "]]
